{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style = \"color:orange\">1. Introduction<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">1.1. Project purpose</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xây dựng và đánh giá tập hợp các mô hình dự báo chuỗi thời gian để dự đoán nhiệt độ cực đại hàng ngày tại các khu vực đô thị và ven biển Việt Nam. Dự án áp dụng cả thuật toán Machine Learning truyền thống và Deep Learning hiện đại nhằm cải thiện độ chính xác dự báo so với các phương pháp thống kê thông thường.\n",
    "\n",
    "<b>Nguồn dữ liệu</b>: Bộ dữ liệu ERA5 (ECMWF) với các bản ghi nhiệt độ từ 1990 đến 2024, được xử lý và biến đổi để huấn luyện các mô hình như Random Forest, XGBoost, LSTM, Transformer, TFT, và N-BEATS.\n",
    "\n",
    "<b>Kết quả</b>: Đánh giá hiệu năng giữa các mô hình qua nhiều kịch bản thực nghiệm và đề xuất hệ thống cảnh báo nhiệt độ sớm ứng dụng thực tế."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">1.2. Data source and description</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Thông tin dữ liệu trong đề tài:</h4>\n",
    "\n",
    "<ul>\n",
    "<li><b>Thời gian thu thập:</b> từ năm <b>1990 đến 2024</b></li>\n",
    "<li><b>Định dạng ban đầu:</b> .grib, sau đó chuyển đổi sang .csv để xử lý</li>\n",
    "</ul>\n",
    "\n",
    "<h4>Các biến số chính trong tập dữ liệu:</h4>\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Tên cột dữ liệu</th>\n",
    "<th>Ý nghĩa</th>\n",
    "<th>Đơn vị đo</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td><code>NAME</code></td><td>Tên tỉnh/thành phố nơi thu thập dữ liệu</td><td>-</td></tr>\n",
    "<tr><td><code>LATITUDE</code></td><td>Vĩ độ địa lý của điểm đo</td><td>Độ</td></tr>\n",
    "<tr><td><code>LONGITUDE</code></td><td>Kinh độ địa lý của điểm đo</td><td>Độ</td></tr>\n",
    "<tr><td><code>YMD</code></td><td>Ngày/tháng/năm đo đạc</td><td>dd/mm/yyyy</td></tr>\n",
    "<tr><td><code>YEAR</code></td><td>Năm đo đạc</td><td>Năm</td></tr>\n",
    "<tr><td><code>MONTH</code></td><td>Tháng đo đạc</td><td>Tháng</td></tr>\n",
    "<tr><td><code>DAY</code></td><td>Ngày đo đạc</td><td>Ngày</td></tr>\n",
    "<tr><td><code>TEMP_max</code></td><td>Nhiệt độ không khí cực đại trong ngày</td><td>°C</td></tr>\n",
    "<tr><td><code>TEMP_ave</code></td><td>Nhiệt độ trung bình trong ngày</td><td>°C</td></tr>\n",
    "<tr><td><code>DEW_ave</code></td><td>Điểm sương trung bình trong ngày</td><td>°C</td></tr>\n",
    "<tr><td><code>DEW_max</code></td><td>Điểm sương cao nhất trong ngày</td><td>°C</td></tr>\n",
    "<tr><td><code>RH_ave</code></td><td>Độ ẩm tương đối trung bình trong ngày</td><td>%</td></tr>\n",
    "<tr><td><code>RH_max</code></td><td>Độ ẩm tương đối cực đại trong ngày</td><td>%</td></tr>\n",
    "<tr><td><code>AT_ave</code></td><td>Nhiệt độ cảm nhận trung bình trong ngày (Apparent Temp.)</td><td>°C</td></tr>\n",
    "<tr><td><code>AT_max</code></td><td>Nhiệt độ cảm nhận cao nhất trong ngày</td><td>°C</td></tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<p><b>Biến mục tiêu chính:</b></p>\n",
    "<ul>\n",
    "<li><code>TEMP_max</code> — Nhiệt độ không khí cực đại hàng ngày (°C)</li>\n",
    "</ul>\n",
    "\n",
    "<p><b>Lưu ý:</b> Dữ liệu gốc của ERA5 có thể chứa giá trị thiếu, giá trị ngoại lai và một số dị bản khí tượng đặc thù. Do đó, quá trình làm sạch dữ liệu, xử lý giá trị thiếu, phát hiện ngoại lệ và chuẩn hóa dữ liệu là các bước bắt buộc trước khi tiến hành huấn luyện và dự báo.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_type = {\n",
    "    'NAME'       : 'Categorical',        # Tên tỉnh/thành phố (chuỗi)\n",
    "    'LATITUDE'   : 'Numerical',          # Vĩ độ (°)\n",
    "    'LONGITUDE'  : 'Numerical',          # Kinh độ (°)\n",
    "    'YMD'        : 'Datetime',           # Ngày/tháng/năm (dd/mm/yyyy)\n",
    "    'YEAR'       : 'Numerical',          # Năm (năm)\n",
    "    'MONTH'      : 'Numerical',          # Tháng (1-12)\n",
    "    'DAY'        : 'Numerical',          # Ngày (1-31)\n",
    "\n",
    "    'TEMP_max'   : 'Numerical',          # Nhiệt độ cực đại trong ngày (°C)\n",
    "    'TEMP_ave'   : 'Numerical',          # Nhiệt độ trung bình trong ngày (°C)\n",
    "    'DEW_ave'    : 'Numerical',          # Điểm sương trung bình trong ngày (°C)\n",
    "    'DEW_max'    : 'Numerical',          # Điểm sương cực đại trong ngày (°C)\n",
    "    'RH_ave'     : 'Numerical',          # Độ ẩm tương đối trung bình trong ngày (%)\n",
    "    'RH_max'     : 'Numerical',          # Độ ẩm tương đối cực đại trong ngày (%)\n",
    "    'AT_ave'     : 'Numerical',          # Nhiệt độ cảm nhận trung bình trong ngày (°C)\n",
    "    'AT_max'     : 'Numerical'           # Nhiệt độ cảm nhận cực đại trong ngày (°C)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">1.3. Goals</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../image/Ảnh chụp màn hình 2025-06-24 210616.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">2. Import Libraries</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">2.1. Configuration and display settings</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")  # đường dẫn đến thư mục chứa src\n",
    "\n",
    "from src.utilities import(config, \n",
    "                          dataset, \n",
    "                          features, \n",
    "                          plots)\n",
    "\n",
    "from src.models import(anomaly_models,\n",
    "                       forecasting_models,\n",
    "                       model_utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">2.2. Required Python packages</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import metpy\n",
    "import metpy.calc\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from metpy.units import units\n",
    "from copy        import deepcopy\n",
    "\n",
    "from sklearn.ensemble  import IsolationForest\n",
    "from prophet           import Prophet\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.cluster   import AgglomerativeClustering\n",
    "from sklearn.cluster   import DBSCAN\n",
    "from hdbscan           import HDBSCAN\n",
    "from joblib            import Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">3.Data Collecting</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">3.1. Chuyển các file .grib sang file .csv hoặc .xlsx\t, hoặc .xls (file Excel)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_CaMau = dict({\"CaMau_SurPres\"     : list([\"../data/raw/Ca Mau/.grib/CM_SurPres_1990_2002.grib\",\n",
    "                                              \"../data/raw/Ca Mau/.grib/CM_SurPres_2003_2015.grib\",\n",
    "                                              \"../data/raw/Ca Mau/.grib/CM_SurPres_2016_2024.grib\"]),\n",
    "                  \"CaMau_TotalCloud\"  : list([\"../data/raw/Ca Mau/.grib/CM_TotalCloud_1990_2002.grib\",\n",
    "                                              \"../data/raw/Ca Mau/.grib/CM_TotalCloud_2003_2015.grib\",\n",
    "                                              \"../data/raw/Ca Mau/.grib/CM_TotalCloud_2016_2024.grib\"]),\n",
    "                  \"CaMau_TotalPre\"    : list([\"../data/raw/Ca Mau/.grib/CM_TotalPre_1990_2002.grib\",\n",
    "                                              \"../data/raw/Ca Mau/.grib/CM_TotalPre_2003_2015.grib\",\n",
    "                                              \"../data/raw/Ca Mau/.grib/CM_TotalPre_2016_2024.grib\"]),\n",
    "                  \"CaMau_u10\"         : list([\"../data/raw/Ca Mau/.grib/CM_u10_1990_2002.grib\",\n",
    "                                              \"../data/raw/Ca Mau/.grib/CM_u10_2003_2015.grib\",\n",
    "                                              \"../data/raw/Ca Mau/.grib/CM_u10_2016_2024.grib\"]),\n",
    "                  \"CaMau_v10\"         : list([\"../data/raw/Ca Mau/.grib/CM_v10_1990_2002.grib\",\n",
    "                                              \"../data/raw/Ca Mau/.grib/CM_v10_2003_2015.grib\",\n",
    "                                              \"../data/raw/Ca Mau/.grib/CM_v10_2016_2024.grib\"])})\n",
    "\n",
    "\n",
    "src_DongHoi = dict({\"DongHoi_SurPres\"     : list([\"../data/raw/Dong Hoi/.grib/DH_SurfacePre_1990_2002.grib\",\n",
    "                                                  \"../data/raw/Dong Hoi/.grib/DH_SurfacePre_2003_2015.grib\",\n",
    "                                                  \"../data/raw/Dong Hoi/.grib/DH_SurfacePre_2016_2024.grib\"]),\n",
    "                    \"DongHoi_TotalCloud\"  : list([\"../data/raw/Dong Hoi/.grib/DH_TotalCloud_1990_2002.grib\",\n",
    "                                                  \"../data/raw/Dong Hoi/.grib/DH_TotalCloud_2003_2015.grib\",\n",
    "                                                  \"../data/raw/Dong Hoi/.grib/DH_TotalCloud_2016_2024.grib\"]),\n",
    "                    \"DongHoi_TotalPre\"    : list([\"../data/raw/Dong Hoi/.grib/DH_TotalPre_1990_2002.grib\",\n",
    "                                                  \"../data/raw/Dong Hoi/.grib/DH_TotalPre_2003_2015.grib\",\n",
    "                                                  \"../data/raw/Dong Hoi/.grib/DH_TotalPre_2016_2024.grib\"]),\n",
    "                    \"DongHoi_u10\"         : list([\"../data/raw/Dong Hoi/.grib/DH_u10_1990_2002.grib\",\n",
    "                                                  \"../data/raw/Dong Hoi/.grib/DH_u10_2003_2015.grib\",\n",
    "                                                  \"../data/raw/Dong Hoi/.grib/DH_u10_2016_2024.grib\"]),\n",
    "                    \"DongHoi_v10\"         : list([\"../data/raw/Dong Hoi/.grib/DH_v10_1990_2002.grib\",\n",
    "                                                  \"../data/raw/Dong Hoi/.grib/DH_v10_2003_2015.grib\",\n",
    "                                                  \"../data/raw/Dong Hoi/.grib/DH_v10_2016_2024.grib\"])})\n",
    "\n",
    "\n",
    "src_NoiBai = dict({\"NB_SurPres\"       : list([\"../data/raw/Noi Bai/.grib/NB_SurPres_1990_2002.grib\",\n",
    "                                              \"../data/raw/Noi Bai/.grib/NB_SurPres_2003_2015.grib\",\n",
    "                                              \"../data/raw/Noi Bai/.grib/NB_SurPres_2016_2024.grib\"]),\n",
    "                   \"NB_TotalCloud\"    : list([\"../data/raw/Noi Bai/.grib/NB_TotalCloud_1990_2002.grib\",\n",
    "                                              \"../data/raw/Noi Bai/.grib/NB_TotalCloud_2003_2015.grib\",\n",
    "                                              \"../data/raw/Noi Bai/.grib/NB_TotalCloud_2016_2024.grib\"]),\n",
    "                   \"NB_TotalPre\"      : list([\"../data/raw/Noi Bai/.grib/NB_TotalPre_1990_2002.grib\",\n",
    "                                              \"../data/raw/Noi Bai/.grib/NB_TotalPre_2003_2015.grib\",\n",
    "                                              \"../data/raw/Noi Bai/.grib/NB_TotalPre_2016_2024.grib\"]),\n",
    "                   \"NB_u10\"           : list([\"../data/raw/Noi Bai/.grib/NB_u10_1990_2002.grib\",\n",
    "                                              \"../data/raw/Noi Bai/.grib/NB_u10_2003_2015.grib\",\n",
    "                                              \"../data/raw/Noi Bai/.grib/NB_u10_2016_2024.grib\"]),\n",
    "                   \"NB_v10\"           : list([\"../data/raw/Noi Bai/.grib/NB_v10_1990_2002.grib\",\n",
    "                                              \"../data/raw/Noi Bai/.grib/NB_v10_2003_2015.grib\",\n",
    "                                              \"../data/raw/Noi Bai/.grib/NB_v10_2016_2024.grib\"])})\n",
    "\n",
    "\n",
    "src_QuyNhon = dict({\"QN_SurPres\"      : list([\"../data/raw/Quy Nhon/.grib/QN_SurPres_1990_2002.grib\",\n",
    "                                              \"../data/raw/Quy Nhon/.grib/QN_SurPres_2003_2015.grib\",\n",
    "                                              \"../data/raw/Quy Nhon/.grib/QN_SurPres_2016_2024.grib\"]),\n",
    "                    \"QN_TotalCloud\"   : list([\"../data/raw/Quy Nhon/.grib/QN_TotalCloud_1990_2002.grib\",\n",
    "                                              \"../data/raw/Quy Nhon/.grib/QN_TotalCloud_2003_2015.grib\",\n",
    "                                              \"../data/raw/Quy Nhon/.grib/QN_TotalCloud_2016_2024.grib\"]),\n",
    "                    \"QN_TotalPre\"     : list([\"../data/raw/Quy Nhon/.grib/QN_TotalPre_1990_2002.grib\",\n",
    "                                              \"../data/raw/Quy Nhon/.grib/QN_TotalPre_2003_2015.grib\",\n",
    "                                              \"../data/raw/Quy Nhon/.grib/QN_TotalPre_2016_2024.grib\"]),\n",
    "                    \"QN_u10\"          : list([\"../data/raw/Quy Nhon/.grib/QN_u10_1990_2002.grib\",\n",
    "                                              \"../data/raw/Quy Nhon/.grib/QN_u10_2003_2015.grib\",\n",
    "                                              \"../data/raw/Quy Nhon/.grib/QN_u10_2016_2024.grib\"]),\n",
    "                    \"QN_v10\"          : list([\"../data/raw/Quy Nhon/.grib/QN_v10_1990_2002.grib\",\n",
    "                                              \"../data/raw/Quy Nhon/.grib/QN_v10_2003_2015.grib\",\n",
    "                                              \"../data/raw/Quy Nhon/.grib/QN_v10_2016_2024.grib\"])})\n",
    "\n",
    "\n",
    "src_ThanhHoa = dict({\"TH_SurPres\"         : list([\"../data/raw/Thanh Hoa/.grib/TH_SurPres_1990_2002.grib\",\n",
    "                                                  \"../data/raw/Thanh Hoa/.grib/TH_SurPres_2003_2015.grib\",\n",
    "                                                  \"../data/raw/Thanh Hoa/.grib/TH_SurPres_2016_2024.grib\"]),\n",
    "                     \"TH_TotalCloud\"      : list([\"../data/raw/Thanh Hoa/.grib/TH_TotalCloud_1990_2002.grib\",\n",
    "                                                  \"../data/raw/Thanh Hoa/.grib/TH_TotalCloud_2003_2015.grib\",\n",
    "                                                  \"../data/raw/Thanh Hoa/.grib/TH_TotalCloud_2016_2024.grib\"]),\n",
    "                     \"TH_TotalPre\"        : list([\"../data/raw/Thanh Hoa/.grib/TH_TotalPre_1990_2002.grib\",\n",
    "                                                  \"../data/raw/Thanh Hoa/.grib/TH_TotalPre_2003_2015.grib\",\n",
    "                                                  \"../data/raw/Thanh Hoa/.grib/TH_TotalPre_2016_2024.grib\"]),\n",
    "                     \"TH_u10\"             : list([\"../data/raw/Thanh Hoa/.grib/TH_u10_1990_2002.grib\",\n",
    "                                                  \"../data/raw/Thanh Hoa/.grib/TH_u10_2003_2015.grib\",\n",
    "                                                  \"../data/raw/Thanh Hoa/.grib/TH_u10_2016_2024.grib\"]),\n",
    "                     \"TH_v10\"             : list([\"../data/raw/Thanh Hoa/.grib/TH_v10_1990_2002.grib\",\n",
    "                                                  \"../data/raw/Thanh Hoa/.grib/TH_v10_2003_2015.grib\",\n",
    "                                                  \"../data/raw/Thanh Hoa/.grib/TH_v10_2016_2024.grib\"])})\n",
    "\n",
    "\n",
    "src_TSN = dict({\"TSN_SurPres\"         : list([\"../data/raw/TSN/.grib/TSN_Press_1990_2002.grib\",\n",
    "                                              \"../data/raw/TSN/.grib/TSN_Press_2003_2015.grib\",\n",
    "                                              \"../data/raw/TSN/.grib/TSN_Press_2016_2024.grib\"]),\n",
    "                \"TSN_TotalCloud\"      : list([\"../data/raw/TSN/.grib/TSN_TotalCloud_1990_2002.grib\",\n",
    "                                              \"../data/raw/TSN/.grib/TSN_TotalCloud_2003_2015.grib\",\n",
    "                                              \"../data/raw/TSN/.grib/TSN_TotalCloud_2016_2024.grib\"]),\n",
    "                \"TSN_TotalPre\"        : list([\"../data/raw/TSN/.grib/TSN_TotalPre_1990_2002.grib\",\n",
    "                                              \"../data/raw/TSN/.grib/TSN_TotalPre_2003_2015.grib\",\n",
    "                                              \"../data/raw/TSN/.grib/TSN_TotalPre_2016_2024.grib\"]),\n",
    "                \"TSN_u10\"             : list([\"../data/raw/TSN/.grib/TSN_u10_1990_2002.grib\",\n",
    "                                              \"../data/raw/TSN/.grib/TSN_u10_2003_2015.grib\",\n",
    "                                              \"../data/raw/TSN/.grib/TSN_u10_2016-2023.grib\"]),\n",
    "                \"TSN_v10\"             : list([\"../data/raw/TSN/.grib/TSN_v10_1990_2002.grib\",\n",
    "                                              \"../data/raw/TSN/.grib/TSN_v10_2003_2015.grib\",\n",
    "                                              \"../data/raw/TSN/.grib/TSN_v10_2016_2023.grib\"])})\n",
    "\n",
    "src = src_CaMau | src_DongHoi | src_NoiBai | src_QuyNhon  | src_ThanhHoa | src_TSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name, file_list in src.items():\n",
    "    for file_path in file_list:\n",
    "        # Lấy thư mục chứa file .grib\n",
    "        grib_folder = os.path.dirname(file_path)\n",
    "\n",
    "        # Thư mục .csv cùng cấp với .grib\n",
    "        csv_folder = os.path.join(os.path.dirname(grib_folder), '.csv')\n",
    "        os.makedirs(csv_folder, exist_ok=True)\n",
    "\n",
    "        # Tên file .csv trùng với tên file .grib nhưng đổi đuôi\n",
    "        file_name = os.path.basename(file_path).replace(\".grib\", \".csv\")\n",
    "\n",
    "        # File đích nằm trong thư mục .csv\n",
    "        des_file = os.path.join(csv_folder, file_name)\n",
    "\n",
    "        # Nếu chưa tồn tại mới convert\n",
    "        if not os.path.exists(des_file):\n",
    "            print(f\"~> Convert: {file_path} → {des_file}\")\n",
    "            dataset.grib_to_csv(file_path, des_file)\n",
    "        else:\n",
    "            print(f\"✔️ Tồn tại: {des_file}, bỏ qua.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">3.2. Tiếp theo, đổi thời gian trong cột \"time\" (UTC+00) của file vừa mới chuyển về giờ Việt Nam (UTC+07)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name, file_list in src.items():\n",
    "    print(f\"\\n🔸 Đang xử lý biến: {var_name}\")\n",
    "\n",
    "    # Lấy thư mục chứa file csv gốc (từ file đầu tiên thôi)\n",
    "    csv_path_example = file_list[0].replace(\".grib\", \".csv\")\n",
    "    csv_folder = os.path.dirname(csv_path_example)\n",
    "    merged_folder = os.path.join(csv_folder, \"merged\")\n",
    "    os.makedirs(merged_folder, exist_ok=True)\n",
    "\n",
    "    # Tên file đích\n",
    "    out_path = os.path.join(merged_folder, f\"{var_name}_merged.csv\")\n",
    "\n",
    "    # Nếu file đích đã tồn tại thì bỏ qua\n",
    "    if os.path.exists(out_path):\n",
    "        print(f\"⚠️ Đã tồn tại: {out_path}, bỏ qua.\")\n",
    "        continue\n",
    "\n",
    "    df_list = []  # List lưu các DataFrame để concat\n",
    "\n",
    "    for file_path in file_list:\n",
    "        csv_path = file_path.replace(\".grib\", \".csv\")\n",
    "\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"❌ Không tìm thấy file: {csv_path}, bỏ qua.\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df = features.utc_to_vietnam(df, column_name=\"time\")\n",
    "        df_list.append(df)\n",
    "        print(f\"✔️ Đã xử lý: {csv_path}\")\n",
    "\n",
    "    # Nếu không có file nào thì bỏ qua merge\n",
    "    if not df_list:\n",
    "        print(f\"⚠️ Không có dữ liệu cho {var_name}, bỏ qua.\")\n",
    "        continue\n",
    "\n",
    "    # Nối tất cả DataFrame lại với nhau\n",
    "    df_merged = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # Lưu kết quả\n",
    "    df_merged.to_csv(out_path, index=False)\n",
    "    print(f\"✅ Đã lưu: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">3.3. Ghép các file khí tượng vào file nhiệt độ để thành data đầy đủ cho model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các cách loại bỏ dòng trùng trong pandas\n",
    "\n",
    "<table border=\"1\" cellpadding=\"6\" cellspacing=\"0\">\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Cách dùng</th>\n",
    "<th>Mô tả</th>\n",
    "<th>Ví dụ</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "\n",
    "<tr>\n",
    "<td><code>df.drop_duplicates()</code></td>\n",
    "<td>Loại bỏ toàn bộ các dòng bị trùng tất cả giá trị ở tất cả các cột.</td>\n",
    "<td><pre>df.drop_duplicates()</pre></td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td><code>df.drop_duplicates(subset=[\"col1\"])</code></td>\n",
    "<td>Chỉ kiểm tra trùng lặp theo cột <code>col1</code>, giữ lại dòng đầu tiên.</td>\n",
    "<td><pre>df.drop_duplicates(subset=[\"time\"])</pre></td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td><code>df.drop_duplicates(keep=\"last\")</code></td>\n",
    "<td>Giữ lại dòng cuối cùng trong số các dòng bị trùng.</td>\n",
    "<td><pre>df.drop_duplicates(subset=[\"time\"], keep=\"last\")</pre></td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td><code>df.drop_duplicates(ignore_index=True)</code></td>\n",
    "<td>Xóa dòng trùng, đồng thời reset lại chỉ số (index).</td>\n",
    "<td><pre>df.drop_duplicates(ignore_index=True)</pre></td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td><code>df.loc[~df.index.duplicated() ]</code></td>\n",
    "<td>Nếu <code>time</code> đang là index, loại bỏ các index trùng, giữ lại dòng đầu tiên.</td>\n",
    "<td><pre>df.loc[~df.index.duplicated()]</pre></td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td><code>df.loc[~df.index.duplicated(keep=\"last\") ]</code></td>\n",
    "<td>Loại bỏ các index trùng, giữ lại dòng cuối cùng.</td>\n",
    "<td><pre>df.loc[~df.index.duplicated(keep=\"last\")]</pre></td>\n",
    "</tr>\n",
    "\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_key = [\"time\", \"latitude\", \"longitude\", \"number\", \"step\", \"surface\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Ca Mau</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SurPres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CaMau_SurPres = pd.read_csv(\"../data/raw/Ca Mau/.csv/merged/CaMau_SurPres_merged.csv\")\n",
    "df_CaMau_SurPres = df_CaMau_SurPres.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols             = primary_key + [\"sp\"]\n",
    "df_CaMau_SurPres = df_CaMau_SurPres[cols]\n",
    "\n",
    "df_CaMau_SurPres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CaMau_TotalCloud = pd.read_csv(\"../data/raw/Ca Mau/.csv/merged/CaMau_TotalCloud_merged.csv\")\n",
    "df_CaMau_TotalCloud = df_CaMau_TotalCloud.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols                = primary_key + [\"tcc\"]\n",
    "df_CaMau_TotalCloud = df_CaMau_TotalCloud[cols]\n",
    "\n",
    "df_CaMau_TotalCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalPre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CaMau_TotalPre = pd.read_csv(\"../data/raw/Ca Mau/.csv/merged/CaMau_TotalPre_merged.csv\")\n",
    "df_CaMau_TotalPre = df_CaMau_TotalPre.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols              = primary_key + [\"tp\"]\n",
    "df_CaMau_TotalPre = df_CaMau_TotalPre[cols]\n",
    "\n",
    "\n",
    "df_CaMau_TotalPre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Chuyển time = time + step\n",
    "    Chuyển step = 0 days\n",
    "\"\"\"\n",
    "df_CaMau_TotalPre[\"time\"] = pd.to_datetime(df_CaMau_TotalPre[\"time\"]) + pd.to_timedelta(df_CaMau_TotalPre[\"step\"])\n",
    "df_CaMau_TotalPre[\"step\"] = \"0 days\"\n",
    "\n",
    "df_CaMau_TotalPre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "u10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CaMau_u10 = pd.read_csv(\"../data/raw/Ca Mau/.csv/merged/CaMau_u10_merged.csv\")\n",
    "df_CaMau_u10 = df_CaMau_u10.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols         = primary_key + [\"u10\"]\n",
    "df_CaMau_u10 = df_CaMau_u10[cols]\n",
    "\n",
    "df_CaMau_u10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CaMau_v10 = pd.read_csv(\"../data/raw/Ca Mau/.csv/merged/CaMau_v10_merged.csv\")\n",
    "df_CaMau_v10 = df_CaMau_v10.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols         = primary_key + [\"v10\"]\n",
    "df_CaMau_v10 = df_CaMau_v10[cols]\n",
    "\n",
    "df_CaMau_v10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = list([df_CaMau_SurPres, df_CaMau_TotalCloud, df_CaMau_TotalPre, df_CaMau_u10, df_CaMau_v10])\n",
    "\n",
    "for df in datasets:\n",
    "    df = features.utc_to_vietnam(df,\"time\")\n",
    "\n",
    "df_merged = dataset.merge_df(datasets = datasets,\n",
    "                             on       = primary_key, \n",
    "                             how      = \"outer\")\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compare_df(df_A = df_merged,\n",
    "                   df_B = df_CaMau_TotalPre,\n",
    "                   key  = primary_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(\"../data/raw/Ca Mau/.csv/merged/CaMau_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Dong Hoi</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SurPres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DH_SurPres = pd.read_csv(\"../data/raw//Dong Hoi/.csv/merged/DongHoi_SurPres_merged.csv\")\n",
    "df_DH_SurPres = df_DH_SurPres.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols             = primary_key + [\"sp\"]\n",
    "df_DH_SurPres = df_DH_SurPres[cols]\n",
    "\n",
    "df_DH_SurPres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DH_TotalCloud = pd.read_csv(\"../data/raw/Dong Hoi/.csv/merged/DongHoi_TotalCloud_merged.csv\")\n",
    "df_DH_TotalCloud = df_DH_TotalCloud.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols                = primary_key + [\"tcc\"]\n",
    "df_DH_TotalCloud = df_DH_TotalCloud[cols]\n",
    "\n",
    "df_DH_TotalCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalPre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DH_TotalPre = pd.read_csv(\"../data/raw/Dong Hoi/.csv/merged/DongHoi_TotalPre_merged.csv\")\n",
    "df_DH_TotalPre = df_DH_TotalPre.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols           = primary_key + [\"tp\"]\n",
    "df_DH_TotalPre = df_DH_TotalPre[cols]\n",
    "\n",
    "df_DH_TotalPre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Chuyển time = time + step\n",
    "    Chuyển step = 0 days\n",
    "\"\"\"\n",
    "df_DH_TotalPre[\"time\"] = pd.to_datetime(df_DH_TotalPre[\"time\"]) + pd.to_timedelta(df_DH_TotalPre[\"step\"])\n",
    "df_DH_TotalPre[\"step\"] = \"0 days\"\n",
    "\n",
    "df_DH_TotalPre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "u10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DH_u10 = pd.read_csv(\"../data/raw/Dong Hoi/.csv/merged/DongHoi_u10_merged.csv\")\n",
    "df_DH_u10 = df_DH_u10.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols         = primary_key + [\"u10\"]\n",
    "df_DH_u10 = df_DH_u10[cols]\n",
    "\n",
    "df_DH_u10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DH_v10 = pd.read_csv(\"../data/raw/Dong Hoi/.csv/merged/DongHoi_v10_merged.csv\")\n",
    "df_DH_v10 = df_DH_v10.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols         = primary_key + [\"v10\"]\n",
    "df_DH_v10 = df_DH_v10[cols]\n",
    "\n",
    "df_DH_v10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = list([df_DH_SurPres, df_DH_TotalCloud, df_DH_TotalPre, df_DH_u10, df_DH_v10])\n",
    "\n",
    "for df in datasets:\n",
    "    df = features.utc_to_vietnam(df,\"time\")\n",
    "\n",
    "df_merged = dataset.merge_df(datasets = datasets,\n",
    "                             on       = primary_key, \n",
    "                             how      = \"outer\")\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compare_df(df_A = df_merged,\n",
    "                   df_B = df_DH_TotalPre,\n",
    "                   key  = primary_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(\"../data/raw/Dong Hoi/.csv/merged/DH_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Noi Bai</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SurPres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NB_SurPres = pd.read_csv(\"../data/raw/Noi Bai/.csv/merged/NB_SurPres_merged.csv\")\n",
    "df_NB_SurPres = df_NB_SurPres.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols          = primary_key + [\"sp\"]\n",
    "df_NB_SurPres = df_NB_SurPres[cols]\n",
    "\n",
    "df_NB_SurPres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NB_TotalCloud = pd.read_csv(\"../data/raw/Noi Bai/.csv/merged/NB_TotalCloud_merged.csv\")\n",
    "df_NB_TotalCloud = df_NB_TotalCloud.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols             = primary_key + [\"tcc\"]\n",
    "df_NB_TotalCloud = df_NB_TotalCloud[cols]\n",
    "\n",
    "df_NB_TotalCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalPre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NB_TotalPre = pd.read_csv(\"../data/raw/Noi Bai/.csv/merged/NB_TotalPre_merged.csv\")\n",
    "# df_CaMau_TotalPre = df_CaMau_TotalPre.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols           = primary_key + [\"tp\"]\n",
    "df_NB_TotalPre = df_NB_TotalPre[cols]\n",
    "\n",
    "df_NB_TotalPre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Chuyển time = time + step\n",
    "    Chuyển step = 0 days\n",
    "\"\"\"\n",
    "df_NB_TotalPre[\"time\"] = pd.to_datetime(df_NB_TotalPre[\"time\"]) + pd.to_timedelta(df_NB_TotalPre[\"step\"])\n",
    "df_NB_TotalPre[\"step\"] = \"0 days\"\n",
    "\n",
    "df_NB_TotalPre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "u10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NB_u10 = pd.read_csv(\"../data/raw/Noi Bai/.csv/merged/NB_u10_merged.csv\")\n",
    "df_NB_u10 = df_NB_u10.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols      = primary_key + [\"u10\"]\n",
    "df_NB_u10 = df_NB_u10[cols]\n",
    "\n",
    "df_NB_u10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NB_v10 = pd.read_csv(\"../data/raw/Noi Bai/.csv/merged/NB_v10_merged.csv\")\n",
    "df_NB_v10 = df_NB_v10.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols      = primary_key + [\"v10\"]\n",
    "df_NB_v10 = df_NB_v10[cols]\n",
    "\n",
    "df_NB_v10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = list([df_NB_SurPres, df_NB_TotalCloud, df_NB_TotalPre, df_NB_u10, df_NB_v10])\n",
    "\n",
    "for df in datasets:\n",
    "    df = features.utc_to_vietnam(df,\"time\")\n",
    "\n",
    "df_merged = dataset.merge_df(datasets = datasets,\n",
    "                             on       = primary_key, \n",
    "                             how      = \"outer\")\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compare_df(df_A = df_merged,\n",
    "                   df_B = df_NB_TotalPre,\n",
    "                   key  = primary_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(\"../data/raw/Noi Bai/.csv/merged/NB_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Quy Nhon</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SurPres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_QN_SurPres = pd.read_csv(\"../data/raw/Quy Nhon/.csv/merged/QN_SurPres_merged.csv\")\n",
    "df_QN_SurPres = df_QN_SurPres.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols          = primary_key + [\"sp\"]\n",
    "df_QN_SurPres = df_QN_SurPres[cols]\n",
    "\n",
    "df_QN_SurPres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_QN_TotalCloud = pd.read_csv(\"../data/raw/Quy Nhon/.csv/merged/QN_TotalCloud_merged.csv\")\n",
    "df_QN_TotalCloud = df_QN_TotalCloud.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols             = primary_key + [\"tcc\"]\n",
    "df_QN_TotalCloud = df_QN_TotalCloud[cols]\n",
    "\n",
    "df_QN_TotalCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalPre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_QN_TotalPre = pd.read_csv(\"../data/raw/Quy Nhon/.csv/merged/QN_TotalPre_merged.csv\")\n",
    "df_QN_TotalPre = df_QN_TotalPre.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols           = primary_key + [\"tp\"]\n",
    "df_QN_TotalPre = df_QN_TotalPre[cols]\n",
    "\n",
    "df_QN_TotalPre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Chuyển time = time + step\n",
    "    Chuyển step = 0 days\n",
    "\"\"\"\n",
    "df_QN_TotalPre[\"time\"] = pd.to_datetime(df_QN_TotalPre[\"time\"]) + pd.to_timedelta(df_QN_TotalPre[\"step\"])\n",
    "df_QN_TotalPre[\"step\"] = \"0 days\"\n",
    "\n",
    "df_QN_TotalPre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "u10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_QN_u10 = pd.read_csv(\"../data/raw/Quy Nhon/.csv/merged/QN_u10_merged.csv\")\n",
    "df_QN_u10 = df_QN_u10.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols      = primary_key + [\"u10\"]\n",
    "df_QN_u10 = df_QN_u10[cols]\n",
    "\n",
    "df_QN_u10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_QN_v10 = pd.read_csv(\"../data/raw/Quy Nhon/.csv/merged/QN_v10_merged.csv\")\n",
    "df_QN_v10 = df_QN_v10.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols      = primary_key + [\"v10\"]\n",
    "df_QN_v10 = df_QN_v10[cols]\n",
    "\n",
    "df_QN_v10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = list([df_QN_SurPres, df_QN_TotalCloud, df_QN_TotalPre, df_QN_u10, df_QN_v10])\n",
    "\n",
    "for df in datasets:\n",
    "    df = features.utc_to_vietnam(df,\"time\")\n",
    "\n",
    "df_merged = dataset.merge_df(datasets = datasets,\n",
    "                             on       = primary_key, \n",
    "                             how      = \"outer\")\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compare_df(df_A = df_merged,\n",
    "                   df_B = df_QN_TotalPre,\n",
    "                   key  = primary_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(\"../data/raw/Quy Nhon/.csv/merged/QN_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Thanh Hoa</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SurPres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TH_SurPres = pd.read_csv(\"../data/raw/Thanh Hoa/.csv/merged/TH_SurPres_merged.csv\")\n",
    "df_TH_SurPres = df_TH_SurPres.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols             = primary_key + [\"sp\"]\n",
    "df_TH_SurPres = df_TH_SurPres[cols]\n",
    "\n",
    "df_TH_SurPres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TH_TotalCloud = pd.read_csv(\"../data/raw/Thanh Hoa/.csv/merged/TH_TotalCloud_merged.csv\")\n",
    "df_TH_TotalCloud = df_TH_TotalCloud.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols                = primary_key + [\"tcc\"]\n",
    "df_TH_TotalCloud = df_TH_TotalCloud[cols]\n",
    "\n",
    "df_TH_TotalCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalPre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TH_TotalPre = pd.read_csv(\"../data/raw/Thanh Hoa/.csv/merged/TH_TotalPre_merged.csv\")\n",
    "df_TH_TotalPre = df_TH_TotalPre.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols              = primary_key + [\"tp\"]\n",
    "df_TH_TotalPre = df_TH_TotalPre[cols]\n",
    "\n",
    "df_TH_TotalPre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Chuyển time = time + step\n",
    "    Chuyển step = 0 days\n",
    "\"\"\"\n",
    "df_TH_TotalPre[\"time\"] = pd.to_datetime(df_TH_TotalPre[\"time\"]) + pd.to_timedelta(df_TH_TotalPre[\"step\"])\n",
    "df_TH_TotalPre[\"step\"] = \"0 days\"\n",
    "\n",
    "df_TH_TotalPre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "u10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TH_u10 = pd.read_csv(\"../data/raw/Thanh Hoa/.csv/merged/TH_u10_merged.csv\")\n",
    "df_TH_u10 = df_TH_u10.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols         = primary_key + [\"u10\"]\n",
    "df_TH_u10 = df_TH_u10[cols]\n",
    "\n",
    "df_TH_u10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TH_v10 = pd.read_csv(\"../data/raw/Thanh Hoa/.csv/merged/TH_v10_merged.csv\")\n",
    "df_TH_v10 = df_TH_v10.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols         = primary_key + [\"v10\"]\n",
    "df_TH_v10 = df_TH_v10[cols]\n",
    "\n",
    "df_TH_v10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = list([df_TH_SurPres, df_TH_TotalCloud, df_TH_TotalPre, df_TH_u10, df_TH_v10])\n",
    "\n",
    "for df in datasets:\n",
    "    df = features.utc_to_vietnam(df,\"time\")\n",
    "\n",
    "df_merged = dataset.merge_df(datasets = datasets,\n",
    "                             on       = primary_key, \n",
    "                             how      = \"outer\")\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compare_df(df_A = df_merged,\n",
    "                   df_B = df_TH_TotalPre,\n",
    "                   key  = primary_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(\"../data/raw/Thanh Hoa/.csv/merged/TH_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">TSN</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SurPres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TSN_SurPres = pd.read_csv(\"../data/raw/TSN/.csv/merged/TSN_SurPres_merged.csv\")\n",
    "df_TSN_SurPres = df_TSN_SurPres.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols           = primary_key + [\"sp\"]\n",
    "df_TSN_SurPres = df_TSN_SurPres[cols]\n",
    "\n",
    "df_TSN_SurPres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TSN_TotalCloud = pd.read_csv(\"../data/raw/TSN/.csv/merged/TSN_TotalCloud_merged.csv\")\n",
    "df_TSN_TotalCloud = df_TSN_TotalCloud.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols              = primary_key + [\"tcc\"]\n",
    "df_TSN_TotalCloud = df_TSN_TotalCloud[cols]\n",
    "\n",
    "df_TSN_TotalCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalPre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TSN_TotalPre = pd.read_csv(\"../data/raw/TSN/.csv/merged/TSN_TotalPre_merged.csv\")\n",
    "df_TSN_TotalPre = df_TSN_TotalPre.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols            = primary_key + [\"tp\"]\n",
    "df_TSN_TotalPre = df_TSN_TotalPre[cols]\n",
    "\n",
    "df_TSN_TotalPre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Chuyển time = time + step\n",
    "    Chuyển step = 0 days\n",
    "\"\"\"\n",
    "df_TSN_TotalPre[\"time\"] = pd.to_datetime(df_TSN_TotalPre[\"time\"]) + pd.to_timedelta(df_TSN_TotalPre[\"step\"])\n",
    "df_TSN_TotalPre[\"step\"] = \"0 days\"\n",
    "\n",
    "df_TSN_TotalPre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "u10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TSN_u10 = pd.read_csv(\"../data/raw/TSN/.csv/merged/TSN_u10_merged.csv\")\n",
    "df_TSN_u10 = df_TSN_u10.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols       = primary_key + [\"u10\"]\n",
    "df_TSN_u10 = df_TSN_u10[cols]\n",
    "\n",
    "df_TSN_u10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TSN_v10 = pd.read_csv(\"../data/raw/TSN/.csv/merged/TSN_v10_merged.csv\")\n",
    "df_TSN_v10 = df_TSN_v10.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# Sắp xếp lại thứ tự cột\n",
    "cols       = primary_key + [\"v10\"]\n",
    "df_TSN_v10 = df_TSN_v10[cols]\n",
    "\n",
    "df_TSN_v10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = list([df_TSN_SurPres, df_TSN_TotalCloud, df_TSN_TotalPre, df_TSN_u10, df_TSN_v10])\n",
    "\n",
    "for df in datasets:\n",
    "    df = features.utc_to_vietnam(df,\"time\")\n",
    "\n",
    "df_merged = dataset.merge_df(datasets = datasets,\n",
    "                             on       = primary_key, \n",
    "                             how      = \"outer\")\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compare_df(df_A = df_merged,\n",
    "                   df_B = df_TSN_TotalPre,\n",
    "                   key  = primary_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(\"../data/raw/TSN/.csv/merged/TSN_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">4.Data Cleaning</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các biến số chính trong tập dữ liệu:\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Tên cột dữ liệu</th>\n",
    "<th>Ý nghĩa</th>\n",
    "<th>Đơn vị đo</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td><code>sp</code></td><td>Áp suất khí quyển tại mặt đất</td><td>Pa</td></tr>\n",
    "<tr><td><code>tcc</code></td><td>Tổng độ che phủ mây toàn phần</td><td>%</td></tr>\n",
    "<tr><td><code>tp</code></td><td>Lượng mưa tích lũy</td><td>mm</td></tr>\n",
    "<tr><td><code>u10</code></td><td>Thành phần vận tốc gió theo trục Đông-Tây ở độ cao 10m</td><td>m/s</td></tr>\n",
    "<tr><td><code>v10</code></td><td>Thành phần vận tốc gió theo trục Bắc-Nam ở độ cao 10m</td><td>m/s</td></tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [\"sp\", \"tcc\", \"tp\", \"u10\", \"v10\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">4.1. Handling duplicate values</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = list([\"../data/raw/Ca Mau/.csv/merged/CaMau_merged.csv\",\n",
    "            \"../data/raw/Dong Hoi/.csv/merged/DH_merged.csv\",\n",
    "            \"../data/raw/Noi Bai/.csv/merged/NB_merged.csv\",\n",
    "            \"../data/raw/Quy Nhon/.csv/merged/QN_merged.csv\",\n",
    "            \"../data/raw/Thanh Hoa/.csv/merged/TH_merged.csv\",\n",
    "            \"../data/raw/TSN/.csv/merged/TSN_merged.csv\"])\n",
    "\n",
    "station = dict({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, val in enumerate(src):\n",
    "    df = pd.read_csv(val, parse_dates=[\"time\"])\n",
    "    station_name = os.path.basename(val).replace(\"_merged.csv\", \"\")\n",
    "    station[station_name] = df  # lưu vào dict theo tên trạm\n",
    "    print(f\"🔸 Trạm: {station_name}\")\n",
    "\n",
    "    dataset.ProportionDuplicate_aproach1(df[\"time\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in station.items():\n",
    "    print(f\"🔸 Trạm: {name}\")\n",
    "\n",
    "    station[name] = dataset.HandleDuplicate_drop(data   = df,\n",
    "                                                 subset = \"time\",\n",
    "                                                 keep   = \"first\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in station.items():\n",
    "    print(f\"🔸 Trạm: {name}\")\n",
    "    \n",
    "    dataset.ProportionDuplicate_aproach1(df[\"time\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">4.2. Handling missing values</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Index</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://datastud.dev/posts/time-series-resample\">Filling Gaps in Time Series Data</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in station.items():\n",
    "    print(f\"🔸 Trạm: {name}\")\n",
    "\n",
    "    gap_df = dataset.find_time_gaps(data       = df.set_index(\"time\"), \n",
    "                                    start_time = \"1990-01-01 00:00:00+07:00\", \n",
    "                                    end_time   = \"2025-01-01 00:00:00+07:00\",\n",
    "                                    freq       = \"1h\")\n",
    "    print(gap_df)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in station.items():\n",
    "    print(f\"🔸 Trạm: {name}\")\n",
    "    \n",
    "    station[name] = dataset.fill_time_gaps(data       = df.set_index(\"time\"), \n",
    "                                           start_time = \"1990-01-01 00:00:00+07:00\", \n",
    "                                           end_time   = \"2025-01-01 00:00:00+07:00\",\n",
    "                                           freq       = \"1h\")\n",
    "    print(f\"✅ Đã xong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in station.items():\n",
    "    print(f\"🔸 Trạm: {name}\")\n",
    "\n",
    "    gap_df = dataset.find_time_gaps(data       = df.set_index(\"time\"), \n",
    "                                    start_time = \"1990-01-01 00:00:00+07:00\", \n",
    "                                    end_time   = \"2025-01-01 00:00:00+07:00\",\n",
    "                                    freq       = \"1h\")\n",
    "    print(gap_df)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Features</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📊 So sánh các phương pháp xử lý giá trị thiếu\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Phương pháp</th>\n",
    "<th>Ưu điểm</th>\n",
    "<th>Nhược điểm</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td><code>dropna()</code></td>\n",
    "<td>Nhanh gọn, triệt để</td>\n",
    "<td>Mất dữ liệu</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><code>fillna(mean/median)</code></td>\n",
    "<td>Dễ thực hiện, hợp với dữ liệu số</td>\n",
    "<td>Có thể làm giảm phương sai</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><code>ffill()</code> / <code>bfill()</code></td>\n",
    "<td>Dùng tốt với time series</td>\n",
    "<td>Có thể tạo bias</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><code>interpolate()</code></td>\n",
    "<td>Tự nhiên, giữ xu hướng</td>\n",
    "<td>Cần dữ liệu liên tục</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><code>groupby fill</code></td>\n",
    "<td>Phân nhóm thông minh</td>\n",
    "<td>Chậm với dữ liệu lớn</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "✅ Kết luận:\n",
    "<ul>\n",
    "<li><b>Time series</b> → <code>interpolate()</code>, <code>ffill()</code></li>\n",
    "<li><b>Dữ liệu nhóm</b> → <code>groupby fill</code></li>\n",
    "<li><b>Dữ liệu có ít NaN</b> → <code>dropna()</code></li>\n",
    "<li><b>Dữ liệu số nhiều NaN</b> → <code>fillna(mean/median)</code> hoặc <code>interpolate()</code></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Thứ tự | Phương pháp                                              | Áp dụng khi                                                                   | Ưu tiên                                                                 |\n",
    "| :----- | :------------------------------------------------------- | :---------------------------------------------------------------------------- | :---------------------------------------------------------------------- |\n",
    "| **1**  | `interpolate(method=\"time\")`                             | Dữ liệu liên tục, time series, các biến số khí tượng biến động theo thời gian | ✅ Ưu tiên đầu                                                           |\n",
    "| **2**  | `ffill()` / `bfill()`                                    | Nếu `interpolate` không fill hết, dùng giá trị trước/sau gần nhất             | ⚠️ Chỉ nên dùng khi dữ liệu không có sự biến động lớn hoặc missing ngắn |\n",
    "| **3**  | `groupby fill` (theo ngày, tháng, năm hoặc theo station) | Nếu dữ liệu phân nhóm và mỗi nhóm đủ dày                                      | Tốt cho dữ liệu dạng panel, hoặc khí tượng từng trạm                    |\n",
    "| **4**  | `fillna(mean/median)`                                    | Nếu số lượng missing còn nhỏ lẻ, hoặc sau khi các cách trên vẫn còn missing   | Dùng được với numerical không quá nhạy cảm                              |\n",
    "| **5**  | `dropna()`                                               | Khi số lượng missing ít, hoặc không thể nội suy mà giữ lại sẽ gây sai lệch    | Cuối cùng                                                               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in station.items():\n",
    "    print(f\"🔸 Trạm: {name}\")\n",
    "\n",
    "    dataset.ProportionMissing_aproach1(df[numerical_features])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in station.items():\n",
    "    print(f\"🔸 Trạm: {name}\")\n",
    "    \n",
    "    # Set DatetimeIndex\n",
    "    station[name] = df.set_index(\"time\")\n",
    "\n",
    "    # Nội suy các biến numerical\n",
    "    df_filled = dataset.HandleMissing_interpolate(data   = station[name], \n",
    "                                                  method = \"time\")\n",
    "\n",
    "    # Gán lại vào DataFrame gốc \n",
    "    station[name] = df_filled.reset_index()\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in station.items():\n",
    "    print(f\"🔸 Trạm: {name}\")\n",
    "\n",
    "    dataset.ProportionMissing_aproach1(df[numerical_features])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in station.items():\n",
    "    print(f\"🔸 Trạm: {name}\")\n",
    "    \n",
    "    # Set DatetimeIndex\n",
    "    station[name] = df.set_index(\"time\")\n",
    "\n",
    "    # Nội suy các biến numerical\n",
    "    df_filled = dataset.HandleMissing_fillna(data   = station[name], \n",
    "                                             method = \"median\")\n",
    "\n",
    "    # Gán lại vào DataFrame gốc \n",
    "    station[name] = df_filled.reset_index()\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in station.items():\n",
    "    print(f\"🔸 Trạm: {name}\")\n",
    "\n",
    "    dataset.ProportionMissing_aproach1(df[numerical_features])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"CaMau\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">4.3. Handling mismatch values</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">4.4. Handling abnormal <~> outlier values <u>(basic)</u></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📖 Tổng hợp cách xử lý outlier cho Time Series\n",
    "| Phương pháp                                  | Mô tả                                                                             | Khi nào dùng                                  | Ưu điểm        | Nhược điểm                           |\n",
    "| :------------------------------------------- | :-------------------------------------------------------------------------------- | :-------------------------------------------- | :------------- | :----------------------------------- |\n",
    "| **Remove**                                   | Loại các điểm dữ liệu ngoài ngưỡng                                                | Nếu rõ ràng là lỗi đo đạc                     | Dễ làm         | Mất dữ liệu, đứt chuỗi               |\n",
    "| **Interpolation**                            | Nội suy giá trị tại vị trí outlier bằng trung bình giữa trước và sau              | Khi outlier là lỗi cục bộ                     | Giữ mượt chuỗi | Không đúng nếu outlier là event thực |\n",
    "| **LOCF (Last Observation Carried Forward)**  | Lấy giá trị trước đó thay thế                                                     | Dữ liệu nhạy biến động thấp (ví dụ: nhiệt độ) | Dễ cài         | Nếu outlier đúng là event, bị sai    |\n",
    "| **NOCB (Next Observation Carried Backward)** | Lấy giá trị tiếp theo thay thế                                                    | Tương tự LOCF                                 | Dễ làm         | Sai với biến thiên nhanh             |\n",
    "| **Smoothing (Rolling Mean / EWMA)**          | Làm mượt chuỗi, giảm ảnh hưởng outlier bằng trung bình trượt                      | Nếu cần dự báo dài hạn                        | Giảm nhiễu     | Mất tín hiệu ngắn hạn                |\n",
    "| **Model-based Detection**                    | Dùng Isolation Forest, Prophet changepoint detection… phát hiện outlier rồi xử lý | Khi data nhiều outlier hoặc nhiều mùa vụ      | Nhận diện tốt  | Tốn thời gian                        |\n",
    "| **Capping theo percentile/IQR từng đoạn**    | Chỉ capping giá trị tại vị trí bất thường                                         | Nếu không muốn interpolate                    | Giữ data       | Mất thông tin cực trị                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📊 Hướng xử lý theo loại outlier trong time series\n",
    "| Loại outlier                            | Cách xử lý tốt                        |\n",
    "| :-------------------------------------- | :------------------------------------ |\n",
    "| **Lỗi nhập liệu, spike dị thường**      | Remove / LOCF / Interpolate           |\n",
    "| **Số liệu sự kiện thực (mưa lớn, bão)** | Giữ nguyên                            |\n",
    "| **Nhiễu nhẹ / bất thường nhỏ**          | Rolling Mean / EWMA smoothing         |\n",
    "| **Đứt đoạn giá trị**                    | Interpolation (linear/spline)         |\n",
    "| **Spike vào dịp lễ, sự kiện (như tết)** | Có thể tạo biến indicator thay vì xóa |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <a href=\"https://towardsdatascience.com/the-ultimate-guide-to-finding-outliers-in-your-time-series-data-part-1-1bf81e09ade4-2/\">The Ultimate Guide to Finding Outliers in Your Time-Series Data (Part 1)</a> <--> Phương pháp xác định (visual/statistical)\n",
    "- <a href=\"https://medium.com/data-science/the-ultimate-guide-to-finding-outliers-in-your-time-series-data-part-2-674c25837f29\">The Ultimate Guide to Finding Outliers in Your Time-Series Data (Part 2)</a> <--> Sử dụng machine learning\n",
    "- <a href=\"https://towardsdatascience.com/the-ultimate-guide-to-finding-outliers-in-your-time-series-data-part-3-0ff73ce28ca3-2/\">The Ultimate Guide to Finding Outliers in Your Time-Series Data (Part 3)</a> <--> “Outliers Found: Now What?” – quản lý outliers\n",
    "- <a href=\"https://towardsdatascience.com/evaluating-the-impact-of-outlier-treatment-in-time-series-b4fac4cabe94/\">The Ultimate Guide to Finding Outliers in Your Time-Series Data (Part 4)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📖 Các giá trị freq thường dùng trong resample() Pandas:\n",
    "| Tần suất       | `freq` string        | Ý nghĩa                                   |\n",
    "| :------------- | :------------------- | :---------------------------------------- |\n",
    "| **Hàng giây**  | `\"S\"`                | Mỗi giây                                  |\n",
    "| **Hàng phút**  | `\"T\"` (hoặc `\"min\"`) | Mỗi phút                                  |\n",
    "| **Hàng giờ**   | `\"H\"`                | Mỗi giờ                                   |\n",
    "| **Hàng ngày**  | `\"D\"`                | Mỗi ngày                                  |\n",
    "| **Hàng tuần**  | `\"W\"`                | Mỗi tuần (mặc định kết thúc vào Chủ nhật) |\n",
    "| **Hàng tháng** | `\"M\"`                | Cuối mỗi tháng                            |\n",
    "| **Đầu tháng**  | `\"MS\"`               | Đầu mỗi tháng                             |\n",
    "| **Hàng quý**   | `\"Q\"`                | Cuối mỗi quý (3, 6, 9, 12)                |\n",
    "| **Đầu quý**    | `\"QS\"`               | Đầu mỗi quý                               |\n",
    "| **Hàng năm**   | `\"A\"` hoặc `\"Y\"`     | Cuối mỗi năm (mặc định 31-12)             |\n",
    "| **Đầu năm**    | `\"AS\"` hoặc `\"YS\"`   | Đầu mỗi năm (1-1)                         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"CaMau\"].loc[:, [\"time\", \"v10\"]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = station[\"CaMau\"].loc[:, [\"time\", \"v10\"]].set_index(\"time\").resample(\"1H\").mean().interpolate()\n",
    "new_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Ca Mau</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_feature_trends_over_time(data         = station[\"CaMau\"],\n",
    "                                    data_cols    = numerical_features,\n",
    "                                    station_name = \"CaMau\",\n",
    "                                    start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                    end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                    freq         = \"1Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_feature_outliers_over_time(data         = station[\"CaMau\"],\n",
    "                                      data_cols    = numerical_features,\n",
    "                                      station_name = \"CaMau\",\n",
    "                                      method       = \"statistic\",\n",
    "                                      display      = False,\n",
    "                                      start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                      end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                      freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"CaMau\"] = dataset.handle_feature_outliers_over_time(data         = station[\"CaMau\"],\n",
    "                                                             data_cols    = numerical_features,\n",
    "                                                             station_name = \"CaMau\",\n",
    "                                                             method       = \"statistic\",\n",
    "                                                             display      = True,\n",
    "                                                             start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                                             end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                                             freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Dong Hoi</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_feature_trends_over_time(data         = station[\"DH\"],\n",
    "                                    data_cols    = numerical_features,\n",
    "                                    station_name = \"DH\",\n",
    "                                    start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                    end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                    freq         = \"1Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_feature_outliers_over_time(data         = station[\"DH\"],\n",
    "                                      data_cols    = numerical_features,\n",
    "                                      station_name = \"DH\",\n",
    "                                      method       = \"statistic\",\n",
    "                                      display      = False,\n",
    "                                      start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                      end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                      freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"DH\"] = dataset.handle_feature_outliers_over_time(data         = station[\"DH\"],\n",
    "                                                          data_cols    = numerical_features,\n",
    "                                                          station_name = \"DH\",\n",
    "                                                          method       = \"statistic\",\n",
    "                                                          display      = True,\n",
    "                                                          start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                                          end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                                          freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Noi Bai</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_feature_trends_over_time(data         = station[\"NB\"],\n",
    "                                    data_cols    = numerical_features,\n",
    "                                    station_name = \"NB\",\n",
    "                                    start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                    end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                    freq         = \"1Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_feature_outliers_over_time(data         = station[\"NB\"],\n",
    "                                      data_cols    = numerical_features,\n",
    "                                      station_name = \"NB\",\n",
    "                                      method       = \"statistic\",\n",
    "                                      display      = False,\n",
    "                                      start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                      end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                      freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"NB\"] = dataset.handle_feature_outliers_over_time(data         = station[\"NB\"],\n",
    "                                                          data_cols    = numerical_features,\n",
    "                                                          station_name = \"NB\",\n",
    "                                                          method       = \"statistic\",\n",
    "                                                          display      = True,\n",
    "                                                          start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                                          end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                                          freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Quy Nhon</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_feature_trends_over_time(data         = station[\"QN\"],\n",
    "                                    data_cols    = numerical_features,\n",
    "                                    station_name = \"QN\",\n",
    "                                    start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                    end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                    freq         = \"1Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_feature_outliers_over_time(data         = station[\"QN\"],\n",
    "                                      data_cols    = numerical_features,\n",
    "                                      station_name = \"QN\",\n",
    "                                      method       = \"statistic\",\n",
    "                                      display      = False,\n",
    "                                      start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                      end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                      freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"QN\"] = dataset.handle_feature_outliers_over_time(data         = station[\"QN\"],\n",
    "                                                          data_cols    = numerical_features,\n",
    "                                                          station_name = \"QN\",\n",
    "                                                          method       = \"statistic\",\n",
    "                                                          display      = True,\n",
    "                                                          start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                                          end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                                          freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Thanh Hoa</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_feature_trends_over_time(data         = station[\"TH\"],\n",
    "                                    data_cols    = numerical_features,\n",
    "                                    station_name = \"TH\",\n",
    "                                    start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                    end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                    freq         = \"1Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_feature_outliers_over_time(data         = station[\"TH\"],\n",
    "                                      data_cols    = numerical_features,\n",
    "                                      station_name = \"TH\",\n",
    "                                      method       = \"statistic\",\n",
    "                                      display      = False,\n",
    "                                      start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                      end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                      freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"TH\"] = dataset.handle_feature_outliers_over_time(data         = station[\"TH\"],\n",
    "                                                          data_cols    = numerical_features,\n",
    "                                                          station_name = \"TH\",\n",
    "                                                          method       = \"statistic\",\n",
    "                                                          display      = True,\n",
    "                                                          start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                                          end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                                          freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">TSN</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_feature_trends_over_time(data         = station[\"TSN\"],\n",
    "                                    data_cols    = numerical_features,\n",
    "                                    station_name = \"TSN\",\n",
    "                                    start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                    end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                    freq         = \"1Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_feature_outliers_over_time(data         = station[\"TSN\"],\n",
    "                                      data_cols    = numerical_features,\n",
    "                                      station_name = \"TSN\",\n",
    "                                      method       = \"statistic\",\n",
    "                                      display      = False,\n",
    "                                      start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                      end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                      freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"TSN\"] = dataset.handle_feature_outliers_over_time(data         = station[\"TSN\"],\n",
    "                                                           data_cols    = numerical_features,\n",
    "                                                           station_name = \"TSN\",\n",
    "                                                           method       = \"statistic\",\n",
    "                                                           display      = True,\n",
    "                                                           start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                                           end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                                           freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">4.5. Save data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"TSN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo thư mục nếu chưa có\n",
    "os.makedirs(\"../data/processed/datasets\", exist_ok=True)\n",
    "\n",
    "# Duyệt từng station và lưu file\n",
    "for name, df in station.items():\n",
    "    file_name = f\"../data/processed/datasets/{name}90.24_cleaned.csv\"\n",
    "    df.to_csv(file_name, index=False)\n",
    "    print(f\"Đã lưu: {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">5. Data Preprocessing</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">5.1. Handling abnormal <~> outlier values <u>(intermediate)</u></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 1️⃣ So sánh Univariate Time Series và Multivariate Time Series\n",
    "| Tiêu chí               | **Univariate Time Series**                      | **Multivariate Time Series**                                                 |\n",
    "| :--------------------- | :---------------------------------------------- | :--------------------------------------------------------------------------- |\n",
    "| **Định nghĩa**         | Dự báo 1 biến duy nhất theo thời gian           | Dự báo 1 hoặc nhiều biến, có thể phụ thuộc thêm các biến khác theo thời gian |\n",
    "| **Đầu vào (features)** | Chỉ có giá trị của chính biến đó theo thời gian | Có thể bao gồm nhiều biến liên quan (covariates) cùng thời gian              |\n",
    "| **Độ phức tạp**        | Thấp hơn, dễ triển khai                         | Cao hơn do tính đến mối quan hệ giữa các biến                                |\n",
    "| **Ví dụ**              | Dự báo nhiệt độ từng ngày tại 1 thành phố       | Dự báo nhiệt độ từng ngày + độ ẩm, tốc độ gió, kinh độ/vĩ độ, ngày lễ…       |\n",
    "| **Kỹ thuật áp dụng**   | ARIMA, Prophet, LSTM, N-BEATS, Moving Average…  | VAR, LSTM với nhiều input, TFT, Multivariate N-BEATS…                        |\n",
    "| **Ưu điểm**            | Dữ liệu và mô hình đơn giản, nhanh chóng        | Có thể tận dụng mối quan hệ giữa các biến để dự báo chính xác hơn            |\n",
    "| **Nhược điểm**         | Không tận dụng được các yếu tố ảnh hưởng khác   | Cần nhiều dữ liệu hơn, xử lý phức tạp, dễ gặp vấn đề với outlier             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 2️⃣ Cách xử lý outlier trong multivariate time series\n",
    "| Phương pháp                                     | Cách thực hiện                                                                    | Ưu điểm                                 | Nhược điểm                                     |\n",
    "| :---------------------------------------------- | :-------------------------------------------------------------------------------- | :-------------------------------------- | :--------------------------------------------- |\n",
    "| **Z-score / IQR cho từng biến**                 | Tính z-score hoặc IQR cho từng feature, loại bỏ giá trị vượt ngưỡng               | Dễ triển khai, đơn giản                 | Chỉ xử lý được từng biến, không xét tương quan |\n",
    "| **Multivariate Z-score (Mahalanobis distance)** | Tính khoảng cách Mahalanobis giữa mỗi điểm và phân phối tổng thể                  | Xét được mối quan hệ giữa các biến      | Cần giả định phân phối dữ liệu gần chuẩn       |\n",
    "| **Isolation Forest / One-Class SVM**            | Dùng mô hình unsupervised để phát hiện điểm bất thường trên toàn bộ feature space | Tự động, không cần giả định phân phối   | Chậm với dữ liệu lớn, tuning khó               |\n",
    "| **Autoencoder / LSTM-AE**                       | Train autoencoder, điểm nào reconstruction error lớn thì là outlier               | Phù hợp time series, capture non-linear | Cần nhiều dữ liệu, training lâu                |\n",
    "| **Visual inspection (scatter matrix, PCA)**     | Giảm số chiều xuống 2D/3D để trực quan và phát hiện outlier                       | Nhanh, trực quan                        | Khó áp dụng khi dữ liệu quá lớn                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 3️⃣ Gợi ý cho bài toán của bạn:\n",
    "\n",
    "Với bài toán dự báo nhiệt độ cực đại theo thời gian có geo-feature\n",
    "\n",
    "→ Nên dùng Isolation Forest hoặc Autoencoder để phát hiện outlier multivariate vì:\n",
    "\n",
    "- Có thể capture mối quan hệ giữa nhiệt độ, độ ẩm, kinh độ, vĩ độ, thành phố\n",
    "- Không cần giả định phân phối chuẩn\n",
    "- Hợp cho dữ liệu nhiều biến dạng time series\n",
    "\n",
    "Nếu dataset vừa phải thì dùng Isolation Forest\n",
    "Nếu dataset lớn hoặc quan hệ non-linear mạnh thì dùng LSTM-AE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 4️⃣ Tóm lại\n",
    "\n",
    "- Univariate chỉ với 1 biến — đơn giản nhưng không tận dụng được yếu tố khác\n",
    "- Multivariate phức tạp hơn, nhiều thông tin hơn nhưng phải xử lý outlier đúng cách.\n",
    "\n",
    "Outlier Multivariate nên xử lý bằng Isolation Forest, Mahalanobis Distance hoặc Autoencoder để xét tương quan nhiều chiều."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📖 Các giá trị freq thường dùng trong resample() Pandas:\n",
    "| Tần suất       | `freq` string        | Ý nghĩa                                   |\n",
    "| :------------- | :------------------- | :---------------------------------------- |\n",
    "| **Hàng giây**  | `\"S\"`                | Mỗi giây                                  |\n",
    "| **Hàng phút**  | `\"T\"` (hoặc `\"min\"`) | Mỗi phút                                  |\n",
    "| **Hàng giờ**   | `\"H\"`                | Mỗi giờ                                   |\n",
    "| **Hàng ngày**  | `\"D\"`                | Mỗi ngày                                  |\n",
    "| **Hàng tuần**  | `\"W\"`                | Mỗi tuần (mặc định kết thúc vào Chủ nhật) |\n",
    "| **Hàng tháng** | `\"M\"`                | Cuối mỗi tháng                            |\n",
    "| **Đầu tháng**  | `\"MS\"`               | Đầu mỗi tháng                             |\n",
    "| **Hàng quý**   | `\"Q\"`                | Cuối mỗi quý (3, 6, 9, 12)                |\n",
    "| **Đầu quý**    | `\"QS\"`               | Đầu mỗi quý                               |\n",
    "| **Hàng năm**   | `\"A\"` hoặc `\"Y\"`     | Cuối mỗi năm (mặc định 31-12)             |\n",
    "| **Đầu năm**    | `\"AS\"` hoặc `\"YS\"`   | Đầu mỗi năm (1-1)                         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots.plot_feature_trends_over_time(data         = station[\"CaMau\"],\n",
    "#                                     data_cols    = numerical_features,\n",
    "#                                     station_name = \"CaMau\",\n",
    "#                                     start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "#                                     end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "#                                     freq         = \"1M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">CaMau</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots.plot_feature_outliers_over_time(data         = station[\"CaMau\"],\n",
    "#                                       data_cols    = numerical_features,\n",
    "#                                       station_name = \"CaMau\",\n",
    "#                                       method       = \"machine_learning\",\n",
    "#                                       models       = dict({\n",
    "#                                                            \"IsolationForest\"         : IsolationForest(bootstrap     = False, \n",
    "#                                                                                                        contamination = \"auto\", \n",
    "#                                                                                                        max_features  = 1.0, \n",
    "#                                                                                                        max_samples   = \"auto\", \n",
    "#                                                                                                        n_estimators  = 100, \n",
    "#                                                                                                        n_jobs        = None, \n",
    "#                                                                                                        random_state  = None, \n",
    "#                                                                                                        verbose       = 0, \n",
    "#                                                                                                        warm_start    = False),\n",
    "#                                                            \"LocalOutlierFactor\"      : LocalOutlierFactor(algorithm     = \"auto\", \n",
    "#                                                                                                           contamination = \"auto\", \n",
    "#                                                                                                           leaf_size     = 30, \n",
    "#                                                                                                           metric        = \"minkowski\", \n",
    "#                                                                                                           metric_params = None, \n",
    "#                                                                                                           n_jobs        = None, \n",
    "#                                                                                                           n_neighbors   = 20, \n",
    "#                                                                                                           novelty       = False, \n",
    "#                                                                                                           p             = 2),\n",
    "#                                                            \"Prophet\"                 : Prophet(growth                  = \"linear\", # SLOW!!!\n",
    "#                                                                                                changepoints            = None,\n",
    "#                                                                                                n_changepoints          = 25,\n",
    "#                                                                                                changepoint_range       = 0.8,\n",
    "#                                                                                                yearly_seasonality      = \"auto\",\n",
    "#                                                                                                weekly_seasonality      = \"auto\",\n",
    "#                                                                                                daily_seasonality       = \"auto\",\n",
    "#                                                                                                holidays                = None,\n",
    "#                                                                                                seasonality_mode        = \"additive\",\n",
    "#                                                                                                seasonality_prior_scale = 10.0,\n",
    "#                                                                                                holidays_prior_scale    = 10.0,\n",
    "#                                                                                                changepoint_prior_scale = 0.05,\n",
    "#                                                                                                mcmc_samples            = 0,\n",
    "#                                                                                                interval_width          = 0.80,\n",
    "#                                                                                                uncertainty_samples     = 1000,\n",
    "#                                                                                                stan_backend            = None),\n",
    "#                                                            \"AgglomerativeClustering\" : AgglomerativeClustering(compute_distances  = False,  # SLOW!!!\n",
    "#                                                                                                                compute_full_tree  = 'auto', \n",
    "#                                                                                                             #    connectivity       = None, # KHONG SUA\n",
    "#                                                                                                                distance_threshold = None, \n",
    "#                                                                                                             #    linkage            = 'ward', # KHONG SUA\n",
    "#                                                                                                                memory             = None, \n",
    "#                                                                                                                metric             = 'euclidean', \n",
    "#                                                                                                                n_clusters         = 3),\n",
    "#                                                            \"HDBSCAN\"                  : HDBSCAN(min_cluster_size               = 5,\n",
    "#                                                                                                                        min_samples                    = None,\n",
    "#                                                                                                                        metric                         = 'euclidean',\n",
    "#                                                                                                                        alpha                          = 1.0,\n",
    "#                                                                                                                        p                              = None,\n",
    "#                                                                                                                        algorithm                      = 'best',\n",
    "#                                                                                                                        leaf_size                      = 40,\n",
    "#                                                                                                                        memory                         = Memory(location=None),\n",
    "#                                                                                                                        approx_min_span_tree           = True,\n",
    "#                                                                                                                        gen_min_span_tree              = False,\n",
    "#                                                                                                                        core_dist_n_jobs               = 1,\n",
    "#                                                                                                                        cluster_selection_method       = 'eom',\n",
    "#                                                                                                                        allow_single_cluster           = False,\n",
    "#                                                                                                                        prediction_data                = False,\n",
    "#                                                                                                                        match_reference_implementation = False),\n",
    "#                                                            \"VanillaAutoencoder\"      : \"Dell bt ghi gi o day de tuning @@@\"\n",
    "#                                                            }),\n",
    "#                                       factor       = 1.5,   # Stick with Prophet model\n",
    "#                                       window_size  = 10,    # Stick with Agglomerative Clustering & DBSCAN model\n",
    "#                                       dendrogram   = False, # Stick with Agglomerative Clustering model\n",
    "#                                       display      = False,\n",
    "#                                       start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "#                                       end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "#                                       freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "\n",
    "\n",
    "station[\"CaMau\"] = dataset.handle_feature_outliers_over_time(data         = station[\"CaMau\"],\n",
    "                                                             data_cols    = numerical_features,\n",
    "                                                             station_name = \"CaMau\",\n",
    "                                                             method       = \"machine_learning\",\n",
    "                                                             models       = dict({\n",
    "                                                                                  \"IsolationForest\"         : IsolationForest(bootstrap     = False, \n",
    "                                                                                                                              contamination = \"auto\", \n",
    "                                                                                                                              max_features  = 1.0, \n",
    "                                                                                                                              max_samples   = \"auto\", \n",
    "                                                                                                                              n_estimators  = 100, \n",
    "                                                                                                                              n_jobs        = None, \n",
    "                                                                                                                              random_state  = None, \n",
    "                                                                                                                              verbose       = 0, \n",
    "                                                                                                                              warm_start    = False),\n",
    "                                                                                  \"LocalOutlierFactor\"      : LocalOutlierFactor(algorithm     = \"auto\", \n",
    "                                                                                                                                 contamination = \"auto\", \n",
    "                                                                                                                                 leaf_size     = 30, \n",
    "                                                                                                                                 metric        = \"minkowski\", \n",
    "                                                                                                                                 metric_params = None, \n",
    "                                                                                                                                 n_jobs        = None, \n",
    "                                                                                                                                 n_neighbors   = 20, \n",
    "                                                                                                                                 novelty       = False, \n",
    "                                                                                                                                 p             = 2),\n",
    "                                                                                  \"Prophet\"                 : Prophet(growth                  = \"linear\", # SLOW!!!\n",
    "                                                                                                                      changepoints            = None,\n",
    "                                                                                                                      n_changepoints          = 25,\n",
    "                                                                                                                      changepoint_range       = 0.8,\n",
    "                                                                                                                      yearly_seasonality      = \"auto\",\n",
    "                                                                                                                      weekly_seasonality      = \"auto\",\n",
    "                                                                                                                      daily_seasonality       = \"auto\",\n",
    "                                                                                                                      holidays                = None,\n",
    "                                                                                                                      seasonality_mode        = \"additive\",\n",
    "                                                                                                                      seasonality_prior_scale = 10.0,\n",
    "                                                                                                                      holidays_prior_scale    = 10.0,\n",
    "                                                                                                                      changepoint_prior_scale = 0.05,\n",
    "                                                                                                                      mcmc_samples            = 0,\n",
    "                                                                                                                      interval_width          = 0.80,\n",
    "                                                                                                                      uncertainty_samples     = 1000,\n",
    "                                                                                                                      stan_backend            = None),\n",
    "                                                                                  \"AgglomerativeClustering\" : AgglomerativeClustering(compute_distances  = False,  # SLOW!!!\n",
    "                                                                                                                                      compute_full_tree  = 'auto', \n",
    "                                                                                                                                   #    connectivity       = None, # KHONG SUA\n",
    "                                                                                                                                      distance_threshold = None, \n",
    "                                                                                                                                   #    linkage            = 'ward', # KHONG SUA\n",
    "                                                                                                                                      memory             = None, \n",
    "                                                                                                                                      metric             = 'euclidean', \n",
    "                                                                                                                                      n_clusters         = 3),\n",
    "                                                                                  \"HDBSCAN\"                  : HDBSCAN(min_cluster_size               = 5,\n",
    "                                                                                                                       min_samples                    = None,\n",
    "                                                                                                                       metric                         = 'euclidean',\n",
    "                                                                                                                       alpha                          = 1.0,\n",
    "                                                                                                                       p                              = None,\n",
    "                                                                                                                       algorithm                      = 'best',\n",
    "                                                                                                                       leaf_size                      = 40,\n",
    "                                                                                                                       memory                         = Memory(location=None),\n",
    "                                                                                                                       approx_min_span_tree           = True,\n",
    "                                                                                                                       gen_min_span_tree              = False,\n",
    "                                                                                                                       core_dist_n_jobs               = 1,\n",
    "                                                                                                                       cluster_selection_method       = 'eom',\n",
    "                                                                                                                       allow_single_cluster           = False,\n",
    "                                                                                                                       prediction_data                = False,\n",
    "                                                                                                                       match_reference_implementation = False),\n",
    "                                                                                  \"VanillaAutoencoder\"      : \"Dell bt ghi gi o day de tuning @@@\"\n",
    "                                                                                  }),\n",
    "                                                             factor       = 1.5,   # Stick with Prophet model\n",
    "                                                             window_size  = 10,    # Stick with Agglomerative Clustering & HDBSCAN model\n",
    "                                                             dendrogram   = False, # Stick with Agglomerative Clustering model\n",
    "                                                             display      = True,\n",
    "                                                             start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                                             end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                                             freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Dong Hoi</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots.plot_feature_outliers_over_time(data         = station[\"DH\"],\n",
    "#                                       data_cols    = numerical_features,\n",
    "#                                       station_name = \"DongHoi\",\n",
    "#                                       method       = \"machine_learning\",\n",
    "#                                       models       = dict({\n",
    "#                                                            \"IsolationForest\"         : IsolationForest(bootstrap     = False, \n",
    "#                                                                                                        contamination = \"auto\", \n",
    "#                                                                                                        max_features  = 1.0, \n",
    "#                                                                                                        max_samples   = \"auto\", \n",
    "#                                                                                                        n_estimators  = 100, \n",
    "#                                                                                                        n_jobs        = None, \n",
    "#                                                                                                        random_state  = None, \n",
    "#                                                                                                        verbose       = 0, \n",
    "#                                                                                                        warm_start    = False),\n",
    "#                                                            \"LocalOutlierFactor\"      : LocalOutlierFactor(algorithm     = \"auto\", \n",
    "#                                                                                                           contamination = \"auto\", \n",
    "#                                                                                                           leaf_size     = 30, \n",
    "#                                                                                                           metric        = \"minkowski\", \n",
    "#                                                                                                           metric_params = None, \n",
    "#                                                                                                           n_jobs        = None, \n",
    "#                                                                                                           n_neighbors   = 20, \n",
    "#                                                                                                           novelty       = False, \n",
    "#                                                                                                           p             = 2),\n",
    "#                                                            \"Prophet\"                 : Prophet(growth                  = \"linear\", # SLOW!!!\n",
    "#                                                                                                changepoints            = None,\n",
    "#                                                                                                n_changepoints          = 25,\n",
    "#                                                                                                changepoint_range       = 0.8,\n",
    "#                                                                                                yearly_seasonality      = \"auto\",\n",
    "#                                                                                                weekly_seasonality      = \"auto\",\n",
    "#                                                                                                daily_seasonality       = \"auto\",\n",
    "#                                                                                                holidays                = None,\n",
    "#                                                                                                seasonality_mode        = \"additive\",\n",
    "#                                                                                                seasonality_prior_scale = 10.0,\n",
    "#                                                                                                holidays_prior_scale    = 10.0,\n",
    "#                                                                                                changepoint_prior_scale = 0.05,\n",
    "#                                                                                                mcmc_samples            = 0,\n",
    "#                                                                                                interval_width          = 0.80,\n",
    "#                                                                                                uncertainty_samples     = 1000,\n",
    "#                                                                                                stan_backend            = None),\n",
    "#                                                            \"AgglomerativeClustering\" : AgglomerativeClustering(compute_distances  = False,  # SLOW!!!\n",
    "#                                                                                                                compute_full_tree  = 'auto', \n",
    "#                                                                                                             #    connectivity       = None, # KHONG SUA\n",
    "#                                                                                                                distance_threshold = None, \n",
    "#                                                                                                             #    linkage            = 'ward', # KHONG SUA\n",
    "#                                                                                                                memory             = None, \n",
    "#                                                                                                                metric             = 'euclidean', \n",
    "#                                                                                                                n_clusters         = 3),\n",
    "#                                                            \"HDBSCAN\"                  : HDBSCAN(min_cluster_size               = 5,\n",
    "#                                                                                                                        min_samples                    = None,\n",
    "#                                                                                                                        metric                         = 'euclidean',\n",
    "#                                                                                                                        alpha                          = 1.0,\n",
    "#                                                                                                                        p                              = None,\n",
    "#                                                                                                                        algorithm                      = 'best',\n",
    "#                                                                                                                        leaf_size                      = 40,\n",
    "#                                                                                                                        memory                         = Memory(location=None),\n",
    "#                                                                                                                        approx_min_span_tree           = True,\n",
    "#                                                                                                                        gen_min_span_tree              = False,\n",
    "#                                                                                                                        core_dist_n_jobs               = 1,\n",
    "#                                                                                                                        cluster_selection_method       = 'eom',\n",
    "#                                                                                                                        allow_single_cluster           = False,\n",
    "#                                                                                                                        prediction_data                = False,\n",
    "#                                                                                                                        match_reference_implementation = False),\n",
    "#                                                            \"VanillaAutoencoder\"      : \"Dell bt ghi gi o day de tuning @@@\"\n",
    "#                                                            }),\n",
    "#                                       factor       = 1.5,   # Stick with Prophet model\n",
    "#                                       window_size  = 10,    # Stick with Agglomerative Clustering & DBSCAN model\n",
    "#                                       dendrogram   = False, # Stick with Agglomerative Clustering model\n",
    "#                                       display      = False,\n",
    "#                                       start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "#                                       end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "#                                       freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"DH\"] = dataset.handle_feature_outliers_over_time(data         = station[\"DH\"],\n",
    "                                                          data_cols    = numerical_features,\n",
    "                                                          station_name = \"DongHoi\",\n",
    "                                                          method       = \"machine_learning\",\n",
    "                                                          models       = dict({\n",
    "                                                                               \"IsolationForest\"         : IsolationForest(bootstrap     = False, \n",
    "                                                                                                                           contamination = \"auto\", \n",
    "                                                                                                                           max_features  = 1.0, \n",
    "                                                                                                                           max_samples   = \"auto\", \n",
    "                                                                                                                           n_estimators  = 100, \n",
    "                                                                                                                           n_jobs        = None, \n",
    "                                                                                                                           random_state  = None, \n",
    "                                                                                                                           verbose       = 0, \n",
    "                                                                                                                           warm_start    = False),\n",
    "                                                                               \"LocalOutlierFactor\"      : LocalOutlierFactor(algorithm     = \"auto\", \n",
    "                                                                                                                              contamination = \"auto\", \n",
    "                                                                                                                              leaf_size     = 30, \n",
    "                                                                                                                              metric        = \"minkowski\", \n",
    "                                                                                                                              metric_params = None, \n",
    "                                                                                                                              n_jobs        = None, \n",
    "                                                                                                                              n_neighbors   = 20, \n",
    "                                                                                                                              novelty       = False, \n",
    "                                                                                                                              p             = 2),\n",
    "                                                                               \"Prophet\"                 : Prophet(growth                  = \"linear\", # SLOW!!!\n",
    "                                                                                                                   changepoints            = None,\n",
    "                                                                                                                   n_changepoints          = 25,\n",
    "                                                                                                                   changepoint_range       = 0.8,\n",
    "                                                                                                                   yearly_seasonality      = \"auto\",\n",
    "                                                                                                                   weekly_seasonality      = \"auto\",\n",
    "                                                                                                                   daily_seasonality       = \"auto\",\n",
    "                                                                                                                   holidays                = None,\n",
    "                                                                                                                   seasonality_mode        = \"additive\",\n",
    "                                                                                                                   seasonality_prior_scale = 10.0,\n",
    "                                                                                                                   holidays_prior_scale    = 10.0,\n",
    "                                                                                                                   changepoint_prior_scale = 0.05,\n",
    "                                                                                                                   mcmc_samples            = 0,\n",
    "                                                                                                                   interval_width          = 0.80,\n",
    "                                                                                                                   uncertainty_samples     = 1000,\n",
    "                                                                                                                   stan_backend            = None),\n",
    "                                                                               \"AgglomerativeClustering\" : AgglomerativeClustering(compute_distances  = False,  # SLOW!!!\n",
    "                                                                                                                                   compute_full_tree  = 'auto', \n",
    "                                                                                                                                #    connectivity       = None, # KHONG SUA\n",
    "                                                                                                                                   distance_threshold = None, \n",
    "                                                                                                                                #    linkage            = 'ward', # KHONG SUA\n",
    "                                                                                                                                   memory             = None, \n",
    "                                                                                                                                   metric             = 'euclidean', \n",
    "                                                                                                                                   n_clusters         = 3),\n",
    "                                                                               \"HDBSCAN\"                  : HDBSCAN(min_cluster_size               = 5,\n",
    "                                                                                                                       min_samples                    = None,\n",
    "                                                                                                                       metric                         = 'euclidean',\n",
    "                                                                                                                       alpha                          = 1.0,\n",
    "                                                                                                                       p                              = None,\n",
    "                                                                                                                       algorithm                      = 'best',\n",
    "                                                                                                                       leaf_size                      = 40,\n",
    "                                                                                                                       memory                         = Memory(location=None),\n",
    "                                                                                                                       approx_min_span_tree           = True,\n",
    "                                                                                                                       gen_min_span_tree              = False,\n",
    "                                                                                                                       core_dist_n_jobs               = 1,\n",
    "                                                                                                                       cluster_selection_method       = 'eom',\n",
    "                                                                                                                       allow_single_cluster           = False,\n",
    "                                                                                                                       prediction_data                = False,\n",
    "                                                                                                                       match_reference_implementation = False),\n",
    "                                                                               \"VanillaAutoencoder\"      : \"Dell bt ghi gi o day de tuning @@@\"\n",
    "                                                                               }),\n",
    "                                                          factor       = 1.5,   # Stick with Prophet model\n",
    "                                                          window_size  = 10,    # Stick with Agglomerative Clustering & DBSCAN model\n",
    "                                                          dendrogram   = False, # Stick with Agglomerative Clustering model\n",
    "                                                          display      = True,\n",
    "                                                          start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                                          end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                                          freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Noi Bai</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots.plot_feature_outliers_over_time(data         = station[\"NB\"],\n",
    "#                                       data_cols    = numerical_features,\n",
    "#                                       station_name = \"NoiBai\",\n",
    "#                                       method       = \"machine_learning\",\n",
    "#                                       models       = dict({\n",
    "#                                                            \"IsolationForest\"         : IsolationForest(bootstrap     = False, \n",
    "#                                                                                                        contamination = \"auto\", \n",
    "#                                                                                                        max_features  = 1.0, \n",
    "#                                                                                                        max_samples   = \"auto\", \n",
    "#                                                                                                        n_estimators  = 100, \n",
    "#                                                                                                        n_jobs        = None, \n",
    "#                                                                                                        random_state  = None, \n",
    "#                                                                                                        verbose       = 0, \n",
    "#                                                                                                        warm_start    = False),\n",
    "#                                                            \"LocalOutlierFactor\"      : LocalOutlierFactor(algorithm     = \"auto\", \n",
    "#                                                                                                           contamination = \"auto\", \n",
    "#                                                                                                           leaf_size     = 30, \n",
    "#                                                                                                           metric        = \"minkowski\", \n",
    "#                                                                                                           metric_params = None, \n",
    "#                                                                                                           n_jobs        = None, \n",
    "#                                                                                                           n_neighbors   = 20, \n",
    "#                                                                                                           novelty       = False, \n",
    "#                                                                                                           p             = 2),\n",
    "#                                                            \"Prophet\"                 : Prophet(growth                  = \"linear\", # SLOW!!!\n",
    "#                                                                                                changepoints            = None,\n",
    "#                                                                                                n_changepoints          = 25,\n",
    "#                                                                                                changepoint_range       = 0.8,\n",
    "#                                                                                                yearly_seasonality      = \"auto\",\n",
    "#                                                                                                weekly_seasonality      = \"auto\",\n",
    "#                                                                                                daily_seasonality       = \"auto\",\n",
    "#                                                                                                holidays                = None,\n",
    "#                                                                                                seasonality_mode        = \"additive\",\n",
    "#                                                                                                seasonality_prior_scale = 10.0,\n",
    "#                                                                                                holidays_prior_scale    = 10.0,\n",
    "#                                                                                                changepoint_prior_scale = 0.05,\n",
    "#                                                                                                mcmc_samples            = 0,\n",
    "#                                                                                                interval_width          = 0.80,\n",
    "#                                                                                                uncertainty_samples     = 1000,\n",
    "#                                                                                                stan_backend            = None),\n",
    "#                                                            \"AgglomerativeClustering\" : AgglomerativeClustering(compute_distances  = False,  # SLOW!!!\n",
    "#                                                                                                                compute_full_tree  = 'auto', \n",
    "#                                                                                                             #    connectivity       = None, # KHONG SUA\n",
    "#                                                                                                                distance_threshold = None, \n",
    "#                                                                                                             #    linkage            = 'ward', # KHONG SUA\n",
    "#                                                                                                                memory             = None, \n",
    "#                                                                                                                metric             = 'euclidean', \n",
    "#                                                                                                                n_clusters         = 3),\n",
    "#                                                            \"HDBSCAN\"                  : HDBSCAN(min_cluster_size               = 5,\n",
    "#                                                                                                                        min_samples                    = None,\n",
    "#                                                                                                                        metric                         = 'euclidean',\n",
    "#                                                                                                                        alpha                          = 1.0,\n",
    "#                                                                                                                        p                              = None,\n",
    "#                                                                                                                        algorithm                      = 'best',\n",
    "#                                                                                                                        leaf_size                      = 40,\n",
    "#                                                                                                                        memory                         = Memory(location=None),\n",
    "#                                                                                                                        approx_min_span_tree           = True,\n",
    "#                                                                                                                        gen_min_span_tree              = False,\n",
    "#                                                                                                                        core_dist_n_jobs               = 1,\n",
    "#                                                                                                                        cluster_selection_method       = 'eom',\n",
    "#                                                                                                                        allow_single_cluster           = False,\n",
    "#                                                                                                                        prediction_data                = False,\n",
    "#                                                                                                                        match_reference_implementation = False),\n",
    "#                                                            \"VanillaAutoencoder\"      : \"Dell bt ghi gi o day de tuning @@@\"\n",
    "#                                                            }),\n",
    "#                                       factor       = 1.5,   # Stick with Prophet model\n",
    "#                                       window_size  = 10,    # Stick with Agglomerative Clustering & DBSCAN model\n",
    "#                                       dendrogram   = False, # Stick with Agglomerative Clustering model\n",
    "#                                       display      = False,\n",
    "#                                       start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "#                                       end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "#                                       freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"NB\"] = dataset.handle_feature_outliers_over_time(data         = station[\"NB\"],\n",
    "                                                          data_cols    = numerical_features,\n",
    "                                                          station_name = \"NoiBai\",\n",
    "                                                          method       = \"machine_learning\",\n",
    "                                                          models       = dict({\n",
    "                                                                               \"IsolationForest\"         : IsolationForest(bootstrap     = False, \n",
    "                                                                                                                           contamination = \"auto\", \n",
    "                                                                                                                           max_features  = 1.0, \n",
    "                                                                                                                           max_samples   = \"auto\", \n",
    "                                                                                                                           n_estimators  = 100, \n",
    "                                                                                                                           n_jobs        = None, \n",
    "                                                                                                                           random_state  = None, \n",
    "                                                                                                                           verbose       = 0, \n",
    "                                                                                                                           warm_start    = False),\n",
    "                                                                               \"LocalOutlierFactor\"      : LocalOutlierFactor(algorithm     = \"auto\", \n",
    "                                                                                                                              contamination = \"auto\", \n",
    "                                                                                                                              leaf_size     = 30, \n",
    "                                                                                                                              metric        = \"minkowski\", \n",
    "                                                                                                                              metric_params = None, \n",
    "                                                                                                                              n_jobs        = None, \n",
    "                                                                                                                              n_neighbors   = 20, \n",
    "                                                                                                                              novelty       = False, \n",
    "                                                                                                                              p             = 2),\n",
    "                                                                               \"Prophet\"                 : Prophet(growth                  = \"linear\", # SLOW!!!\n",
    "                                                                                                                   changepoints            = None,\n",
    "                                                                                                                   n_changepoints          = 25,\n",
    "                                                                                                                   changepoint_range       = 0.8,\n",
    "                                                                                                                   yearly_seasonality      = \"auto\",\n",
    "                                                                                                                   weekly_seasonality      = \"auto\",\n",
    "                                                                                                                   daily_seasonality       = \"auto\",\n",
    "                                                                                                                   holidays                = None,\n",
    "                                                                                                                   seasonality_mode        = \"additive\",\n",
    "                                                                                                                   seasonality_prior_scale = 10.0,\n",
    "                                                                                                                   holidays_prior_scale    = 10.0,\n",
    "                                                                                                                   changepoint_prior_scale = 0.05,\n",
    "                                                                                                                   mcmc_samples            = 0,\n",
    "                                                                                                                   interval_width          = 0.80,\n",
    "                                                                                                                   uncertainty_samples     = 1000,\n",
    "                                                                                                                   stan_backend            = None),\n",
    "                                                                               \"AgglomerativeClustering\" : AgglomerativeClustering(compute_distances  = False,  # SLOW!!!\n",
    "                                                                                                                                   compute_full_tree  = 'auto', \n",
    "                                                                                                                                #    connectivity       = None, # KHONG SUA\n",
    "                                                                                                                                   distance_threshold = None, \n",
    "                                                                                                                                #    linkage            = 'ward', # KHONG SUA\n",
    "                                                                                                                                   memory             = None, \n",
    "                                                                                                                                   metric             = 'euclidean', \n",
    "                                                                                                                                   n_clusters         = 3),\n",
    "                                                                               \"HDBSCAN\"                  : HDBSCAN(min_cluster_size               = 5,\n",
    "                                                                                                                       min_samples                    = None,\n",
    "                                                                                                                       metric                         = 'euclidean',\n",
    "                                                                                                                       alpha                          = 1.0,\n",
    "                                                                                                                       p                              = None,\n",
    "                                                                                                                       algorithm                      = 'best',\n",
    "                                                                                                                       leaf_size                      = 40,\n",
    "                                                                                                                       memory                         = Memory(location=None),\n",
    "                                                                                                                       approx_min_span_tree           = True,\n",
    "                                                                                                                       gen_min_span_tree              = False,\n",
    "                                                                                                                       core_dist_n_jobs               = 1,\n",
    "                                                                                                                       cluster_selection_method       = 'eom',\n",
    "                                                                                                                       allow_single_cluster           = False,\n",
    "                                                                                                                       prediction_data                = False,\n",
    "                                                                                                                       match_reference_implementation = False),\n",
    "                                                                               \"VanillaAutoencoder\"      : \"Dell bt ghi gi o day de tuning @@@\"\n",
    "                                                                               }),\n",
    "                                                          factor       = 1.5,   # Stick with Prophet model\n",
    "                                                          window_size  = 10,    # Stick with Agglomerative Clustering & DBSCAN model\n",
    "                                                          dendrogram   = False, # Stick with Agglomerative Clustering model\n",
    "                                                          display      = True,\n",
    "                                                          start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                                          end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                                          freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Quy Nhon</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots.plot_feature_outliers_over_time(data         = station[\"QN\"],\n",
    "#                                       data_cols    = numerical_features,\n",
    "#                                       station_name = \"QuyNhon\",\n",
    "#                                       method       = \"machine_learning\",\n",
    "#                                       models       = dict({\n",
    "#                                                            \"IsolationForest\"         : IsolationForest(bootstrap     = False, \n",
    "#                                                                                                        contamination = \"auto\", \n",
    "#                                                                                                        max_features  = 1.0, \n",
    "#                                                                                                        max_samples   = \"auto\", \n",
    "#                                                                                                        n_estimators  = 100, \n",
    "#                                                                                                        n_jobs        = None, \n",
    "#                                                                                                        random_state  = None, \n",
    "#                                                                                                        verbose       = 0, \n",
    "#                                                                                                        warm_start    = False),\n",
    "#                                                            \"LocalOutlierFactor\"      : LocalOutlierFactor(algorithm     = \"auto\", \n",
    "#                                                                                                           contamination = \"auto\", \n",
    "#                                                                                                           leaf_size     = 30, \n",
    "#                                                                                                           metric        = \"minkowski\", \n",
    "#                                                                                                           metric_params = None, \n",
    "#                                                                                                           n_jobs        = None, \n",
    "#                                                                                                           n_neighbors   = 20, \n",
    "#                                                                                                           novelty       = False, \n",
    "#                                                                                                           p             = 2),\n",
    "#                                                            \"Prophet\"                 : Prophet(growth                  = \"linear\", # SLOW!!!\n",
    "#                                                                                                changepoints            = None,\n",
    "#                                                                                                n_changepoints          = 25,\n",
    "#                                                                                                changepoint_range       = 0.8,\n",
    "#                                                                                                yearly_seasonality      = \"auto\",\n",
    "#                                                                                                weekly_seasonality      = \"auto\",\n",
    "#                                                                                                daily_seasonality       = \"auto\",\n",
    "#                                                                                                holidays                = None,\n",
    "#                                                                                                seasonality_mode        = \"additive\",\n",
    "#                                                                                                seasonality_prior_scale = 10.0,\n",
    "#                                                                                                holidays_prior_scale    = 10.0,\n",
    "#                                                                                                changepoint_prior_scale = 0.05,\n",
    "#                                                                                                mcmc_samples            = 0,\n",
    "#                                                                                                interval_width          = 0.80,\n",
    "#                                                                                                uncertainty_samples     = 1000,\n",
    "#                                                                                                stan_backend            = None),\n",
    "#                                                            \"AgglomerativeClustering\" : AgglomerativeClustering(compute_distances  = False,  # SLOW!!!\n",
    "#                                                                                                                compute_full_tree  = 'auto', \n",
    "#                                                                                                             #    connectivity       = None, # KHONG SUA\n",
    "#                                                                                                                distance_threshold = None, \n",
    "#                                                                                                             #    linkage            = 'ward', # KHONG SUA\n",
    "#                                                                                                                memory             = None, \n",
    "#                                                                                                                metric             = 'euclidean', \n",
    "#                                                                                                                n_clusters         = 3),\n",
    "#                                                            \"HDBSCAN\"                  : HDBSCAN(min_cluster_size               = 5,\n",
    "#                                                                                                                        min_samples                    = None,\n",
    "#                                                                                                                        metric                         = 'euclidean',\n",
    "#                                                                                                                        alpha                          = 1.0,\n",
    "#                                                                                                                        p                              = None,\n",
    "#                                                                                                                        algorithm                      = 'best',\n",
    "#                                                                                                                        leaf_size                      = 40,\n",
    "#                                                                                                                        memory                         = Memory(location=None),\n",
    "#                                                                                                                        approx_min_span_tree           = True,\n",
    "#                                                                                                                        gen_min_span_tree              = False,\n",
    "#                                                                                                                        core_dist_n_jobs               = 1,\n",
    "#                                                                                                                        cluster_selection_method       = 'eom',\n",
    "#                                                                                                                        allow_single_cluster           = False,\n",
    "#                                                                                                                        prediction_data                = False,\n",
    "#                                                                                                                        match_reference_implementation = False),\n",
    "#                                                            \"VanillaAutoencoder\"      : \"Dell bt ghi gi o day de tuning @@@\"\n",
    "#                                                            }),\n",
    "#                                       factor       = 1.5,   # Stick with Prophet model\n",
    "#                                       window_size  = 10,    # Stick with Agglomerative Clustering & DBSCAN model\n",
    "#                                       dendrogram   = False, # Stick with Agglomerative Clustering model\n",
    "#                                       display      = False,\n",
    "#                                       start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "#                                       end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "#                                       freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"QN\"] = dataset.handle_feature_outliers_over_time(data         = station[\"QN\"],\n",
    "                                                          data_cols    = numerical_features,\n",
    "                                                          station_name = \"QuyNhon\",\n",
    "                                                          method       = \"machine_learning\",\n",
    "                                                          models       = dict({\n",
    "                                                                               \"IsolationForest\"         : IsolationForest(bootstrap     = False, \n",
    "                                                                                                                           contamination = \"auto\", \n",
    "                                                                                                                           max_features  = 1.0, \n",
    "                                                                                                                           max_samples   = \"auto\", \n",
    "                                                                                                                           n_estimators  = 100, \n",
    "                                                                                                                           n_jobs        = None, \n",
    "                                                                                                                           random_state  = None, \n",
    "                                                                                                                           verbose       = 0, \n",
    "                                                                                                                           warm_start    = False),\n",
    "                                                                               \"LocalOutlierFactor\"      : LocalOutlierFactor(algorithm     = \"auto\", \n",
    "                                                                                                                              contamination = \"auto\", \n",
    "                                                                                                                              leaf_size     = 30, \n",
    "                                                                                                                              metric        = \"minkowski\", \n",
    "                                                                                                                              metric_params = None, \n",
    "                                                                                                                              n_jobs        = None, \n",
    "                                                                                                                              n_neighbors   = 20, \n",
    "                                                                                                                              novelty       = False, \n",
    "                                                                                                                              p             = 2),\n",
    "                                                                               \"Prophet\"                 : Prophet(growth                  = \"linear\", # SLOW!!!\n",
    "                                                                                                                   changepoints            = None,\n",
    "                                                                                                                   n_changepoints          = 25,\n",
    "                                                                                                                   changepoint_range       = 0.8,\n",
    "                                                                                                                   yearly_seasonality      = \"auto\",\n",
    "                                                                                                                   weekly_seasonality      = \"auto\",\n",
    "                                                                                                                   daily_seasonality       = \"auto\",\n",
    "                                                                                                                   holidays                = None,\n",
    "                                                                                                                   seasonality_mode        = \"additive\",\n",
    "                                                                                                                   seasonality_prior_scale = 10.0,\n",
    "                                                                                                                   holidays_prior_scale    = 10.0,\n",
    "                                                                                                                   changepoint_prior_scale = 0.05,\n",
    "                                                                                                                   mcmc_samples            = 0,\n",
    "                                                                                                                   interval_width          = 0.80,\n",
    "                                                                                                                   uncertainty_samples     = 1000,\n",
    "                                                                                                                   stan_backend            = None),\n",
    "                                                                               \"AgglomerativeClustering\" : AgglomerativeClustering(compute_distances  = False,  # SLOW!!!\n",
    "                                                                                                                                   compute_full_tree  = 'auto', \n",
    "                                                                                                                                #    connectivity       = None, # KHONG SUA\n",
    "                                                                                                                                   distance_threshold = None, \n",
    "                                                                                                                                #    linkage            = 'ward', # KHONG SUA\n",
    "                                                                                                                                   memory             = None, \n",
    "                                                                                                                                   metric             = 'euclidean', \n",
    "                                                                                                                                   n_clusters         = 3),\n",
    "                                                                               \"HDBSCAN\"                  : HDBSCAN(min_cluster_size               = 5,\n",
    "                                                                                                                       min_samples                    = None,\n",
    "                                                                                                                       metric                         = 'euclidean',\n",
    "                                                                                                                       alpha                          = 1.0,\n",
    "                                                                                                                       p                              = None,\n",
    "                                                                                                                       algorithm                      = 'best',\n",
    "                                                                                                                       leaf_size                      = 40,\n",
    "                                                                                                                       memory                         = Memory(location=None),\n",
    "                                                                                                                       approx_min_span_tree           = True,\n",
    "                                                                                                                       gen_min_span_tree              = False,\n",
    "                                                                                                                       core_dist_n_jobs               = 1,\n",
    "                                                                                                                       cluster_selection_method       = 'eom',\n",
    "                                                                                                                       allow_single_cluster           = False,\n",
    "                                                                                                                       prediction_data                = False,\n",
    "                                                                                                                       match_reference_implementation = False),\n",
    "                                                                               \"VanillaAutoencoder\"      : \"Dell bt ghi gi o day de tuning @@@\"\n",
    "                                                                               }),\n",
    "                                                          factor       = 1.5,   # Stick with Prophet model\n",
    "                                                          window_size  = 10,    # Stick with Agglomerative Clustering & DBSCAN model\n",
    "                                                          dendrogram   = False, # Stick with Agglomerative Clustering model\n",
    "                                                          display      = True,\n",
    "                                                          start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                                          end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                                          freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Thanh Hoa</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots.plot_feature_outliers_over_time(data         = station[\"TH\"],\n",
    "#                                       data_cols    = numerical_features,\n",
    "#                                       station_name = \"ThanhHoa\",\n",
    "#                                       method       = \"machine_learning\",\n",
    "#                                       models       = dict({\n",
    "#                                                            \"IsolationForest\"         : IsolationForest(bootstrap     = False, \n",
    "#                                                                                                        contamination = \"auto\", \n",
    "#                                                                                                        max_features  = 1.0, \n",
    "#                                                                                                        max_samples   = \"auto\", \n",
    "#                                                                                                        n_estimators  = 100, \n",
    "#                                                                                                        n_jobs        = None, \n",
    "#                                                                                                        random_state  = None, \n",
    "#                                                                                                        verbose       = 0, \n",
    "#                                                                                                        warm_start    = False),\n",
    "#                                                            \"LocalOutlierFactor\"      : LocalOutlierFactor(algorithm     = \"auto\", \n",
    "#                                                                                                           contamination = \"auto\", \n",
    "#                                                                                                           leaf_size     = 30, \n",
    "#                                                                                                           metric        = \"minkowski\", \n",
    "#                                                                                                           metric_params = None, \n",
    "#                                                                                                           n_jobs        = None, \n",
    "#                                                                                                           n_neighbors   = 20, \n",
    "#                                                                                                           novelty       = False, \n",
    "#                                                                                                           p             = 2),\n",
    "#                                                            \"Prophet\"                 : Prophet(growth                  = \"linear\", # SLOW!!!\n",
    "#                                                                                                changepoints            = None,\n",
    "#                                                                                                n_changepoints          = 25,\n",
    "#                                                                                                changepoint_range       = 0.8,\n",
    "#                                                                                                yearly_seasonality      = \"auto\",\n",
    "#                                                                                                weekly_seasonality      = \"auto\",\n",
    "#                                                                                                daily_seasonality       = \"auto\",\n",
    "#                                                                                                holidays                = None,\n",
    "#                                                                                                seasonality_mode        = \"additive\",\n",
    "#                                                                                                seasonality_prior_scale = 10.0,\n",
    "#                                                                                                holidays_prior_scale    = 10.0,\n",
    "#                                                                                                changepoint_prior_scale = 0.05,\n",
    "#                                                                                                mcmc_samples            = 0,\n",
    "#                                                                                                interval_width          = 0.80,\n",
    "#                                                                                                uncertainty_samples     = 1000,\n",
    "#                                                                                                stan_backend            = None),\n",
    "#                                                            \"AgglomerativeClustering\" : AgglomerativeClustering(compute_distances  = False,  # SLOW!!!\n",
    "#                                                                                                                compute_full_tree  = 'auto', \n",
    "#                                                                                                             #    connectivity       = None, # KHONG SUA\n",
    "#                                                                                                                distance_threshold = None, \n",
    "#                                                                                                             #    linkage            = 'ward', # KHONG SUA\n",
    "#                                                                                                                memory             = None, \n",
    "#                                                                                                                metric             = 'euclidean', \n",
    "#                                                                                                                n_clusters         = 3),\n",
    "#                                                            \"HDBSCAN\"                  : HDBSCAN(min_cluster_size               = 5,\n",
    "#                                                                                                                        min_samples                    = None,\n",
    "#                                                                                                                        metric                         = 'euclidean',\n",
    "#                                                                                                                        alpha                          = 1.0,\n",
    "#                                                                                                                        p                              = None,\n",
    "#                                                                                                                        algorithm                      = 'best',\n",
    "#                                                                                                                        leaf_size                      = 40,\n",
    "#                                                                                                                        memory                         = Memory(location=None),\n",
    "#                                                                                                                        approx_min_span_tree           = True,\n",
    "#                                                                                                                        gen_min_span_tree              = False,\n",
    "#                                                                                                                        core_dist_n_jobs               = 1,\n",
    "#                                                                                                                        cluster_selection_method       = 'eom',\n",
    "#                                                                                                                        allow_single_cluster           = False,\n",
    "#                                                                                                                        prediction_data                = False,\n",
    "#                                                                                                                        match_reference_implementation = False),\n",
    "#                                                            \"VanillaAutoencoder\"      : \"Dell bt ghi gi o day de tuning @@@\"\n",
    "#                                                            }),\n",
    "#                                       factor       = 1.5,   # Stick with Prophet model\n",
    "#                                       window_size  = 10,    # Stick with Agglomerative Clustering & DBSCAN model\n",
    "#                                       dendrogram   = False, # Stick with Agglomerative Clustering model\n",
    "#                                       display      = False,\n",
    "#                                       start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "#                                       end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "#                                       freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"TH\"] = dataset.handle_feature_outliers_over_time(data         = station[\"TH\"],\n",
    "                                                          data_cols    = numerical_features,\n",
    "                                                          station_name = \"ThanhHoa\",\n",
    "                                                          method       = \"machine_learning\",\n",
    "                                                          models       = dict({\n",
    "                                                                               \"IsolationForest\"         : IsolationForest(bootstrap     = False, \n",
    "                                                                                                                           contamination = \"auto\", \n",
    "                                                                                                                           max_features  = 1.0, \n",
    "                                                                                                                           max_samples   = \"auto\", \n",
    "                                                                                                                           n_estimators  = 100, \n",
    "                                                                                                                           n_jobs        = None, \n",
    "                                                                                                                           random_state  = None, \n",
    "                                                                                                                           verbose       = 0, \n",
    "                                                                                                                           warm_start    = False),\n",
    "                                                                               \"LocalOutlierFactor\"      : LocalOutlierFactor(algorithm     = \"auto\", \n",
    "                                                                                                                              contamination = \"auto\", \n",
    "                                                                                                                              leaf_size     = 30, \n",
    "                                                                                                                              metric        = \"minkowski\", \n",
    "                                                                                                                              metric_params = None, \n",
    "                                                                                                                              n_jobs        = None, \n",
    "                                                                                                                              n_neighbors   = 20, \n",
    "                                                                                                                              novelty       = False, \n",
    "                                                                                                                              p             = 2),\n",
    "                                                                               \"Prophet\"                 : Prophet(growth                  = \"linear\", # SLOW!!!\n",
    "                                                                                                                   changepoints            = None,\n",
    "                                                                                                                   n_changepoints          = 25,\n",
    "                                                                                                                   changepoint_range       = 0.8,\n",
    "                                                                                                                   yearly_seasonality      = \"auto\",\n",
    "                                                                                                                   weekly_seasonality      = \"auto\",\n",
    "                                                                                                                   daily_seasonality       = \"auto\",\n",
    "                                                                                                                   holidays                = None,\n",
    "                                                                                                                   seasonality_mode        = \"additive\",\n",
    "                                                                                                                   seasonality_prior_scale = 10.0,\n",
    "                                                                                                                   holidays_prior_scale    = 10.0,\n",
    "                                                                                                                   changepoint_prior_scale = 0.05,\n",
    "                                                                                                                   mcmc_samples            = 0,\n",
    "                                                                                                                   interval_width          = 0.80,\n",
    "                                                                                                                   uncertainty_samples     = 1000,\n",
    "                                                                                                                   stan_backend            = None),\n",
    "                                                                               \"AgglomerativeClustering\" : AgglomerativeClustering(compute_distances  = False,  # SLOW!!!\n",
    "                                                                                                                                   compute_full_tree  = 'auto', \n",
    "                                                                                                                                #    connectivity       = None, # KHONG SUA\n",
    "                                                                                                                                   distance_threshold = None, \n",
    "                                                                                                                                #    linkage            = 'ward', # KHONG SUA\n",
    "                                                                                                                                   memory             = None, \n",
    "                                                                                                                                   metric             = 'euclidean', \n",
    "                                                                                                                                   n_clusters         = 3),\n",
    "                                                                               \"HDBSCAN\"                  : HDBSCAN(min_cluster_size               = 5,\n",
    "                                                                                                                       min_samples                    = None,\n",
    "                                                                                                                       metric                         = 'euclidean',\n",
    "                                                                                                                       alpha                          = 1.0,\n",
    "                                                                                                                       p                              = None,\n",
    "                                                                                                                       algorithm                      = 'best',\n",
    "                                                                                                                       leaf_size                      = 40,\n",
    "                                                                                                                       memory                         = Memory(location=None),\n",
    "                                                                                                                       approx_min_span_tree           = True,\n",
    "                                                                                                                       gen_min_span_tree              = False,\n",
    "                                                                                                                       core_dist_n_jobs               = 1,\n",
    "                                                                                                                       cluster_selection_method       = 'eom',\n",
    "                                                                                                                       allow_single_cluster           = False,\n",
    "                                                                                                                       prediction_data                = False,\n",
    "                                                                                                                       match_reference_implementation = False),\n",
    "                                                                               \"VanillaAutoencoder\"      : \"Dell bt ghi gi o day de tuning @@@\"\n",
    "                                                                               }),\n",
    "                                                          factor       = 1.5,   # Stick with Prophet model\n",
    "                                                          window_size  = 10,    # Stick with Agglomerative Clustering & DBSCAN model\n",
    "                                                          dendrogram   = False, # Stick with Agglomerative Clustering model\n",
    "                                                          display      = True,\n",
    "                                                          start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                                          end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                                          freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">TSN</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots.plot_feature_outliers_over_time(data         = station[\"TSN\"],\n",
    "#                                       data_cols    = numerical_features,\n",
    "#                                       station_name = \"TSN\",\n",
    "#                                       method       = \"machine_learning\",\n",
    "#                                       models       = dict({\n",
    "#                                                            \"IsolationForest\"         : IsolationForest(bootstrap     = False, \n",
    "#                                                                                                        contamination = \"auto\", \n",
    "#                                                                                                        max_features  = 1.0, \n",
    "#                                                                                                        max_samples   = \"auto\", \n",
    "#                                                                                                        n_estimators  = 100, \n",
    "#                                                                                                        n_jobs        = None, \n",
    "#                                                                                                        random_state  = None, \n",
    "#                                                                                                        verbose       = 0, \n",
    "#                                                                                                        warm_start    = False),\n",
    "#                                                            \"LocalOutlierFactor\"      : LocalOutlierFactor(algorithm     = \"auto\", \n",
    "#                                                                                                           contamination = \"auto\", \n",
    "#                                                                                                           leaf_size     = 30, \n",
    "#                                                                                                           metric        = \"minkowski\", \n",
    "#                                                                                                           metric_params = None, \n",
    "#                                                                                                           n_jobs        = None, \n",
    "#                                                                                                           n_neighbors   = 20, \n",
    "#                                                                                                           novelty       = False, \n",
    "#                                                                                                           p             = 2),\n",
    "#                                                            \"Prophet\"                 : Prophet(growth                  = \"linear\", # SLOW!!!\n",
    "#                                                                                                changepoints            = None,\n",
    "#                                                                                                n_changepoints          = 25,\n",
    "#                                                                                                changepoint_range       = 0.8,\n",
    "#                                                                                                yearly_seasonality      = \"auto\",\n",
    "#                                                                                                weekly_seasonality      = \"auto\",\n",
    "#                                                                                                daily_seasonality       = \"auto\",\n",
    "#                                                                                                holidays                = None,\n",
    "#                                                                                                seasonality_mode        = \"additive\",\n",
    "#                                                                                                seasonality_prior_scale = 10.0,\n",
    "#                                                                                                holidays_prior_scale    = 10.0,\n",
    "#                                                                                                changepoint_prior_scale = 0.05,\n",
    "#                                                                                                mcmc_samples            = 0,\n",
    "#                                                                                                interval_width          = 0.80,\n",
    "#                                                                                                uncertainty_samples     = 1000,\n",
    "#                                                                                                stan_backend            = None),\n",
    "#                                                            \"AgglomerativeClustering\" : AgglomerativeClustering(compute_distances  = False,  # SLOW!!!\n",
    "#                                                                                                                compute_full_tree  = 'auto', \n",
    "#                                                                                                             #    connectivity       = None, # KHONG SUA\n",
    "#                                                                                                                distance_threshold = None, \n",
    "#                                                                                                             #    linkage            = 'ward', # KHONG SUA\n",
    "#                                                                                                                memory             = None, \n",
    "#                                                                                                                metric             = 'euclidean', \n",
    "#                                                                                                                n_clusters         = 3),\n",
    "#                                                            \"HDBSCAN\"                  : HDBSCAN(min_cluster_size               = 5,\n",
    "#                                                                                                                        min_samples                    = None,\n",
    "#                                                                                                                        metric                         = 'euclidean',\n",
    "#                                                                                                                        alpha                          = 1.0,\n",
    "#                                                                                                                        p                              = None,\n",
    "#                                                                                                                        algorithm                      = 'best',\n",
    "#                                                                                                                        leaf_size                      = 40,\n",
    "#                                                                                                                        memory                         = Memory(location=None),\n",
    "#                                                                                                                        approx_min_span_tree           = True,\n",
    "#                                                                                                                        gen_min_span_tree              = False,\n",
    "#                                                                                                                        core_dist_n_jobs               = 1,\n",
    "#                                                                                                                        cluster_selection_method       = 'eom',\n",
    "#                                                                                                                        allow_single_cluster           = False,\n",
    "#                                                                                                                        prediction_data                = False,\n",
    "#                                                                                                                        match_reference_implementation = False),\n",
    "#                                                            \"VanillaAutoencoder\"      : \"Dell bt ghi gi o day de tuning @@@\"\n",
    "#                                                            }),\n",
    "#                                       factor       = 1.5,   # Stick with Prophet model\n",
    "#                                       window_size  = 10,    # Stick with Agglomerative Clustering & DBSCAN model\n",
    "#                                       dendrogram   = False, # Stick with Agglomerative Clustering model\n",
    "#                                       display      = False,\n",
    "#                                       start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "#                                       end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "#                                       freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"TSN\"] = dataset.handle_feature_outliers_over_time(data         = station[\"TSN\"],\n",
    "                                                           data_cols    = numerical_features,\n",
    "                                                           station_name = \"TSN\",\n",
    "                                                           method       = \"machine_learning\",\n",
    "                                                           models       = dict({\n",
    "                                                                                \"IsolationForest\"         : IsolationForest(bootstrap     = False, \n",
    "                                                                                                                            contamination = \"auto\", \n",
    "                                                                                                                            max_features  = 1.0, \n",
    "                                                                                                                            max_samples   = \"auto\", \n",
    "                                                                                                                            n_estimators  = 100, \n",
    "                                                                                                                            n_jobs        = None, \n",
    "                                                                                                                            random_state  = None, \n",
    "                                                                                                                            verbose       = 0, \n",
    "                                                                                                                            warm_start    = False),\n",
    "                                                                                \"LocalOutlierFactor\"      : LocalOutlierFactor(algorithm     = \"auto\", \n",
    "                                                                                                                               contamination = \"auto\", \n",
    "                                                                                                                               leaf_size     = 30, \n",
    "                                                                                                                               metric        = \"minkowski\", \n",
    "                                                                                                                               metric_params = None, \n",
    "                                                                                                                               n_jobs        = None, \n",
    "                                                                                                                               n_neighbors   = 20, \n",
    "                                                                                                                               novelty       = False, \n",
    "                                                                                                                               p             = 2),\n",
    "                                                                                \"Prophet\"                 : Prophet(growth                  = \"linear\", # SLOW!!!\n",
    "                                                                                                                    changepoints            = None,\n",
    "                                                                                                                    n_changepoints          = 25,\n",
    "                                                                                                                    changepoint_range       = 0.8,\n",
    "                                                                                                                    yearly_seasonality      = \"auto\",\n",
    "                                                                                                                    weekly_seasonality      = \"auto\",\n",
    "                                                                                                                    daily_seasonality       = \"auto\",\n",
    "                                                                                                                    holidays                = None,\n",
    "                                                                                                                    seasonality_mode        = \"additive\",\n",
    "                                                                                                                    seasonality_prior_scale = 10.0,\n",
    "                                                                                                                    holidays_prior_scale    = 10.0,\n",
    "                                                                                                                    changepoint_prior_scale = 0.05,\n",
    "                                                                                                                    mcmc_samples            = 0,\n",
    "                                                                                                                    interval_width          = 0.80,\n",
    "                                                                                                                    uncertainty_samples     = 1000,\n",
    "                                                                                                                    stan_backend            = None),\n",
    "                                                                                \"AgglomerativeClustering\" : AgglomerativeClustering(compute_distances  = False,  # SLOW!!!\n",
    "                                                                                                                                    compute_full_tree  = 'auto', \n",
    "                                                                                                                                 #    connectivity       = None, # KHONG SUA\n",
    "                                                                                                                                    distance_threshold = None, \n",
    "                                                                                                                                 #    linkage            = 'ward', # KHONG SUA\n",
    "                                                                                                                                    memory             = None, \n",
    "                                                                                                                                    metric             = 'euclidean', \n",
    "                                                                                                                                    n_clusters         = 3),\n",
    "                                                                                \"HDBSCAN\"                  : HDBSCAN(min_cluster_size               = 5,\n",
    "                                                                                                                       min_samples                    = None,\n",
    "                                                                                                                       metric                         = 'euclidean',\n",
    "                                                                                                                       alpha                          = 1.0,\n",
    "                                                                                                                       p                              = None,\n",
    "                                                                                                                       algorithm                      = 'best',\n",
    "                                                                                                                       leaf_size                      = 40,\n",
    "                                                                                                                       memory                         = Memory(location=None),\n",
    "                                                                                                                       approx_min_span_tree           = True,\n",
    "                                                                                                                       gen_min_span_tree              = False,\n",
    "                                                                                                                       core_dist_n_jobs               = 1,\n",
    "                                                                                                                       cluster_selection_method       = 'eom',\n",
    "                                                                                                                       allow_single_cluster           = False,\n",
    "                                                                                                                       prediction_data                = False,\n",
    "                                                                                                                       match_reference_implementation = False),\n",
    "                                                                                \"VanillaAutoencoder\"      : \"Dell bt ghi gi o day de tuning @@@\"\n",
    "                                                                                }),\n",
    "                                                           factor       = 1.5,   # Stick with Prophet model\n",
    "                                                           window_size  = 10,    # Stick with Agglomerative Clustering & DBSCAN model\n",
    "                                                           dendrogram   = False, # Stick with Agglomerative Clustering model\n",
    "                                                           display      = True,\n",
    "                                                           start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                                           end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                                           freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import tensorflow as tf\n",
    "# pd.options.mode.chained_assignment = None\n",
    "# from matplotlib.pylab import rcParams\n",
    "# import importlib\n",
    "# importlib.reload(dataset)\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# sns.set_theme(style='whitegrid', palette='muted')\n",
    "# rcParams['figure.figsize'] = 14, 8\n",
    "# np.random.seed(1)\n",
    "# tf.random.set_seed(1)\n",
    "\n",
    "# df = station[\"TSN\"]\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feature in numerical_features:\n",
    "#     plt.figure(figsize=(14, 6))\n",
    "#     sns.lineplot(data=df, x='time', y=feature, label=feature)\n",
    "#     plt.xlabel('Time')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.legend()\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = int(len(df) * 0.8)\n",
    "# test_size = len(df) - train_size\n",
    "# train, test = df.iloc[0:train_size], df.iloc[train_size:len(df)]\n",
    "# print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_sp = dataset.LSTMAutoencoder()\n",
    "# detector_tcc = dataset.LSTMAutoencoder()\n",
    "# detector_tp = dataset.LSTMAutoencoder()\n",
    "# detector_u10 = dataset.LSTMAutoencoder()\n",
    "# detector_v10 = dataset.LSTMAutoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_sp.prepare_data(train, test, 'sp', 24)\n",
    "# detector_tcc.prepare_data(train, test, 'tcc', 24)\n",
    "# detector_tp.prepare_data(train, test, 'tp', 24)\n",
    "# detector_u10.prepare_data(train, test, 'u10', 24)\n",
    "# detector_v10.prepare_data(train, test, 'v10', 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_sp.train(epochs=20, batch_size=256, model_path='/TSN/sp')\n",
    "# detector_tcc.train(epochs=20, batch_size=256, model_path='/TSN/tcc')\n",
    "# detector_tp.train(epochs=20, batch_size=256, model_path='/TSN/tp')\n",
    "# detector_u10.train(epochs=20, batch_size=256, model_path='/TSN/u10')\n",
    "# detector_v10.train(epochs=20, batch_size=256, model_path='/TSN/v10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_data=df[['time', 'sp']]\n",
    "# detector_sp.detect_anomalies(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_sp.plot_anomaly_scores(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_sp.reconstruct_and_replace_anomalies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_sp.plot_reconstruct_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_data=df[['time', 'tcc']]\n",
    "# detector_tcc.detect_anomalies(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_tcc.plot_anomaly_scores(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_tcc.reconstruct_and_replace_anomalies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_tcc.plot_reconstruct_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_data=df[['time', 'tp']]\n",
    "# detector_tp.detect_anomalies(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_tp.plot_anomaly_scores(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_tp.reconstruct_and_replace_anomalies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_tp.plot_reconstruct_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_data=df[['time', 'u10']]\n",
    "# detector_u10.detect_anomalies(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_u10.plot_anomaly_scores(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_u10.reconstruct_and_replace_anomalies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_u10.plot_reconstruct_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_data=df[['time', 'v10']]\n",
    "# detector_v10.detect_anomalies(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_v10.plot_anomaly_scores(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_v10.reconstruct_and_replace_anomalies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_v10.plot_reconstruct_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">5.2. Feature Engineering</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Tính hướng gió, vận tốc gió (gửi file Excel cô tính cho nhóm)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in station.items():\n",
    "    # Convert data u10, v10 sang dang m/s\n",
    "    u_unit = df[\"u10\"].to_numpy() * units(\"m/s\")\n",
    "    v_unit = df[\"v10\"].to_numpy() * units(\"m/s\")\n",
    "    \n",
    "    # Tinh wind direction, huong gio thoi la from\n",
    "    wind_direction_deg = metpy.calc.wind_direction(u          = u_unit,\n",
    "                                                   v          = v_unit,\n",
    "                                                   convention = \"from\")\n",
    "    \n",
    "    # Tinh wind speed\n",
    "    wind_speed = metpy.calc.wind_speed(u = u_unit,\n",
    "                                       v = v_unit)\n",
    "    \n",
    "    # Chuyen data wind speed tu dang series sang dang dataframe\n",
    "    wind_speed         = pd.DataFrame(data = wind_speed,         columns = [\"wind_speed\"])\n",
    "    wind_direction_deg = pd.DataFrame(data = wind_direction_deg, columns = [\"wind_direction_deg\"])\n",
    "    \n",
    "    station[name][\"wind_speed\"]         = wind_speed\n",
    "    station[name][\"wind_direction_deg\"] = wind_direction_deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"CaMau\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Time-based features (hour, day, month, season…)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in station.items():\n",
    "    station[name] = features.extract_time_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"CaMau\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Arrange columns</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"CaMau\"].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_pos = list(['time', 'latitude', 'longitude', 'ymd', 'year', 'month', 'day',\n",
    "                 'number', 'step', 'surface', 'sp', 'tcc', 'tp', 'u10', 'v10', 'wind_speed', 'wind_direction_deg'])\n",
    "\n",
    "for name, df in station.items():\n",
    "    station[name] = df[cols_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"CaMau\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">5.3. Save data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"TSN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo thư mục nếu chưa có\n",
    "os.makedirs(\"../data/processed/datasets\", exist_ok=True)\n",
    "\n",
    "# Duyệt từng station và lưu file\n",
    "for name, df in station.items():\n",
    "    file_name = f\"../data/processed/datasets/{name}90.24_cleaned.csv\"\n",
    "    df.to_csv(file_name, index=False)\n",
    "    print(f\"Đã lưu: {file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
