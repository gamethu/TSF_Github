{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style = \"color:orange\">1. Introduction<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">1.1. Project purpose</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X√¢y d·ª±ng v√† ƒë√°nh gi√° t·∫≠p h·ª£p c√°c m√¥ h√¨nh d·ª± b√°o chu·ªói th·ªùi gian ƒë·ªÉ d·ª± ƒëo√°n nhi·ªát ƒë·ªô c·ª±c ƒë·∫°i h√†ng ng√†y t·∫°i c√°c khu v·ª±c ƒë√¥ th·ªã v√† ven bi·ªÉn Vi·ªát Nam. D·ª± √°n √°p d·ª•ng c·∫£ thu·∫≠t to√°n Machine Learning truy·ªÅn th·ªëng v√† Deep Learning hi·ªán ƒë·∫°i nh·∫±m c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c d·ª± b√°o so v·ªõi c√°c ph∆∞∆°ng ph√°p th·ªëng k√™ th√¥ng th∆∞·ªùng.\n",
    "\n",
    "<b>Ngu·ªìn d·ªØ li·ªáu</b>: B·ªô d·ªØ li·ªáu ERA5 (ECMWF) v·ªõi c√°c b·∫£n ghi nhi·ªát ƒë·ªô t·ª´ 1990 ƒë·∫øn 2024, ƒë∆∞·ª£c x·ª≠ l√Ω v√† bi·∫øn ƒë·ªïi ƒë·ªÉ hu·∫•n luy·ªán c√°c m√¥ h√¨nh nh∆∞ Random Forest, XGBoost, LSTM, Transformer, TFT, v√† N-BEATS.\n",
    "\n",
    "<b>K·∫øt qu·∫£</b>: ƒê√°nh gi√° hi·ªáu nƒÉng gi·ªØa c√°c m√¥ h√¨nh qua nhi·ªÅu k·ªãch b·∫£n th·ª±c nghi·ªám v√† ƒë·ªÅ xu·∫•t h·ªá th·ªëng c·∫£nh b√°o nhi·ªát ƒë·ªô s·ªõm ·ª©ng d·ª•ng th·ª±c t·∫ø."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">1.2. Data source and description</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Th√¥ng tin d·ªØ li·ªáu trong ƒë·ªÅ t√†i:</h4>\n",
    "\n",
    "<ul>\n",
    "<li><b>Th·ªùi gian thu th·∫≠p:</b> t·ª´ nƒÉm <b>1990 ƒë·∫øn 2024</b></li>\n",
    "<li><b>ƒê·ªãnh d·∫°ng ban ƒë·∫ßu:</b> .grib, sau ƒë√≥ chuy·ªÉn ƒë·ªïi sang .csv ƒë·ªÉ x·ª≠ l√Ω</li>\n",
    "</ul>\n",
    "\n",
    "<h4>C√°c bi·∫øn s·ªë ch√≠nh trong t·∫≠p d·ªØ li·ªáu:</h4>\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>T√™n c·ªôt d·ªØ li·ªáu</th>\n",
    "<th>√ù nghƒ©a</th>\n",
    "<th>ƒê∆°n v·ªã ƒëo</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td><code>NAME</code></td><td>T√™n t·ªânh/th√†nh ph·ªë n∆°i thu th·∫≠p d·ªØ li·ªáu</td><td>-</td></tr>\n",
    "<tr><td><code>LATITUDE</code></td><td>Vƒ© ƒë·ªô ƒë·ªãa l√Ω c·ªßa ƒëi·ªÉm ƒëo</td><td>ƒê·ªô</td></tr>\n",
    "<tr><td><code>LONGITUDE</code></td><td>Kinh ƒë·ªô ƒë·ªãa l√Ω c·ªßa ƒëi·ªÉm ƒëo</td><td>ƒê·ªô</td></tr>\n",
    "<tr><td><code>YMD</code></td><td>Ng√†y/th√°ng/nƒÉm ƒëo ƒë·∫°c</td><td>dd/mm/yyyy</td></tr>\n",
    "<tr><td><code>YEAR</code></td><td>NƒÉm ƒëo ƒë·∫°c</td><td>NƒÉm</td></tr>\n",
    "<tr><td><code>MONTH</code></td><td>Th√°ng ƒëo ƒë·∫°c</td><td>Th√°ng</td></tr>\n",
    "<tr><td><code>DAY</code></td><td>Ng√†y ƒëo ƒë·∫°c</td><td>Ng√†y</td></tr>\n",
    "<tr><td><code>TEMP_max</code></td><td>Nhi·ªát ƒë·ªô kh√¥ng kh√≠ c·ª±c ƒë·∫°i trong ng√†y</td><td>¬∞C</td></tr>\n",
    "<tr><td><code>TEMP_ave</code></td><td>Nhi·ªát ƒë·ªô trung b√¨nh trong ng√†y</td><td>¬∞C</td></tr>\n",
    "<tr><td><code>DEW_ave</code></td><td>ƒêi·ªÉm s∆∞∆°ng trung b√¨nh trong ng√†y</td><td>¬∞C</td></tr>\n",
    "<tr><td><code>DEW_max</code></td><td>ƒêi·ªÉm s∆∞∆°ng cao nh·∫•t trong ng√†y</td><td>¬∞C</td></tr>\n",
    "<tr><td><code>RH_ave</code></td><td>ƒê·ªô ·∫©m t∆∞∆°ng ƒë·ªëi trung b√¨nh trong ng√†y</td><td>%</td></tr>\n",
    "<tr><td><code>RH_max</code></td><td>ƒê·ªô ·∫©m t∆∞∆°ng ƒë·ªëi c·ª±c ƒë·∫°i trong ng√†y</td><td>%</td></tr>\n",
    "<tr><td><code>AT_ave</code></td><td>Nhi·ªát ƒë·ªô c·∫£m nh·∫≠n trung b√¨nh trong ng√†y (Apparent Temp.)</td><td>¬∞C</td></tr>\n",
    "<tr><td><code>AT_max</code></td><td>Nhi·ªát ƒë·ªô c·∫£m nh·∫≠n cao nh·∫•t trong ng√†y</td><td>¬∞C</td></tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<p><b>Bi·∫øn m·ª•c ti√™u ch√≠nh:</b></p>\n",
    "<ul>\n",
    "<li><code>TEMP_max</code> ‚Äî Nhi·ªát ƒë·ªô kh√¥ng kh√≠ c·ª±c ƒë·∫°i h√†ng ng√†y (¬∞C)</li>\n",
    "</ul>\n",
    "\n",
    "<p><b>L∆∞u √Ω:</b> D·ªØ li·ªáu g·ªëc c·ªßa ERA5 c√≥ th·ªÉ ch·ª©a gi√° tr·ªã thi·∫øu, gi√° tr·ªã ngo·∫°i lai v√† m·ªôt s·ªë d·ªã b·∫£n kh√≠ t∆∞·ª£ng ƒë·∫∑c th√π. Do ƒë√≥, qu√° tr√¨nh l√†m s·∫°ch d·ªØ li·ªáu, x·ª≠ l√Ω gi√° tr·ªã thi·∫øu, ph√°t hi·ªán ngo·∫°i l·ªá v√† chu·∫©n h√≥a d·ªØ li·ªáu l√† c√°c b∆∞·ªõc b·∫Øt bu·ªôc tr∆∞·ªõc khi ti·∫øn h√†nh hu·∫•n luy·ªán v√† d·ª± b√°o.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_type = {\n",
    "    'NAME'       : 'Categorical',        # T√™n t·ªânh/th√†nh ph·ªë (chu·ªói)\n",
    "    'LATITUDE'   : 'Numerical',          # Vƒ© ƒë·ªô (¬∞)\n",
    "    'LONGITUDE'  : 'Numerical',          # Kinh ƒë·ªô (¬∞)\n",
    "    'YMD'        : 'Datetime',           # Ng√†y/th√°ng/nƒÉm (dd/mm/yyyy)\n",
    "    'YEAR'       : 'Numerical',          # NƒÉm (nƒÉm)\n",
    "    'MONTH'      : 'Numerical',          # Th√°ng (1-12)\n",
    "    'DAY'        : 'Numerical',          # Ng√†y (1-31)\n",
    "\n",
    "    'TEMP_max'   : 'Numerical',          # Nhi·ªát ƒë·ªô c·ª±c ƒë·∫°i trong ng√†y (¬∞C)\n",
    "    'TEMP_ave'   : 'Numerical',          # Nhi·ªát ƒë·ªô trung b√¨nh trong ng√†y (¬∞C)\n",
    "    'DEW_ave'    : 'Numerical',          # ƒêi·ªÉm s∆∞∆°ng trung b√¨nh trong ng√†y (¬∞C)\n",
    "    'DEW_max'    : 'Numerical',          # ƒêi·ªÉm s∆∞∆°ng c·ª±c ƒë·∫°i trong ng√†y (¬∞C)\n",
    "    'RH_ave'     : 'Numerical',          # ƒê·ªô ·∫©m t∆∞∆°ng ƒë·ªëi trung b√¨nh trong ng√†y (%)\n",
    "    'RH_max'     : 'Numerical',          # ƒê·ªô ·∫©m t∆∞∆°ng ƒë·ªëi c·ª±c ƒë·∫°i trong ng√†y (%)\n",
    "    'AT_ave'     : 'Numerical',          # Nhi·ªát ƒë·ªô c·∫£m nh·∫≠n trung b√¨nh trong ng√†y (¬∞C)\n",
    "    'AT_max'     : 'Numerical'           # Nhi·ªát ƒë·ªô c·∫£m nh·∫≠n c·ª±c ƒë·∫°i trong ng√†y (¬∞C)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">1.3. Goals</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../image/·∫¢nh ch·ª•p m√†n h√¨nh 2025-06-24 210616.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">2. Import Libraries</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">2.1. Configuration and display settings</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")  # ƒë∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a src\n",
    "\n",
    "from src.utilities import(config, \n",
    "                          dataset, \n",
    "                          features, \n",
    "                          plots)\n",
    "\n",
    "from src.models import(anomaly_models,\n",
    "                       forecasting_models,\n",
    "                       model_utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">2.2. Required Python packages</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import metpy\n",
    "import metpy.calc\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from metpy.units import units\n",
    "from copy        import deepcopy\n",
    "\n",
    "from sklearn.ensemble  import IsolationForest\n",
    "from prophet           import Prophet\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.cluster   import AgglomerativeClustering\n",
    "from sklearn.cluster   import DBSCAN\n",
    "from hdbscan           import HDBSCAN\n",
    "from joblib            import Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">3.Data Collecting</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">3.1. Chuy·ªÉn c√°c file .grib sang file .csv ho·∫∑c .xlsx\t, ho·∫∑c .xls (file Excel)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_CaMau = dict({\"CaMau_SurPres\"     : list([\"../data/raw/Ca Mau/.grib/CM_SurPres_1990_2002.grib\",\n",
    "                                              \"../data/raw/Ca Mau/.grib/CM_SurPres_2003_2015.grib\",\n",
    "                                              \"../data/raw/Ca Mau/.grib/CM_SurPres_2016_2024.grib\"]),\n",
    "                  \"CaMau_TotalCloud\"  : list([\"../data/raw/Ca Mau/.grib/CM_TotalCloud_1990_2002.grib\",\n",
    "                                              \"../data/raw/Ca Mau/.grib/CM_TotalCloud_2003_2015.grib\",\n",
    "                                              \"../data/raw/Ca Mau/.grib/CM_TotalCloud_2016_2024.grib\"]),\n",
    "                  \"CaMau_TotalPre\"    : list([\"../data/raw/Ca Mau/.grib/CM_TotalPre_1990_2002.grib\",\n",
    "                                              \"../data/raw/Ca Mau/.grib/CM_TotalPre_2003_2015.grib\",\n",
    "                                              \"../data/raw/Ca Mau/.grib/CM_TotalPre_2016_2024.grib\"]),\n",
    "                  \"CaMau_u10\"         : list([\"../data/raw/Ca Mau/.grib/CM_u10_1990_2002.grib\",\n",
    "                                              \"../data/raw/Ca Mau/.grib/CM_u10_2003_2015.grib\",\n",
    "                                              \"../data/raw/Ca Mau/.grib/CM_u10_2016_2024.grib\"]),\n",
    "                  \"CaMau_v10\"         : list([\"../data/raw/Ca Mau/.grib/CM_v10_1990_2002.grib\",\n",
    "                                              \"../data/raw/Ca Mau/.grib/CM_v10_2003_2015.grib\",\n",
    "                                              \"../data/raw/Ca Mau/.grib/CM_v10_2016_2024.grib\"])})\n",
    "\n",
    "\n",
    "src_DongHoi = dict({\"DongHoi_SurPres\"     : list([\"../data/raw/Dong Hoi/.grib/DH_SurfacePre_1990_2002.grib\",\n",
    "                                                  \"../data/raw/Dong Hoi/.grib/DH_SurfacePre_2003_2015.grib\",\n",
    "                                                  \"../data/raw/Dong Hoi/.grib/DH_SurfacePre_2016_2024.grib\"]),\n",
    "                    \"DongHoi_TotalCloud\"  : list([\"../data/raw/Dong Hoi/.grib/DH_TotalCloud_1990_2002.grib\",\n",
    "                                                  \"../data/raw/Dong Hoi/.grib/DH_TotalCloud_2003_2015.grib\",\n",
    "                                                  \"../data/raw/Dong Hoi/.grib/DH_TotalCloud_2016_2024.grib\"]),\n",
    "                    \"DongHoi_TotalPre\"    : list([\"../data/raw/Dong Hoi/.grib/DH_TotalPre_1990_2002.grib\",\n",
    "                                                  \"../data/raw/Dong Hoi/.grib/DH_TotalPre_2003_2015.grib\",\n",
    "                                                  \"../data/raw/Dong Hoi/.grib/DH_TotalPre_2016_2024.grib\"]),\n",
    "                    \"DongHoi_u10\"         : list([\"../data/raw/Dong Hoi/.grib/DH_u10_1990_2002.grib\",\n",
    "                                                  \"../data/raw/Dong Hoi/.grib/DH_u10_2003_2015.grib\",\n",
    "                                                  \"../data/raw/Dong Hoi/.grib/DH_u10_2016_2024.grib\"]),\n",
    "                    \"DongHoi_v10\"         : list([\"../data/raw/Dong Hoi/.grib/DH_v10_1990_2002.grib\",\n",
    "                                                  \"../data/raw/Dong Hoi/.grib/DH_v10_2003_2015.grib\",\n",
    "                                                  \"../data/raw/Dong Hoi/.grib/DH_v10_2016_2024.grib\"])})\n",
    "\n",
    "\n",
    "src_NoiBai = dict({\"NB_SurPres\"       : list([\"../data/raw/Noi Bai/.grib/NB_SurPres_1990_2002.grib\",\n",
    "                                              \"../data/raw/Noi Bai/.grib/NB_SurPres_2003_2015.grib\",\n",
    "                                              \"../data/raw/Noi Bai/.grib/NB_SurPres_2016_2024.grib\"]),\n",
    "                   \"NB_TotalCloud\"    : list([\"../data/raw/Noi Bai/.grib/NB_TotalCloud_1990_2002.grib\",\n",
    "                                              \"../data/raw/Noi Bai/.grib/NB_TotalCloud_2003_2015.grib\",\n",
    "                                              \"../data/raw/Noi Bai/.grib/NB_TotalCloud_2016_2024.grib\"]),\n",
    "                   \"NB_TotalPre\"      : list([\"../data/raw/Noi Bai/.grib/NB_TotalPre_1990_2002.grib\",\n",
    "                                              \"../data/raw/Noi Bai/.grib/NB_TotalPre_2003_2015.grib\",\n",
    "                                              \"../data/raw/Noi Bai/.grib/NB_TotalPre_2016_2024.grib\"]),\n",
    "                   \"NB_u10\"           : list([\"../data/raw/Noi Bai/.grib/NB_u10_1990_2002.grib\",\n",
    "                                              \"../data/raw/Noi Bai/.grib/NB_u10_2003_2015.grib\",\n",
    "                                              \"../data/raw/Noi Bai/.grib/NB_u10_2016_2024.grib\"]),\n",
    "                   \"NB_v10\"           : list([\"../data/raw/Noi Bai/.grib/NB_v10_1990_2002.grib\",\n",
    "                                              \"../data/raw/Noi Bai/.grib/NB_v10_2003_2015.grib\",\n",
    "                                              \"../data/raw/Noi Bai/.grib/NB_v10_2016_2024.grib\"])})\n",
    "\n",
    "\n",
    "src_QuyNhon = dict({\"QN_SurPres\"      : list([\"../data/raw/Quy Nhon/.grib/QN_SurPres_1990_2002.grib\",\n",
    "                                              \"../data/raw/Quy Nhon/.grib/QN_SurPres_2003_2015.grib\",\n",
    "                                              \"../data/raw/Quy Nhon/.grib/QN_SurPres_2016_2024.grib\"]),\n",
    "                    \"QN_TotalCloud\"   : list([\"../data/raw/Quy Nhon/.grib/QN_TotalCloud_1990_2002.grib\",\n",
    "                                              \"../data/raw/Quy Nhon/.grib/QN_TotalCloud_2003_2015.grib\",\n",
    "                                              \"../data/raw/Quy Nhon/.grib/QN_TotalCloud_2016_2024.grib\"]),\n",
    "                    \"QN_TotalPre\"     : list([\"../data/raw/Quy Nhon/.grib/QN_TotalPre_1990_2002.grib\",\n",
    "                                              \"../data/raw/Quy Nhon/.grib/QN_TotalPre_2003_2015.grib\",\n",
    "                                              \"../data/raw/Quy Nhon/.grib/QN_TotalPre_2016_2024.grib\"]),\n",
    "                    \"QN_u10\"          : list([\"../data/raw/Quy Nhon/.grib/QN_u10_1990_2002.grib\",\n",
    "                                              \"../data/raw/Quy Nhon/.grib/QN_u10_2003_2015.grib\",\n",
    "                                              \"../data/raw/Quy Nhon/.grib/QN_u10_2016_2024.grib\"]),\n",
    "                    \"QN_v10\"          : list([\"../data/raw/Quy Nhon/.grib/QN_v10_1990_2002.grib\",\n",
    "                                              \"../data/raw/Quy Nhon/.grib/QN_v10_2003_2015.grib\",\n",
    "                                              \"../data/raw/Quy Nhon/.grib/QN_v10_2016_2024.grib\"])})\n",
    "\n",
    "\n",
    "src_ThanhHoa = dict({\"TH_SurPres\"         : list([\"../data/raw/Thanh Hoa/.grib/TH_SurPres_1990_2002.grib\",\n",
    "                                                  \"../data/raw/Thanh Hoa/.grib/TH_SurPres_2003_2015.grib\",\n",
    "                                                  \"../data/raw/Thanh Hoa/.grib/TH_SurPres_2016_2024.grib\"]),\n",
    "                     \"TH_TotalCloud\"      : list([\"../data/raw/Thanh Hoa/.grib/TH_TotalCloud_1990_2002.grib\",\n",
    "                                                  \"../data/raw/Thanh Hoa/.grib/TH_TotalCloud_2003_2015.grib\",\n",
    "                                                  \"../data/raw/Thanh Hoa/.grib/TH_TotalCloud_2016_2024.grib\"]),\n",
    "                     \"TH_TotalPre\"        : list([\"../data/raw/Thanh Hoa/.grib/TH_TotalPre_1990_2002.grib\",\n",
    "                                                  \"../data/raw/Thanh Hoa/.grib/TH_TotalPre_2003_2015.grib\",\n",
    "                                                  \"../data/raw/Thanh Hoa/.grib/TH_TotalPre_2016_2024.grib\"]),\n",
    "                     \"TH_u10\"             : list([\"../data/raw/Thanh Hoa/.grib/TH_u10_1990_2002.grib\",\n",
    "                                                  \"../data/raw/Thanh Hoa/.grib/TH_u10_2003_2015.grib\",\n",
    "                                                  \"../data/raw/Thanh Hoa/.grib/TH_u10_2016_2024.grib\"]),\n",
    "                     \"TH_v10\"             : list([\"../data/raw/Thanh Hoa/.grib/TH_v10_1990_2002.grib\",\n",
    "                                                  \"../data/raw/Thanh Hoa/.grib/TH_v10_2003_2015.grib\",\n",
    "                                                  \"../data/raw/Thanh Hoa/.grib/TH_v10_2016_2024.grib\"])})\n",
    "\n",
    "\n",
    "src_TSN = dict({\"TSN_SurPres\"         : list([\"../data/raw/TSN/.grib/TSN_Press_1990_2002.grib\",\n",
    "                                              \"../data/raw/TSN/.grib/TSN_Press_2003_2015.grib\",\n",
    "                                              \"../data/raw/TSN/.grib/TSN_Press_2016_2024.grib\"]),\n",
    "                \"TSN_TotalCloud\"      : list([\"../data/raw/TSN/.grib/TSN_TotalCloud_1990_2002.grib\",\n",
    "                                              \"../data/raw/TSN/.grib/TSN_TotalCloud_2003_2015.grib\",\n",
    "                                              \"../data/raw/TSN/.grib/TSN_TotalCloud_2016_2024.grib\"]),\n",
    "                \"TSN_TotalPre\"        : list([\"../data/raw/TSN/.grib/TSN_TotalPre_1990_2002.grib\",\n",
    "                                              \"../data/raw/TSN/.grib/TSN_TotalPre_2003_2015.grib\",\n",
    "                                              \"../data/raw/TSN/.grib/TSN_TotalPre_2016_2024.grib\"]),\n",
    "                \"TSN_u10\"             : list([\"../data/raw/TSN/.grib/TSN_u10_1990_2002.grib\",\n",
    "                                              \"../data/raw/TSN/.grib/TSN_u10_2003_2015.grib\",\n",
    "                                              \"../data/raw/TSN/.grib/TSN_u10_2016-2023.grib\"]),\n",
    "                \"TSN_v10\"             : list([\"../data/raw/TSN/.grib/TSN_v10_1990_2002.grib\",\n",
    "                                              \"../data/raw/TSN/.grib/TSN_v10_2003_2015.grib\",\n",
    "                                              \"../data/raw/TSN/.grib/TSN_v10_2016_2023.grib\"])})\n",
    "\n",
    "src = src_CaMau | src_DongHoi | src_NoiBai | src_QuyNhon  | src_ThanhHoa | src_TSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name, file_list in src.items():\n",
    "    for file_path in file_list:\n",
    "        # L·∫•y th∆∞ m·ª•c ch·ª©a file .grib\n",
    "        grib_folder = os.path.dirname(file_path)\n",
    "\n",
    "        # Th∆∞ m·ª•c .csv c√πng c·∫•p v·ªõi .grib\n",
    "        csv_folder = os.path.join(os.path.dirname(grib_folder), '.csv')\n",
    "        os.makedirs(csv_folder, exist_ok=True)\n",
    "\n",
    "        # T√™n file .csv tr√πng v·ªõi t√™n file .grib nh∆∞ng ƒë·ªïi ƒëu√¥i\n",
    "        file_name = os.path.basename(file_path).replace(\".grib\", \".csv\")\n",
    "\n",
    "        # File ƒë√≠ch n·∫±m trong th∆∞ m·ª•c .csv\n",
    "        des_file = os.path.join(csv_folder, file_name)\n",
    "\n",
    "        # N·∫øu ch∆∞a t·ªìn t·∫°i m·ªõi convert\n",
    "        if not os.path.exists(des_file):\n",
    "            print(f\"~> Convert: {file_path} ‚Üí {des_file}\")\n",
    "            dataset.grib_to_csv(file_path, des_file)\n",
    "        else:\n",
    "            print(f\"‚úîÔ∏è T·ªìn t·∫°i: {des_file}, b·ªè qua.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">3.2. Ti·∫øp theo, ƒë·ªïi th·ªùi gian trong c·ªôt \"time\" (UTC+00) c·ªßa file v·ª´a m·ªõi chuy·ªÉn v·ªÅ gi·ªù Vi·ªát Nam (UTC+07)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name, file_list in src.items():\n",
    "    print(f\"\\nüî∏ ƒêang x·ª≠ l√Ω bi·∫øn: {var_name}\")\n",
    "\n",
    "    # L·∫•y th∆∞ m·ª•c ch·ª©a file csv g·ªëc (t·ª´ file ƒë·∫ßu ti√™n th√¥i)\n",
    "    csv_path_example = file_list[0].replace(\".grib\", \".csv\")\n",
    "    csv_folder = os.path.dirname(csv_path_example)\n",
    "    merged_folder = os.path.join(csv_folder, \"merged\")\n",
    "    os.makedirs(merged_folder, exist_ok=True)\n",
    "\n",
    "    # T√™n file ƒë√≠ch\n",
    "    out_path = os.path.join(merged_folder, f\"{var_name}_merged.csv\")\n",
    "\n",
    "    # N·∫øu file ƒë√≠ch ƒë√£ t·ªìn t·∫°i th√¨ b·ªè qua\n",
    "    if os.path.exists(out_path):\n",
    "        print(f\"‚ö†Ô∏è ƒê√£ t·ªìn t·∫°i: {out_path}, b·ªè qua.\")\n",
    "        continue\n",
    "\n",
    "    df_list = []  # List l∆∞u c√°c DataFrame ƒë·ªÉ concat\n",
    "\n",
    "    for file_path in file_list:\n",
    "        csv_path = file_path.replace(\".grib\", \".csv\")\n",
    "\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"‚ùå Kh√¥ng t√¨m th·∫•y file: {csv_path}, b·ªè qua.\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df = features.utc_to_vietnam(df, column_name=\"time\")\n",
    "        df_list.append(df)\n",
    "        print(f\"‚úîÔ∏è ƒê√£ x·ª≠ l√Ω: {csv_path}\")\n",
    "\n",
    "    # N·∫øu kh√¥ng c√≥ file n√†o th√¨ b·ªè qua merge\n",
    "    if not df_list:\n",
    "        print(f\"‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu cho {var_name}, b·ªè qua.\")\n",
    "        continue\n",
    "\n",
    "    # N·ªëi t·∫•t c·∫£ DataFrame l·∫°i v·ªõi nhau\n",
    "    df_merged = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # L∆∞u k·∫øt qu·∫£\n",
    "    df_merged.to_csv(out_path, index=False)\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">3.3. Gh√©p c√°c file kh√≠ t∆∞·ª£ng v√†o file nhi·ªát ƒë·ªô ƒë·ªÉ th√†nh data ƒë·∫ßy ƒë·ªß cho model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C√°c c√°ch lo·∫°i b·ªè d√≤ng tr√πng trong pandas\n",
    "\n",
    "<table border=\"1\" cellpadding=\"6\" cellspacing=\"0\">\n",
    "<thead>\n",
    "<tr>\n",
    "<th>C√°ch d√πng</th>\n",
    "<th>M√¥ t·∫£</th>\n",
    "<th>V√≠ d·ª•</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "\n",
    "<tr>\n",
    "<td><code>df.drop_duplicates()</code></td>\n",
    "<td>Lo·∫°i b·ªè to√†n b·ªô c√°c d√≤ng b·ªã tr√πng t·∫•t c·∫£ gi√° tr·ªã ·ªü t·∫•t c·∫£ c√°c c·ªôt.</td>\n",
    "<td><pre>df.drop_duplicates()</pre></td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td><code>df.drop_duplicates(subset=[\"col1\"])</code></td>\n",
    "<td>Ch·ªâ ki·ªÉm tra tr√πng l·∫∑p theo c·ªôt <code>col1</code>, gi·ªØ l·∫°i d√≤ng ƒë·∫ßu ti√™n.</td>\n",
    "<td><pre>df.drop_duplicates(subset=[\"time\"])</pre></td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td><code>df.drop_duplicates(keep=\"last\")</code></td>\n",
    "<td>Gi·ªØ l·∫°i d√≤ng cu·ªëi c√πng trong s·ªë c√°c d√≤ng b·ªã tr√πng.</td>\n",
    "<td><pre>df.drop_duplicates(subset=[\"time\"], keep=\"last\")</pre></td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td><code>df.drop_duplicates(ignore_index=True)</code></td>\n",
    "<td>X√≥a d√≤ng tr√πng, ƒë·ªìng th·ªùi reset l·∫°i ch·ªâ s·ªë (index).</td>\n",
    "<td><pre>df.drop_duplicates(ignore_index=True)</pre></td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td><code>df.loc[~df.index.duplicated() ]</code></td>\n",
    "<td>N·∫øu <code>time</code> ƒëang l√† index, lo·∫°i b·ªè c√°c index tr√πng, gi·ªØ l·∫°i d√≤ng ƒë·∫ßu ti√™n.</td>\n",
    "<td><pre>df.loc[~df.index.duplicated()]</pre></td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td><code>df.loc[~df.index.duplicated(keep=\"last\") ]</code></td>\n",
    "<td>Lo·∫°i b·ªè c√°c index tr√πng, gi·ªØ l·∫°i d√≤ng cu·ªëi c√πng.</td>\n",
    "<td><pre>df.loc[~df.index.duplicated(keep=\"last\")]</pre></td>\n",
    "</tr>\n",
    "\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_key = [\"time\", \"latitude\", \"longitude\", \"number\", \"step\", \"surface\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Ca Mau</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SurPres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CaMau_SurPres = pd.read_csv(\"../data/raw/Ca Mau/.csv/merged/CaMau_SurPres_merged.csv\")\n",
    "df_CaMau_SurPres = df_CaMau_SurPres.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols             = primary_key + [\"sp\"]\n",
    "df_CaMau_SurPres = df_CaMau_SurPres[cols]\n",
    "\n",
    "df_CaMau_SurPres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CaMau_TotalCloud = pd.read_csv(\"../data/raw/Ca Mau/.csv/merged/CaMau_TotalCloud_merged.csv\")\n",
    "df_CaMau_TotalCloud = df_CaMau_TotalCloud.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols                = primary_key + [\"tcc\"]\n",
    "df_CaMau_TotalCloud = df_CaMau_TotalCloud[cols]\n",
    "\n",
    "df_CaMau_TotalCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalPre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CaMau_TotalPre = pd.read_csv(\"../data/raw/Ca Mau/.csv/merged/CaMau_TotalPre_merged.csv\")\n",
    "df_CaMau_TotalPre = df_CaMau_TotalPre.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols              = primary_key + [\"tp\"]\n",
    "df_CaMau_TotalPre = df_CaMau_TotalPre[cols]\n",
    "\n",
    "\n",
    "df_CaMau_TotalPre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Chuy·ªÉn time = time + step\n",
    "    Chuy·ªÉn step = 0 days\n",
    "\"\"\"\n",
    "df_CaMau_TotalPre[\"time\"] = pd.to_datetime(df_CaMau_TotalPre[\"time\"]) + pd.to_timedelta(df_CaMau_TotalPre[\"step\"])\n",
    "df_CaMau_TotalPre[\"step\"] = \"0 days\"\n",
    "\n",
    "df_CaMau_TotalPre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "u10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CaMau_u10 = pd.read_csv(\"../data/raw/Ca Mau/.csv/merged/CaMau_u10_merged.csv\")\n",
    "df_CaMau_u10 = df_CaMau_u10.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols         = primary_key + [\"u10\"]\n",
    "df_CaMau_u10 = df_CaMau_u10[cols]\n",
    "\n",
    "df_CaMau_u10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CaMau_v10 = pd.read_csv(\"../data/raw/Ca Mau/.csv/merged/CaMau_v10_merged.csv\")\n",
    "df_CaMau_v10 = df_CaMau_v10.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols         = primary_key + [\"v10\"]\n",
    "df_CaMau_v10 = df_CaMau_v10[cols]\n",
    "\n",
    "df_CaMau_v10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = list([df_CaMau_SurPres, df_CaMau_TotalCloud, df_CaMau_TotalPre, df_CaMau_u10, df_CaMau_v10])\n",
    "\n",
    "for df in datasets:\n",
    "    df = features.utc_to_vietnam(df,\"time\")\n",
    "\n",
    "df_merged = dataset.merge_df(datasets = datasets,\n",
    "                             on       = primary_key, \n",
    "                             how      = \"outer\")\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compare_df(df_A = df_merged,\n",
    "                   df_B = df_CaMau_TotalPre,\n",
    "                   key  = primary_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(\"../data/raw/Ca Mau/.csv/merged/CaMau_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Dong Hoi</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SurPres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DH_SurPres = pd.read_csv(\"../data/raw//Dong Hoi/.csv/merged/DongHoi_SurPres_merged.csv\")\n",
    "df_DH_SurPres = df_DH_SurPres.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols             = primary_key + [\"sp\"]\n",
    "df_DH_SurPres = df_DH_SurPres[cols]\n",
    "\n",
    "df_DH_SurPres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DH_TotalCloud = pd.read_csv(\"../data/raw/Dong Hoi/.csv/merged/DongHoi_TotalCloud_merged.csv\")\n",
    "df_DH_TotalCloud = df_DH_TotalCloud.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols                = primary_key + [\"tcc\"]\n",
    "df_DH_TotalCloud = df_DH_TotalCloud[cols]\n",
    "\n",
    "df_DH_TotalCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalPre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DH_TotalPre = pd.read_csv(\"../data/raw/Dong Hoi/.csv/merged/DongHoi_TotalPre_merged.csv\")\n",
    "df_DH_TotalPre = df_DH_TotalPre.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols           = primary_key + [\"tp\"]\n",
    "df_DH_TotalPre = df_DH_TotalPre[cols]\n",
    "\n",
    "df_DH_TotalPre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Chuy·ªÉn time = time + step\n",
    "    Chuy·ªÉn step = 0 days\n",
    "\"\"\"\n",
    "df_DH_TotalPre[\"time\"] = pd.to_datetime(df_DH_TotalPre[\"time\"]) + pd.to_timedelta(df_DH_TotalPre[\"step\"])\n",
    "df_DH_TotalPre[\"step\"] = \"0 days\"\n",
    "\n",
    "df_DH_TotalPre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "u10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DH_u10 = pd.read_csv(\"../data/raw/Dong Hoi/.csv/merged/DongHoi_u10_merged.csv\")\n",
    "df_DH_u10 = df_DH_u10.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols         = primary_key + [\"u10\"]\n",
    "df_DH_u10 = df_DH_u10[cols]\n",
    "\n",
    "df_DH_u10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DH_v10 = pd.read_csv(\"../data/raw/Dong Hoi/.csv/merged/DongHoi_v10_merged.csv\")\n",
    "df_DH_v10 = df_DH_v10.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols         = primary_key + [\"v10\"]\n",
    "df_DH_v10 = df_DH_v10[cols]\n",
    "\n",
    "df_DH_v10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = list([df_DH_SurPres, df_DH_TotalCloud, df_DH_TotalPre, df_DH_u10, df_DH_v10])\n",
    "\n",
    "for df in datasets:\n",
    "    df = features.utc_to_vietnam(df,\"time\")\n",
    "\n",
    "df_merged = dataset.merge_df(datasets = datasets,\n",
    "                             on       = primary_key, \n",
    "                             how      = \"outer\")\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compare_df(df_A = df_merged,\n",
    "                   df_B = df_DH_TotalPre,\n",
    "                   key  = primary_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(\"../data/raw/Dong Hoi/.csv/merged/DH_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Noi Bai</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SurPres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NB_SurPres = pd.read_csv(\"../data/raw/Noi Bai/.csv/merged/NB_SurPres_merged.csv\")\n",
    "df_NB_SurPres = df_NB_SurPres.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols          = primary_key + [\"sp\"]\n",
    "df_NB_SurPres = df_NB_SurPres[cols]\n",
    "\n",
    "df_NB_SurPres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NB_TotalCloud = pd.read_csv(\"../data/raw/Noi Bai/.csv/merged/NB_TotalCloud_merged.csv\")\n",
    "df_NB_TotalCloud = df_NB_TotalCloud.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols             = primary_key + [\"tcc\"]\n",
    "df_NB_TotalCloud = df_NB_TotalCloud[cols]\n",
    "\n",
    "df_NB_TotalCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalPre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NB_TotalPre = pd.read_csv(\"../data/raw/Noi Bai/.csv/merged/NB_TotalPre_merged.csv\")\n",
    "# df_CaMau_TotalPre = df_CaMau_TotalPre.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols           = primary_key + [\"tp\"]\n",
    "df_NB_TotalPre = df_NB_TotalPre[cols]\n",
    "\n",
    "df_NB_TotalPre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Chuy·ªÉn time = time + step\n",
    "    Chuy·ªÉn step = 0 days\n",
    "\"\"\"\n",
    "df_NB_TotalPre[\"time\"] = pd.to_datetime(df_NB_TotalPre[\"time\"]) + pd.to_timedelta(df_NB_TotalPre[\"step\"])\n",
    "df_NB_TotalPre[\"step\"] = \"0 days\"\n",
    "\n",
    "df_NB_TotalPre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "u10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NB_u10 = pd.read_csv(\"../data/raw/Noi Bai/.csv/merged/NB_u10_merged.csv\")\n",
    "df_NB_u10 = df_NB_u10.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols      = primary_key + [\"u10\"]\n",
    "df_NB_u10 = df_NB_u10[cols]\n",
    "\n",
    "df_NB_u10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NB_v10 = pd.read_csv(\"../data/raw/Noi Bai/.csv/merged/NB_v10_merged.csv\")\n",
    "df_NB_v10 = df_NB_v10.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols      = primary_key + [\"v10\"]\n",
    "df_NB_v10 = df_NB_v10[cols]\n",
    "\n",
    "df_NB_v10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = list([df_NB_SurPres, df_NB_TotalCloud, df_NB_TotalPre, df_NB_u10, df_NB_v10])\n",
    "\n",
    "for df in datasets:\n",
    "    df = features.utc_to_vietnam(df,\"time\")\n",
    "\n",
    "df_merged = dataset.merge_df(datasets = datasets,\n",
    "                             on       = primary_key, \n",
    "                             how      = \"outer\")\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compare_df(df_A = df_merged,\n",
    "                   df_B = df_NB_TotalPre,\n",
    "                   key  = primary_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(\"../data/raw/Noi Bai/.csv/merged/NB_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Quy Nhon</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SurPres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_QN_SurPres = pd.read_csv(\"../data/raw/Quy Nhon/.csv/merged/QN_SurPres_merged.csv\")\n",
    "df_QN_SurPres = df_QN_SurPres.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols          = primary_key + [\"sp\"]\n",
    "df_QN_SurPres = df_QN_SurPres[cols]\n",
    "\n",
    "df_QN_SurPres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_QN_TotalCloud = pd.read_csv(\"../data/raw/Quy Nhon/.csv/merged/QN_TotalCloud_merged.csv\")\n",
    "df_QN_TotalCloud = df_QN_TotalCloud.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols             = primary_key + [\"tcc\"]\n",
    "df_QN_TotalCloud = df_QN_TotalCloud[cols]\n",
    "\n",
    "df_QN_TotalCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalPre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_QN_TotalPre = pd.read_csv(\"../data/raw/Quy Nhon/.csv/merged/QN_TotalPre_merged.csv\")\n",
    "df_QN_TotalPre = df_QN_TotalPre.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols           = primary_key + [\"tp\"]\n",
    "df_QN_TotalPre = df_QN_TotalPre[cols]\n",
    "\n",
    "df_QN_TotalPre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Chuy·ªÉn time = time + step\n",
    "    Chuy·ªÉn step = 0 days\n",
    "\"\"\"\n",
    "df_QN_TotalPre[\"time\"] = pd.to_datetime(df_QN_TotalPre[\"time\"]) + pd.to_timedelta(df_QN_TotalPre[\"step\"])\n",
    "df_QN_TotalPre[\"step\"] = \"0 days\"\n",
    "\n",
    "df_QN_TotalPre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "u10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_QN_u10 = pd.read_csv(\"../data/raw/Quy Nhon/.csv/merged/QN_u10_merged.csv\")\n",
    "df_QN_u10 = df_QN_u10.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols      = primary_key + [\"u10\"]\n",
    "df_QN_u10 = df_QN_u10[cols]\n",
    "\n",
    "df_QN_u10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_QN_v10 = pd.read_csv(\"../data/raw/Quy Nhon/.csv/merged/QN_v10_merged.csv\")\n",
    "df_QN_v10 = df_QN_v10.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols      = primary_key + [\"v10\"]\n",
    "df_QN_v10 = df_QN_v10[cols]\n",
    "\n",
    "df_QN_v10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = list([df_QN_SurPres, df_QN_TotalCloud, df_QN_TotalPre, df_QN_u10, df_QN_v10])\n",
    "\n",
    "for df in datasets:\n",
    "    df = features.utc_to_vietnam(df,\"time\")\n",
    "\n",
    "df_merged = dataset.merge_df(datasets = datasets,\n",
    "                             on       = primary_key, \n",
    "                             how      = \"outer\")\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compare_df(df_A = df_merged,\n",
    "                   df_B = df_QN_TotalPre,\n",
    "                   key  = primary_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(\"../data/raw/Quy Nhon/.csv/merged/QN_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Thanh Hoa</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SurPres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TH_SurPres = pd.read_csv(\"../data/raw/Thanh Hoa/.csv/merged/TH_SurPres_merged.csv\")\n",
    "df_TH_SurPres = df_TH_SurPres.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols             = primary_key + [\"sp\"]\n",
    "df_TH_SurPres = df_TH_SurPres[cols]\n",
    "\n",
    "df_TH_SurPres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TH_TotalCloud = pd.read_csv(\"../data/raw/Thanh Hoa/.csv/merged/TH_TotalCloud_merged.csv\")\n",
    "df_TH_TotalCloud = df_TH_TotalCloud.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols                = primary_key + [\"tcc\"]\n",
    "df_TH_TotalCloud = df_TH_TotalCloud[cols]\n",
    "\n",
    "df_TH_TotalCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalPre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TH_TotalPre = pd.read_csv(\"../data/raw/Thanh Hoa/.csv/merged/TH_TotalPre_merged.csv\")\n",
    "df_TH_TotalPre = df_TH_TotalPre.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols              = primary_key + [\"tp\"]\n",
    "df_TH_TotalPre = df_TH_TotalPre[cols]\n",
    "\n",
    "df_TH_TotalPre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Chuy·ªÉn time = time + step\n",
    "    Chuy·ªÉn step = 0 days\n",
    "\"\"\"\n",
    "df_TH_TotalPre[\"time\"] = pd.to_datetime(df_TH_TotalPre[\"time\"]) + pd.to_timedelta(df_TH_TotalPre[\"step\"])\n",
    "df_TH_TotalPre[\"step\"] = \"0 days\"\n",
    "\n",
    "df_TH_TotalPre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "u10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TH_u10 = pd.read_csv(\"../data/raw/Thanh Hoa/.csv/merged/TH_u10_merged.csv\")\n",
    "df_TH_u10 = df_TH_u10.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols         = primary_key + [\"u10\"]\n",
    "df_TH_u10 = df_TH_u10[cols]\n",
    "\n",
    "df_TH_u10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TH_v10 = pd.read_csv(\"../data/raw/Thanh Hoa/.csv/merged/TH_v10_merged.csv\")\n",
    "df_TH_v10 = df_TH_v10.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols         = primary_key + [\"v10\"]\n",
    "df_TH_v10 = df_TH_v10[cols]\n",
    "\n",
    "df_TH_v10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = list([df_TH_SurPres, df_TH_TotalCloud, df_TH_TotalPre, df_TH_u10, df_TH_v10])\n",
    "\n",
    "for df in datasets:\n",
    "    df = features.utc_to_vietnam(df,\"time\")\n",
    "\n",
    "df_merged = dataset.merge_df(datasets = datasets,\n",
    "                             on       = primary_key, \n",
    "                             how      = \"outer\")\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compare_df(df_A = df_merged,\n",
    "                   df_B = df_TH_TotalPre,\n",
    "                   key  = primary_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(\"../data/raw/Thanh Hoa/.csv/merged/TH_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">TSN</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SurPres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TSN_SurPres = pd.read_csv(\"../data/raw/TSN/.csv/merged/TSN_SurPres_merged.csv\")\n",
    "df_TSN_SurPres = df_TSN_SurPres.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols           = primary_key + [\"sp\"]\n",
    "df_TSN_SurPres = df_TSN_SurPres[cols]\n",
    "\n",
    "df_TSN_SurPres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TSN_TotalCloud = pd.read_csv(\"../data/raw/TSN/.csv/merged/TSN_TotalCloud_merged.csv\")\n",
    "df_TSN_TotalCloud = df_TSN_TotalCloud.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols              = primary_key + [\"tcc\"]\n",
    "df_TSN_TotalCloud = df_TSN_TotalCloud[cols]\n",
    "\n",
    "df_TSN_TotalCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalPre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TSN_TotalPre = pd.read_csv(\"../data/raw/TSN/.csv/merged/TSN_TotalPre_merged.csv\")\n",
    "df_TSN_TotalPre = df_TSN_TotalPre.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols            = primary_key + [\"tp\"]\n",
    "df_TSN_TotalPre = df_TSN_TotalPre[cols]\n",
    "\n",
    "df_TSN_TotalPre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Chuy·ªÉn time = time + step\n",
    "    Chuy·ªÉn step = 0 days\n",
    "\"\"\"\n",
    "df_TSN_TotalPre[\"time\"] = pd.to_datetime(df_TSN_TotalPre[\"time\"]) + pd.to_timedelta(df_TSN_TotalPre[\"step\"])\n",
    "df_TSN_TotalPre[\"step\"] = \"0 days\"\n",
    "\n",
    "df_TSN_TotalPre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "u10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TSN_u10 = pd.read_csv(\"../data/raw/TSN/.csv/merged/TSN_u10_merged.csv\")\n",
    "df_TSN_u10 = df_TSN_u10.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols       = primary_key + [\"u10\"]\n",
    "df_TSN_u10 = df_TSN_u10[cols]\n",
    "\n",
    "df_TSN_u10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TSN_v10 = pd.read_csv(\"../data/raw/TSN/.csv/merged/TSN_v10_merged.csv\")\n",
    "df_TSN_v10 = df_TSN_v10.drop(\"valid_time\", axis=1).drop_duplicates()\n",
    "\n",
    "# S·∫Øp x·∫øp l·∫°i th·ª© t·ª± c·ªôt\n",
    "cols       = primary_key + [\"v10\"]\n",
    "df_TSN_v10 = df_TSN_v10[cols]\n",
    "\n",
    "df_TSN_v10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = list([df_TSN_SurPres, df_TSN_TotalCloud, df_TSN_TotalPre, df_TSN_u10, df_TSN_v10])\n",
    "\n",
    "for df in datasets:\n",
    "    df = features.utc_to_vietnam(df,\"time\")\n",
    "\n",
    "df_merged = dataset.merge_df(datasets = datasets,\n",
    "                             on       = primary_key, \n",
    "                             how      = \"outer\")\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compare_df(df_A = df_merged,\n",
    "                   df_B = df_TSN_TotalPre,\n",
    "                   key  = primary_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(\"../data/raw/TSN/.csv/merged/TSN_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">4.Data Cleaning</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C√°c bi·∫øn s·ªë ch√≠nh trong t·∫≠p d·ªØ li·ªáu:\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>T√™n c·ªôt d·ªØ li·ªáu</th>\n",
    "<th>√ù nghƒ©a</th>\n",
    "<th>ƒê∆°n v·ªã ƒëo</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td><code>sp</code></td><td>√Åp su·∫•t kh√≠ quy·ªÉn t·∫°i m·∫∑t ƒë·∫•t</td><td>Pa</td></tr>\n",
    "<tr><td><code>tcc</code></td><td>T·ªïng ƒë·ªô che ph·ªß m√¢y to√†n ph·∫ßn</td><td>%</td></tr>\n",
    "<tr><td><code>tp</code></td><td>L∆∞·ª£ng m∆∞a t√≠ch l≈©y</td><td>mm</td></tr>\n",
    "<tr><td><code>u10</code></td><td>Th√†nh ph·∫ßn v·∫≠n t·ªëc gi√≥ theo tr·ª•c ƒê√¥ng-T√¢y ·ªü ƒë·ªô cao 10m</td><td>m/s</td></tr>\n",
    "<tr><td><code>v10</code></td><td>Th√†nh ph·∫ßn v·∫≠n t·ªëc gi√≥ theo tr·ª•c B·∫Øc-Nam ·ªü ƒë·ªô cao 10m</td><td>m/s</td></tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [\"sp\", \"tcc\", \"tp\", \"u10\", \"v10\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">4.1. Handling duplicate values</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = list([\"../data/raw/Ca Mau/.csv/merged/CaMau_merged.csv\",\n",
    "            \"../data/raw/Dong Hoi/.csv/merged/DH_merged.csv\",\n",
    "            \"../data/raw/Noi Bai/.csv/merged/NB_merged.csv\",\n",
    "            \"../data/raw/Quy Nhon/.csv/merged/QN_merged.csv\",\n",
    "            \"../data/raw/Thanh Hoa/.csv/merged/TH_merged.csv\",\n",
    "            \"../data/raw/TSN/.csv/merged/TSN_merged.csv\"])\n",
    "\n",
    "station = dict({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, val in enumerate(src):\n",
    "    df = pd.read_csv(val, parse_dates=[\"time\"])\n",
    "    station_name = os.path.basename(val).replace(\"_merged.csv\", \"\")\n",
    "    station[station_name] = df  # l∆∞u v√†o dict theo t√™n tr·∫°m\n",
    "    print(f\"üî∏ Tr·∫°m: {station_name}\")\n",
    "\n",
    "    dataset.ProportionDuplicate_aproach1(df[\"time\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in station.items():\n",
    "    print(f\"üî∏ Tr·∫°m: {name}\")\n",
    "\n",
    "    station[name] = dataset.HandleDuplicate_drop(data   = df,\n",
    "                                                 subset = \"time\",\n",
    "                                                 keep   = \"first\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in station.items():\n",
    "    print(f\"üî∏ Tr·∫°m: {name}\")\n",
    "    \n",
    "    dataset.ProportionDuplicate_aproach1(df[\"time\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">4.2. Handling missing values</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Index</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://datastud.dev/posts/time-series-resample\">Filling Gaps in Time Series Data</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in station.items():\n",
    "    print(f\"üî∏ Tr·∫°m: {name}\")\n",
    "\n",
    "    gap_df = dataset.find_time_gaps(data       = df.set_index(\"time\"), \n",
    "                                    start_time = \"1990-01-01 00:00:00+07:00\", \n",
    "                                    end_time   = \"2025-01-01 00:00:00+07:00\",\n",
    "                                    freq       = \"1h\")\n",
    "    print(gap_df)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in station.items():\n",
    "    print(f\"üî∏ Tr·∫°m: {name}\")\n",
    "    \n",
    "    station[name] = dataset.fill_time_gaps(data       = df.set_index(\"time\"), \n",
    "                                           start_time = \"1990-01-01 00:00:00+07:00\", \n",
    "                                           end_time   = \"2025-01-01 00:00:00+07:00\",\n",
    "                                           freq       = \"1h\")\n",
    "    print(f\"‚úÖ ƒê√£ xong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in station.items():\n",
    "    print(f\"üî∏ Tr·∫°m: {name}\")\n",
    "\n",
    "    gap_df = dataset.find_time_gaps(data       = df.set_index(\"time\"), \n",
    "                                    start_time = \"1990-01-01 00:00:00+07:00\", \n",
    "                                    end_time   = \"2025-01-01 00:00:00+07:00\",\n",
    "                                    freq       = \"1h\")\n",
    "    print(gap_df)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Features</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìä So s√°nh c√°c ph∆∞∆°ng ph√°p x·ª≠ l√Ω gi√° tr·ªã thi·∫øu\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Ph∆∞∆°ng ph√°p</th>\n",
    "<th>∆Øu ƒëi·ªÉm</th>\n",
    "<th>Nh∆∞·ª£c ƒëi·ªÉm</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td><code>dropna()</code></td>\n",
    "<td>Nhanh g·ªçn, tri·ªát ƒë·ªÉ</td>\n",
    "<td>M·∫•t d·ªØ li·ªáu</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><code>fillna(mean/median)</code></td>\n",
    "<td>D·ªÖ th·ª±c hi·ªán, h·ª£p v·ªõi d·ªØ li·ªáu s·ªë</td>\n",
    "<td>C√≥ th·ªÉ l√†m gi·∫£m ph∆∞∆°ng sai</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><code>ffill()</code> / <code>bfill()</code></td>\n",
    "<td>D√πng t·ªët v·ªõi time series</td>\n",
    "<td>C√≥ th·ªÉ t·∫°o bias</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><code>interpolate()</code></td>\n",
    "<td>T·ª± nhi√™n, gi·ªØ xu h∆∞·ªõng</td>\n",
    "<td>C·∫ßn d·ªØ li·ªáu li√™n t·ª•c</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><code>groupby fill</code></td>\n",
    "<td>Ph√¢n nh√≥m th√¥ng minh</td>\n",
    "<td>Ch·∫≠m v·ªõi d·ªØ li·ªáu l·ªõn</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "‚úÖ K·∫øt lu·∫≠n:\n",
    "<ul>\n",
    "<li><b>Time series</b> ‚Üí <code>interpolate()</code>, <code>ffill()</code></li>\n",
    "<li><b>D·ªØ li·ªáu nh√≥m</b> ‚Üí <code>groupby fill</code></li>\n",
    "<li><b>D·ªØ li·ªáu c√≥ √≠t NaN</b> ‚Üí <code>dropna()</code></li>\n",
    "<li><b>D·ªØ li·ªáu s·ªë nhi·ªÅu NaN</b> ‚Üí <code>fillna(mean/median)</code> ho·∫∑c <code>interpolate()</code></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Th·ª© t·ª± | Ph∆∞∆°ng ph√°p                                              | √Åp d·ª•ng khi                                                                   | ∆Øu ti√™n                                                                 |\n",
    "| :----- | :------------------------------------------------------- | :---------------------------------------------------------------------------- | :---------------------------------------------------------------------- |\n",
    "| **1**  | `interpolate(method=\"time\")`                             | D·ªØ li·ªáu li√™n t·ª•c, time series, c√°c bi·∫øn s·ªë kh√≠ t∆∞·ª£ng bi·∫øn ƒë·ªông theo th·ªùi gian | ‚úÖ ∆Øu ti√™n ƒë·∫ßu                                                           |\n",
    "| **2**  | `ffill()` / `bfill()`                                    | N·∫øu `interpolate` kh√¥ng fill h·∫øt, d√πng gi√° tr·ªã tr∆∞·ªõc/sau g·∫ßn nh·∫•t             | ‚ö†Ô∏è Ch·ªâ n√™n d√πng khi d·ªØ li·ªáu kh√¥ng c√≥ s·ª± bi·∫øn ƒë·ªông l·ªõn ho·∫∑c missing ng·∫Øn |\n",
    "| **3**  | `groupby fill` (theo ng√†y, th√°ng, nƒÉm ho·∫∑c theo station) | N·∫øu d·ªØ li·ªáu ph√¢n nh√≥m v√† m·ªói nh√≥m ƒë·ªß d√†y                                      | T·ªët cho d·ªØ li·ªáu d·∫°ng panel, ho·∫∑c kh√≠ t∆∞·ª£ng t·ª´ng tr·∫°m                    |\n",
    "| **4**  | `fillna(mean/median)`                                    | N·∫øu s·ªë l∆∞·ª£ng missing c√≤n nh·ªè l·∫ª, ho·∫∑c sau khi c√°c c√°ch tr√™n v·∫´n c√≤n missing   | D√πng ƒë∆∞·ª£c v·ªõi numerical kh√¥ng qu√° nh·∫°y c·∫£m                              |\n",
    "| **5**  | `dropna()`                                               | Khi s·ªë l∆∞·ª£ng missing √≠t, ho·∫∑c kh√¥ng th·ªÉ n·ªôi suy m√† gi·ªØ l·∫°i s·∫Ω g√¢y sai l·ªách    | Cu·ªëi c√πng                                                               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in station.items():\n",
    "    print(f\"üî∏ Tr·∫°m: {name}\")\n",
    "\n",
    "    dataset.ProportionMissing_aproach1(df[numerical_features])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in station.items():\n",
    "    print(f\"üî∏ Tr·∫°m: {name}\")\n",
    "    \n",
    "    # Set DatetimeIndex\n",
    "    station[name] = df.set_index(\"time\")\n",
    "\n",
    "    # N·ªôi suy c√°c bi·∫øn numerical\n",
    "    df_filled = dataset.HandleMissing_interpolate(data   = station[name], \n",
    "                                                  method = \"time\")\n",
    "\n",
    "    # G√°n l·∫°i v√†o DataFrame g·ªëc \n",
    "    station[name] = df_filled.reset_index()\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in station.items():\n",
    "    print(f\"üî∏ Tr·∫°m: {name}\")\n",
    "\n",
    "    dataset.ProportionMissing_aproach1(df[numerical_features])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in station.items():\n",
    "    print(f\"üî∏ Tr·∫°m: {name}\")\n",
    "    \n",
    "    # Set DatetimeIndex\n",
    "    station[name] = df.set_index(\"time\")\n",
    "\n",
    "    # N·ªôi suy c√°c bi·∫øn numerical\n",
    "    df_filled = dataset.HandleMissing_fillna(data   = station[name], \n",
    "                                             method = \"median\")\n",
    "\n",
    "    # G√°n l·∫°i v√†o DataFrame g·ªëc \n",
    "    station[name] = df_filled.reset_index()\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in station.items():\n",
    "    print(f\"üî∏ Tr·∫°m: {name}\")\n",
    "\n",
    "    dataset.ProportionMissing_aproach1(df[numerical_features])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"CaMau\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">4.3. Handling mismatch values</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">4.4. Handling abnormal <~> outlier values <u>(basic)</u></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìñ T·ªïng h·ª£p c√°ch x·ª≠ l√Ω outlier cho Time Series\n",
    "| Ph∆∞∆°ng ph√°p                                  | M√¥ t·∫£                                                                             | Khi n√†o d√πng                                  | ∆Øu ƒëi·ªÉm        | Nh∆∞·ª£c ƒëi·ªÉm                           |\n",
    "| :------------------------------------------- | :-------------------------------------------------------------------------------- | :-------------------------------------------- | :------------- | :----------------------------------- |\n",
    "| **Remove**                                   | Lo·∫°i c√°c ƒëi·ªÉm d·ªØ li·ªáu ngo√†i ng∆∞·ª°ng                                                | N·∫øu r√µ r√†ng l√† l·ªói ƒëo ƒë·∫°c                     | D·ªÖ l√†m         | M·∫•t d·ªØ li·ªáu, ƒë·ª©t chu·ªói               |\n",
    "| **Interpolation**                            | N·ªôi suy gi√° tr·ªã t·∫°i v·ªã tr√≠ outlier b·∫±ng trung b√¨nh gi·ªØa tr∆∞·ªõc v√† sau              | Khi outlier l√† l·ªói c·ª•c b·ªô                     | Gi·ªØ m∆∞·ª£t chu·ªói | Kh√¥ng ƒë√∫ng n·∫øu outlier l√† event th·ª±c |\n",
    "| **LOCF (Last Observation Carried Forward)**  | L·∫•y gi√° tr·ªã tr∆∞·ªõc ƒë√≥ thay th·∫ø                                                     | D·ªØ li·ªáu nh·∫°y bi·∫øn ƒë·ªông th·∫•p (v√≠ d·ª•: nhi·ªát ƒë·ªô) | D·ªÖ c√†i         | N·∫øu outlier ƒë√∫ng l√† event, b·ªã sai    |\n",
    "| **NOCB (Next Observation Carried Backward)** | L·∫•y gi√° tr·ªã ti·∫øp theo thay th·∫ø                                                    | T∆∞∆°ng t·ª± LOCF                                 | D·ªÖ l√†m         | Sai v·ªõi bi·∫øn thi√™n nhanh             |\n",
    "| **Smoothing (Rolling Mean / EWMA)**          | L√†m m∆∞·ª£t chu·ªói, gi·∫£m ·∫£nh h∆∞·ªüng outlier b·∫±ng trung b√¨nh tr∆∞·ª£t                      | N·∫øu c·∫ßn d·ª± b√°o d√†i h·∫°n                        | Gi·∫£m nhi·ªÖu     | M·∫•t t√≠n hi·ªáu ng·∫Øn h·∫°n                |\n",
    "| **Model-based Detection**                    | D√πng Isolation Forest, Prophet changepoint detection‚Ä¶ ph√°t hi·ªán outlier r·ªìi x·ª≠ l√Ω | Khi data nhi·ªÅu outlier ho·∫∑c nhi·ªÅu m√πa v·ª•      | Nh·∫≠n di·ªán t·ªët  | T·ªën th·ªùi gian                        |\n",
    "| **Capping theo percentile/IQR t·ª´ng ƒëo·∫°n**    | Ch·ªâ capping gi√° tr·ªã t·∫°i v·ªã tr√≠ b·∫•t th∆∞·ªùng                                         | N·∫øu kh√¥ng mu·ªën interpolate                    | Gi·ªØ data       | M·∫•t th√¥ng tin c·ª±c tr·ªã                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìä H∆∞·ªõng x·ª≠ l√Ω theo lo·∫°i outlier trong time series\n",
    "| Lo·∫°i outlier                            | C√°ch x·ª≠ l√Ω t·ªët                        |\n",
    "| :-------------------------------------- | :------------------------------------ |\n",
    "| **L·ªói nh·∫≠p li·ªáu, spike d·ªã th∆∞·ªùng**      | Remove / LOCF / Interpolate           |\n",
    "| **S·ªë li·ªáu s·ª± ki·ªán th·ª±c (m∆∞a l·ªõn, b√£o)** | Gi·ªØ nguy√™n                            |\n",
    "| **Nhi·ªÖu nh·∫π / b·∫•t th∆∞·ªùng nh·ªè**          | Rolling Mean / EWMA smoothing         |\n",
    "| **ƒê·ª©t ƒëo·∫°n gi√° tr·ªã**                    | Interpolation (linear/spline)         |\n",
    "| **Spike v√†o d·ªãp l·ªÖ, s·ª± ki·ªán (nh∆∞ t·∫øt)** | C√≥ th·ªÉ t·∫°o bi·∫øn indicator thay v√¨ x√≥a |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <a href=\"https://towardsdatascience.com/the-ultimate-guide-to-finding-outliers-in-your-time-series-data-part-1-1bf81e09ade4-2/\">The Ultimate Guide to Finding Outliers in Your Time-Series Data (Part 1)</a> <--> Ph∆∞∆°ng ph√°p x√°c ƒë·ªãnh (visual/statistical)\n",
    "- <a href=\"https://medium.com/data-science/the-ultimate-guide-to-finding-outliers-in-your-time-series-data-part-2-674c25837f29\">The Ultimate Guide to Finding Outliers in Your Time-Series Data (Part 2)</a> <--> S·ª≠ d·ª•ng machine learning\n",
    "- <a href=\"https://towardsdatascience.com/the-ultimate-guide-to-finding-outliers-in-your-time-series-data-part-3-0ff73ce28ca3-2/\">The Ultimate Guide to Finding Outliers in Your Time-Series Data (Part 3)</a> <--> ‚ÄúOutliers Found: Now What?‚Äù ‚Äì qu·∫£n l√Ω outliers\n",
    "- <a href=\"https://towardsdatascience.com/evaluating-the-impact-of-outlier-treatment-in-time-series-b4fac4cabe94/\">The Ultimate Guide to Finding Outliers in Your Time-Series Data (Part 4)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìñ C√°c gi√° tr·ªã freq th∆∞·ªùng d√πng trong resample() Pandas:\n",
    "| T·∫ßn su·∫•t       | `freq` string        | √ù nghƒ©a                                   |\n",
    "| :------------- | :------------------- | :---------------------------------------- |\n",
    "| **H√†ng gi√¢y**  | `\"S\"`                | M·ªói gi√¢y                                  |\n",
    "| **H√†ng ph√∫t**  | `\"T\"` (ho·∫∑c `\"min\"`) | M·ªói ph√∫t                                  |\n",
    "| **H√†ng gi·ªù**   | `\"H\"`                | M·ªói gi·ªù                                   |\n",
    "| **H√†ng ng√†y**  | `\"D\"`                | M·ªói ng√†y                                  |\n",
    "| **H√†ng tu·∫ßn**  | `\"W\"`                | M·ªói tu·∫ßn (m·∫∑c ƒë·ªãnh k·∫øt th√∫c v√†o Ch·ªß nh·∫≠t) |\n",
    "| **H√†ng th√°ng** | `\"M\"`                | Cu·ªëi m·ªói th√°ng                            |\n",
    "| **ƒê·∫ßu th√°ng**  | `\"MS\"`               | ƒê·∫ßu m·ªói th√°ng                             |\n",
    "| **H√†ng qu√Ω**   | `\"Q\"`                | Cu·ªëi m·ªói qu√Ω (3, 6, 9, 12)                |\n",
    "| **ƒê·∫ßu qu√Ω**    | `\"QS\"`               | ƒê·∫ßu m·ªói qu√Ω                               |\n",
    "| **H√†ng nƒÉm**   | `\"A\"` ho·∫∑c `\"Y\"`     | Cu·ªëi m·ªói nƒÉm (m·∫∑c ƒë·ªãnh 31-12)             |\n",
    "| **ƒê·∫ßu nƒÉm**    | `\"AS\"` ho·∫∑c `\"YS\"`   | ƒê·∫ßu m·ªói nƒÉm (1-1)                         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"CaMau\"].loc[:, [\"time\", \"v10\"]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = station[\"CaMau\"].loc[:, [\"time\", \"v10\"]].set_index(\"time\").resample(\"1H\").mean().interpolate()\n",
    "new_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Ca Mau</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_feature_trends_over_time(data         = station[\"CaMau\"],\n",
    "                                    data_cols    = numerical_features,\n",
    "                                    station_name = \"CaMau\",\n",
    "                                    start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                    end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                    freq         = \"1Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_feature_outliers_over_time(data         = station[\"CaMau\"],\n",
    "                                      data_cols    = numerical_features,\n",
    "                                      station_name = \"CaMau\",\n",
    "                                      method       = \"statistic\",\n",
    "                                      display      = False,\n",
    "                                      start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                      end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                      freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"CaMau\"] = dataset.handle_feature_outliers_over_time(data         = station[\"CaMau\"],\n",
    "                                                             data_cols    = numerical_features,\n",
    "                                                             station_name = \"CaMau\",\n",
    "                                                             method       = \"statistic\",\n",
    "                                                             display      = True,\n",
    "                                                             start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                                             end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                                             freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Dong Hoi</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_feature_trends_over_time(data         = station[\"DH\"],\n",
    "                                    data_cols    = numerical_features,\n",
    "                                    station_name = \"DH\",\n",
    "                                    start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                    end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                    freq         = \"1Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_feature_outliers_over_time(data         = station[\"DH\"],\n",
    "                                      data_cols    = numerical_features,\n",
    "                                      station_name = \"DH\",\n",
    "                                      method       = \"statistic\",\n",
    "                                      display      = False,\n",
    "                                      start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                      end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                      freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"DH\"] = dataset.handle_feature_outliers_over_time(data         = station[\"DH\"],\n",
    "                                                          data_cols    = numerical_features,\n",
    "                                                          station_name = \"DH\",\n",
    "                                                          method       = \"statistic\",\n",
    "                                                          display      = True,\n",
    "                                                          start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                                          end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                                          freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Noi Bai</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_feature_trends_over_time(data         = station[\"NB\"],\n",
    "                                    data_cols    = numerical_features,\n",
    "                                    station_name = \"NB\",\n",
    "                                    start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                    end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                    freq         = \"1Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_feature_outliers_over_time(data         = station[\"NB\"],\n",
    "                                      data_cols    = numerical_features,\n",
    "                                      station_name = \"NB\",\n",
    "                                      method       = \"statistic\",\n",
    "                                      display      = False,\n",
    "                                      start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                      end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                      freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"NB\"] = dataset.handle_feature_outliers_over_time(data         = station[\"NB\"],\n",
    "                                                          data_cols    = numerical_features,\n",
    "                                                          station_name = \"NB\",\n",
    "                                                          method       = \"statistic\",\n",
    "                                                          display      = True,\n",
    "                                                          start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                                          end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                                          freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Quy Nhon</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_feature_trends_over_time(data         = station[\"QN\"],\n",
    "                                    data_cols    = numerical_features,\n",
    "                                    station_name = \"QN\",\n",
    "                                    start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                    end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                    freq         = \"1Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_feature_outliers_over_time(data         = station[\"QN\"],\n",
    "                                      data_cols    = numerical_features,\n",
    "                                      station_name = \"QN\",\n",
    "                                      method       = \"statistic\",\n",
    "                                      display      = False,\n",
    "                                      start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                      end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                      freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"QN\"] = dataset.handle_feature_outliers_over_time(data         = station[\"QN\"],\n",
    "                                                          data_cols    = numerical_features,\n",
    "                                                          station_name = \"QN\",\n",
    "                                                          method       = \"statistic\",\n",
    "                                                          display      = True,\n",
    "                                                          start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                                          end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                                          freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Thanh Hoa</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_feature_trends_over_time(data         = station[\"TH\"],\n",
    "                                    data_cols    = numerical_features,\n",
    "                                    station_name = \"TH\",\n",
    "                                    start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                    end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                    freq         = \"1Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_feature_outliers_over_time(data         = station[\"TH\"],\n",
    "                                      data_cols    = numerical_features,\n",
    "                                      station_name = \"TH\",\n",
    "                                      method       = \"statistic\",\n",
    "                                      display      = False,\n",
    "                                      start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                      end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                      freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"TH\"] = dataset.handle_feature_outliers_over_time(data         = station[\"TH\"],\n",
    "                                                          data_cols    = numerical_features,\n",
    "                                                          station_name = \"TH\",\n",
    "                                                          method       = \"statistic\",\n",
    "                                                          display      = True,\n",
    "                                                          start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                                          end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                                          freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">TSN</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_feature_trends_over_time(data         = station[\"TSN\"],\n",
    "                                    data_cols    = numerical_features,\n",
    "                                    station_name = \"TSN\",\n",
    "                                    start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                    end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                    freq         = \"1Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_feature_outliers_over_time(data         = station[\"TSN\"],\n",
    "                                      data_cols    = numerical_features,\n",
    "                                      station_name = \"TSN\",\n",
    "                                      method       = \"statistic\",\n",
    "                                      display      = False,\n",
    "                                      start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                      end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                      freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"TSN\"] = dataset.handle_feature_outliers_over_time(data         = station[\"TSN\"],\n",
    "                                                           data_cols    = numerical_features,\n",
    "                                                           station_name = \"TSN\",\n",
    "                                                           method       = \"statistic\",\n",
    "                                                           display      = True,\n",
    "                                                           start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                                           end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                                           freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">4.5. Save data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"TSN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥\n",
    "os.makedirs(\"../data/processed/datasets\", exist_ok=True)\n",
    "\n",
    "# Duy·ªát t·ª´ng station v√† l∆∞u file\n",
    "for name, df in station.items():\n",
    "    file_name = f\"../data/processed/datasets/{name}90.24_cleaned.csv\"\n",
    "    df.to_csv(file_name, index=False)\n",
    "    print(f\"ƒê√£ l∆∞u: {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">5. Data Preprocessing</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">5.1. Handling abnormal <~> outlier values <u>(intermediate)</u></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå 1Ô∏è‚É£ So s√°nh Univariate Time Series v√† Multivariate Time Series\n",
    "| Ti√™u ch√≠               | **Univariate Time Series**                      | **Multivariate Time Series**                                                 |\n",
    "| :--------------------- | :---------------------------------------------- | :--------------------------------------------------------------------------- |\n",
    "| **ƒê·ªãnh nghƒ©a**         | D·ª± b√°o 1 bi·∫øn duy nh·∫•t theo th·ªùi gian           | D·ª± b√°o 1 ho·∫∑c nhi·ªÅu bi·∫øn, c√≥ th·ªÉ ph·ª• thu·ªôc th√™m c√°c bi·∫øn kh√°c theo th·ªùi gian |\n",
    "| **ƒê·∫ßu v√†o (features)** | Ch·ªâ c√≥ gi√° tr·ªã c·ªßa ch√≠nh bi·∫øn ƒë√≥ theo th·ªùi gian | C√≥ th·ªÉ bao g·ªìm nhi·ªÅu bi·∫øn li√™n quan (covariates) c√πng th·ªùi gian              |\n",
    "| **ƒê·ªô ph·ª©c t·∫°p**        | Th·∫•p h∆°n, d·ªÖ tri·ªÉn khai                         | Cao h∆°n do t√≠nh ƒë·∫øn m·ªëi quan h·ªá gi·ªØa c√°c bi·∫øn                                |\n",
    "| **V√≠ d·ª•**              | D·ª± b√°o nhi·ªát ƒë·ªô t·ª´ng ng√†y t·∫°i 1 th√†nh ph·ªë       | D·ª± b√°o nhi·ªát ƒë·ªô t·ª´ng ng√†y + ƒë·ªô ·∫©m, t·ªëc ƒë·ªô gi√≥, kinh ƒë·ªô/vƒ© ƒë·ªô, ng√†y l·ªÖ‚Ä¶       |\n",
    "| **K·ªπ thu·∫≠t √°p d·ª•ng**   | ARIMA, Prophet, LSTM, N-BEATS, Moving Average‚Ä¶  | VAR, LSTM v·ªõi nhi·ªÅu input, TFT, Multivariate N-BEATS‚Ä¶                        |\n",
    "| **∆Øu ƒëi·ªÉm**            | D·ªØ li·ªáu v√† m√¥ h√¨nh ƒë∆°n gi·∫£n, nhanh ch√≥ng        | C√≥ th·ªÉ t·∫≠n d·ª•ng m·ªëi quan h·ªá gi·ªØa c√°c bi·∫øn ƒë·ªÉ d·ª± b√°o ch√≠nh x√°c h∆°n            |\n",
    "| **Nh∆∞·ª£c ƒëi·ªÉm**         | Kh√¥ng t·∫≠n d·ª•ng ƒë∆∞·ª£c c√°c y·∫øu t·ªë ·∫£nh h∆∞·ªüng kh√°c   | C·∫ßn nhi·ªÅu d·ªØ li·ªáu h∆°n, x·ª≠ l√Ω ph·ª©c t·∫°p, d·ªÖ g·∫∑p v·∫•n ƒë·ªÅ v·ªõi outlier             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå 2Ô∏è‚É£ C√°ch x·ª≠ l√Ω outlier trong multivariate time series\n",
    "| Ph∆∞∆°ng ph√°p                                     | C√°ch th·ª±c hi·ªán                                                                    | ∆Øu ƒëi·ªÉm                                 | Nh∆∞·ª£c ƒëi·ªÉm                                     |\n",
    "| :---------------------------------------------- | :-------------------------------------------------------------------------------- | :-------------------------------------- | :--------------------------------------------- |\n",
    "| **Z-score / IQR cho t·ª´ng bi·∫øn**                 | T√≠nh z-score ho·∫∑c IQR cho t·ª´ng feature, lo·∫°i b·ªè gi√° tr·ªã v∆∞·ª£t ng∆∞·ª°ng               | D·ªÖ tri·ªÉn khai, ƒë∆°n gi·∫£n                 | Ch·ªâ x·ª≠ l√Ω ƒë∆∞·ª£c t·ª´ng bi·∫øn, kh√¥ng x√©t t∆∞∆°ng quan |\n",
    "| **Multivariate Z-score (Mahalanobis distance)** | T√≠nh kho·∫£ng c√°ch Mahalanobis gi·ªØa m·ªói ƒëi·ªÉm v√† ph√¢n ph·ªëi t·ªïng th·ªÉ                  | X√©t ƒë∆∞·ª£c m·ªëi quan h·ªá gi·ªØa c√°c bi·∫øn      | C·∫ßn gi·∫£ ƒë·ªãnh ph√¢n ph·ªëi d·ªØ li·ªáu g·∫ßn chu·∫©n       |\n",
    "| **Isolation Forest / One-Class SVM**            | D√πng m√¥ h√¨nh unsupervised ƒë·ªÉ ph√°t hi·ªán ƒëi·ªÉm b·∫•t th∆∞·ªùng tr√™n to√†n b·ªô feature space | T·ª± ƒë·ªông, kh√¥ng c·∫ßn gi·∫£ ƒë·ªãnh ph√¢n ph·ªëi   | Ch·∫≠m v·ªõi d·ªØ li·ªáu l·ªõn, tuning kh√≥               |\n",
    "| **Autoencoder / LSTM-AE**                       | Train autoencoder, ƒëi·ªÉm n√†o reconstruction error l·ªõn th√¨ l√† outlier               | Ph√π h·ª£p time series, capture non-linear | C·∫ßn nhi·ªÅu d·ªØ li·ªáu, training l√¢u                |\n",
    "| **Visual inspection (scatter matrix, PCA)**     | Gi·∫£m s·ªë chi·ªÅu xu·ªëng 2D/3D ƒë·ªÉ tr·ª±c quan v√† ph√°t hi·ªán outlier                       | Nhanh, tr·ª±c quan                        | Kh√≥ √°p d·ª•ng khi d·ªØ li·ªáu qu√° l·ªõn                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå 3Ô∏è‚É£ G·ª£i √Ω cho b√†i to√°n c·ªßa b·∫°n:\n",
    "\n",
    "V·ªõi b√†i to√°n d·ª± b√°o nhi·ªát ƒë·ªô c·ª±c ƒë·∫°i theo th·ªùi gian c√≥ geo-feature\n",
    "\n",
    "‚Üí N√™n d√πng Isolation Forest ho·∫∑c Autoencoder ƒë·ªÉ ph√°t hi·ªán outlier multivariate v√¨:\n",
    "\n",
    "- C√≥ th·ªÉ capture m·ªëi quan h·ªá gi·ªØa nhi·ªát ƒë·ªô, ƒë·ªô ·∫©m, kinh ƒë·ªô, vƒ© ƒë·ªô, th√†nh ph·ªë\n",
    "- Kh√¥ng c·∫ßn gi·∫£ ƒë·ªãnh ph√¢n ph·ªëi chu·∫©n\n",
    "- H·ª£p cho d·ªØ li·ªáu nhi·ªÅu bi·∫øn d·∫°ng time series\n",
    "\n",
    "N·∫øu dataset v·ª´a ph·∫£i th√¨ d√πng Isolation Forest\n",
    "N·∫øu dataset l·ªõn ho·∫∑c quan h·ªá non-linear m·∫°nh th√¨ d√πng LSTM-AE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå 4Ô∏è‚É£ T√≥m l·∫°i\n",
    "\n",
    "- Univariate ch·ªâ v·ªõi 1 bi·∫øn ‚Äî ƒë∆°n gi·∫£n nh∆∞ng kh√¥ng t·∫≠n d·ª•ng ƒë∆∞·ª£c y·∫øu t·ªë kh√°c\n",
    "- Multivariate ph·ª©c t·∫°p h∆°n, nhi·ªÅu th√¥ng tin h∆°n nh∆∞ng ph·∫£i x·ª≠ l√Ω outlier ƒë√∫ng c√°ch.\n",
    "\n",
    "Outlier Multivariate n√™n x·ª≠ l√Ω b·∫±ng Isolation Forest, Mahalanobis Distance ho·∫∑c Autoencoder ƒë·ªÉ x√©t t∆∞∆°ng quan nhi·ªÅu chi·ªÅu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìñ C√°c gi√° tr·ªã freq th∆∞·ªùng d√πng trong resample() Pandas:\n",
    "| T·∫ßn su·∫•t       | `freq` string        | √ù nghƒ©a                                   |\n",
    "| :------------- | :------------------- | :---------------------------------------- |\n",
    "| **H√†ng gi√¢y**  | `\"S\"`                | M·ªói gi√¢y                                  |\n",
    "| **H√†ng ph√∫t**  | `\"T\"` (ho·∫∑c `\"min\"`) | M·ªói ph√∫t                                  |\n",
    "| **H√†ng gi·ªù**   | `\"H\"`                | M·ªói gi·ªù                                   |\n",
    "| **H√†ng ng√†y**  | `\"D\"`                | M·ªói ng√†y                                  |\n",
    "| **H√†ng tu·∫ßn**  | `\"W\"`                | M·ªói tu·∫ßn (m·∫∑c ƒë·ªãnh k·∫øt th√∫c v√†o Ch·ªß nh·∫≠t) |\n",
    "| **H√†ng th√°ng** | `\"M\"`                | Cu·ªëi m·ªói th√°ng                            |\n",
    "| **ƒê·∫ßu th√°ng**  | `\"MS\"`               | ƒê·∫ßu m·ªói th√°ng                             |\n",
    "| **H√†ng qu√Ω**   | `\"Q\"`                | Cu·ªëi m·ªói qu√Ω (3, 6, 9, 12)                |\n",
    "| **ƒê·∫ßu qu√Ω**    | `\"QS\"`               | ƒê·∫ßu m·ªói qu√Ω                               |\n",
    "| **H√†ng nƒÉm**   | `\"A\"` ho·∫∑c `\"Y\"`     | Cu·ªëi m·ªói nƒÉm (m·∫∑c ƒë·ªãnh 31-12)             |\n",
    "| **ƒê·∫ßu nƒÉm**    | `\"AS\"` ho·∫∑c `\"YS\"`   | ƒê·∫ßu m·ªói nƒÉm (1-1)                         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots.plot_feature_trends_over_time(data         = station[\"CaMau\"],\n",
    "#                                     data_cols    = numerical_features,\n",
    "#                                     station_name = \"CaMau\",\n",
    "#                                     start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "#                                     end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "#                                     freq         = \"1M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">CaMau</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots.plot_feature_outliers_over_time(data         = station[\"CaMau\"],\n",
    "#                                       data_cols    = numerical_features,\n",
    "#                                       station_name = \"CaMau\",\n",
    "#                                       method       = \"machine_learning\",\n",
    "#                                       models       = dict({\n",
    "#                                                            \"IsolationForest\"         : IsolationForest(bootstrap     = False, \n",
    "#                                                                                                        contamination = \"auto\", \n",
    "#                                                                                                        max_features  = 1.0, \n",
    "#                                                                                                        max_samples   = \"auto\", \n",
    "#                                                                                                        n_estimators  = 100, \n",
    "#                                                                                                        n_jobs        = None, \n",
    "#                                                                                                        random_state  = None, \n",
    "#                                                                                                        verbose       = 0, \n",
    "#                                                                                                        warm_start    = False),\n",
    "#                                                            \"LocalOutlierFactor\"      : LocalOutlierFactor(algorithm     = \"auto\", \n",
    "#                                                                                                           contamination = \"auto\", \n",
    "#                                                                                                           leaf_size     = 30, \n",
    "#                                                                                                           metric        = \"minkowski\", \n",
    "#                                                                                                           metric_params = None, \n",
    "#                                                                                                           n_jobs        = None, \n",
    "#                                                                                                           n_neighbors   = 20, \n",
    "#                                                                                                           novelty       = False, \n",
    "#                                                                                                           p             = 2),\n",
    "#                                                            \"Prophet\"                 : Prophet(growth                  = \"linear\", # SLOW!!!\n",
    "#                                                                                                changepoints            = None,\n",
    "#                                                                                                n_changepoints          = 25,\n",
    "#                                                                                                changepoint_range       = 0.8,\n",
    "#                                                                                                yearly_seasonality      = \"auto\",\n",
    "#                                                                                                weekly_seasonality      = \"auto\",\n",
    "#                                                                                                daily_seasonality       = \"auto\",\n",
    "#                                                                                                holidays                = None,\n",
    "#                                                                                                seasonality_mode        = \"additive\",\n",
    "#                                                                                                seasonality_prior_scale = 10.0,\n",
    "#                                                                                                holidays_prior_scale    = 10.0,\n",
    "#                                                                                                changepoint_prior_scale = 0.05,\n",
    "#                                                                                                mcmc_samples            = 0,\n",
    "#                                                                                                interval_width          = 0.80,\n",
    "#                                                                                                uncertainty_samples     = 1000,\n",
    "#                                                                                                stan_backend            = None),\n",
    "#                                                            \"AgglomerativeClustering\" : AgglomerativeClustering(compute_distances  = False,  # SLOW!!!\n",
    "#                                                                                                                compute_full_tree  = 'auto', \n",
    "#                                                                                                             #    connectivity       = None, # KHONG SUA\n",
    "#                                                                                                                distance_threshold = None, \n",
    "#                                                                                                             #    linkage            = 'ward', # KHONG SUA\n",
    "#                                                                                                                memory             = None, \n",
    "#                                                                                                                metric             = 'euclidean', \n",
    "#                                                                                                                n_clusters         = 3),\n",
    "#                                                            \"HDBSCAN\"                  : HDBSCAN(min_cluster_size               = 5,\n",
    "#                                                                                                                        min_samples                    = None,\n",
    "#                                                                                                                        metric                         = 'euclidean',\n",
    "#                                                                                                                        alpha                          = 1.0,\n",
    "#                                                                                                                        p                              = None,\n",
    "#                                                                                                                        algorithm                      = 'best',\n",
    "#                                                                                                                        leaf_size                      = 40,\n",
    "#                                                                                                                        memory                         = Memory(location=None),\n",
    "#                                                                                                                        approx_min_span_tree           = True,\n",
    "#                                                                                                                        gen_min_span_tree              = False,\n",
    "#                                                                                                                        core_dist_n_jobs               = 1,\n",
    "#                                                                                                                        cluster_selection_method       = 'eom',\n",
    "#                                                                                                                        allow_single_cluster           = False,\n",
    "#                                                                                                                        prediction_data                = False,\n",
    "#                                                                                                                        match_reference_implementation = False),\n",
    "#                                                            \"VanillaAutoencoder\"      : \"Dell bt ghi gi o day de tuning @@@\"\n",
    "#                                                            }),\n",
    "#                                       factor       = 1.5,   # Stick with Prophet model\n",
    "#                                       window_size  = 10,    # Stick with Agglomerative Clustering & DBSCAN model\n",
    "#                                       dendrogram   = False, # Stick with Agglomerative Clustering model\n",
    "#                                       display      = False,\n",
    "#                                       start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "#                                       end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "#                                       freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "\n",
    "\n",
    "station[\"CaMau\"] = dataset.handle_feature_outliers_over_time(data         = station[\"CaMau\"],\n",
    "                                                             data_cols    = numerical_features,\n",
    "                                                             station_name = \"CaMau\",\n",
    "                                                             method       = \"machine_learning\",\n",
    "                                                             models       = dict({\n",
    "                                                                                  \"IsolationForest\"         : IsolationForest(bootstrap     = False, \n",
    "                                                                                                                              contamination = \"auto\", \n",
    "                                                                                                                              max_features  = 1.0, \n",
    "                                                                                                                              max_samples   = \"auto\", \n",
    "                                                                                                                              n_estimators  = 100, \n",
    "                                                                                                                              n_jobs        = None, \n",
    "                                                                                                                              random_state  = None, \n",
    "                                                                                                                              verbose       = 0, \n",
    "                                                                                                                              warm_start    = False),\n",
    "                                                                                  \"LocalOutlierFactor\"      : LocalOutlierFactor(algorithm     = \"auto\", \n",
    "                                                                                                                                 contamination = \"auto\", \n",
    "                                                                                                                                 leaf_size     = 30, \n",
    "                                                                                                                                 metric        = \"minkowski\", \n",
    "                                                                                                                                 metric_params = None, \n",
    "                                                                                                                                 n_jobs        = None, \n",
    "                                                                                                                                 n_neighbors   = 20, \n",
    "                                                                                                                                 novelty       = False, \n",
    "                                                                                                                                 p             = 2),\n",
    "                                                                                  \"Prophet\"                 : Prophet(growth                  = \"linear\", # SLOW!!!\n",
    "                                                                                                                      changepoints            = None,\n",
    "                                                                                                                      n_changepoints          = 25,\n",
    "                                                                                                                      changepoint_range       = 0.8,\n",
    "                                                                                                                      yearly_seasonality      = \"auto\",\n",
    "                                                                                                                      weekly_seasonality      = \"auto\",\n",
    "                                                                                                                      daily_seasonality       = \"auto\",\n",
    "                                                                                                                      holidays                = None,\n",
    "                                                                                                                      seasonality_mode        = \"additive\",\n",
    "                                                                                                                      seasonality_prior_scale = 10.0,\n",
    "                                                                                                                      holidays_prior_scale    = 10.0,\n",
    "                                                                                                                      changepoint_prior_scale = 0.05,\n",
    "                                                                                                                      mcmc_samples            = 0,\n",
    "                                                                                                                      interval_width          = 0.80,\n",
    "                                                                                                                      uncertainty_samples     = 1000,\n",
    "                                                                                                                      stan_backend            = None),\n",
    "                                                                                  \"AgglomerativeClustering\" : AgglomerativeClustering(compute_distances  = False,  # SLOW!!!\n",
    "                                                                                                                                      compute_full_tree  = 'auto', \n",
    "                                                                                                                                   #    connectivity       = None, # KHONG SUA\n",
    "                                                                                                                                      distance_threshold = None, \n",
    "                                                                                                                                   #    linkage            = 'ward', # KHONG SUA\n",
    "                                                                                                                                      memory             = None, \n",
    "                                                                                                                                      metric             = 'euclidean', \n",
    "                                                                                                                                      n_clusters         = 3),\n",
    "                                                                                  \"HDBSCAN\"                  : HDBSCAN(min_cluster_size               = 5,\n",
    "                                                                                                                       min_samples                    = None,\n",
    "                                                                                                                       metric                         = 'euclidean',\n",
    "                                                                                                                       alpha                          = 1.0,\n",
    "                                                                                                                       p                              = None,\n",
    "                                                                                                                       algorithm                      = 'best',\n",
    "                                                                                                                       leaf_size                      = 40,\n",
    "                                                                                                                       memory                         = Memory(location=None),\n",
    "                                                                                                                       approx_min_span_tree           = True,\n",
    "                                                                                                                       gen_min_span_tree              = False,\n",
    "                                                                                                                       core_dist_n_jobs               = 1,\n",
    "                                                                                                                       cluster_selection_method       = 'eom',\n",
    "                                                                                                                       allow_single_cluster           = False,\n",
    "                                                                                                                       prediction_data                = False,\n",
    "                                                                                                                       match_reference_implementation = False),\n",
    "                                                                                  \"VanillaAutoencoder\"      : \"Dell bt ghi gi o day de tuning @@@\"\n",
    "                                                                                  }),\n",
    "                                                             factor       = 1.5,   # Stick with Prophet model\n",
    "                                                             window_size  = 10,    # Stick with Agglomerative Clustering & HDBSCAN model\n",
    "                                                             dendrogram   = False, # Stick with Agglomerative Clustering model\n",
    "                                                             display      = True,\n",
    "                                                             start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                                             end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                                             freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Dong Hoi</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots.plot_feature_outliers_over_time(data         = station[\"DH\"],\n",
    "#                                       data_cols    = numerical_features,\n",
    "#                                       station_name = \"DongHoi\",\n",
    "#                                       method       = \"machine_learning\",\n",
    "#                                       models       = dict({\n",
    "#                                                            \"IsolationForest\"         : IsolationForest(bootstrap     = False, \n",
    "#                                                                                                        contamination = \"auto\", \n",
    "#                                                                                                        max_features  = 1.0, \n",
    "#                                                                                                        max_samples   = \"auto\", \n",
    "#                                                                                                        n_estimators  = 100, \n",
    "#                                                                                                        n_jobs        = None, \n",
    "#                                                                                                        random_state  = None, \n",
    "#                                                                                                        verbose       = 0, \n",
    "#                                                                                                        warm_start    = False),\n",
    "#                                                            \"LocalOutlierFactor\"      : LocalOutlierFactor(algorithm     = \"auto\", \n",
    "#                                                                                                           contamination = \"auto\", \n",
    "#                                                                                                           leaf_size     = 30, \n",
    "#                                                                                                           metric        = \"minkowski\", \n",
    "#                                                                                                           metric_params = None, \n",
    "#                                                                                                           n_jobs        = None, \n",
    "#                                                                                                           n_neighbors   = 20, \n",
    "#                                                                                                           novelty       = False, \n",
    "#                                                                                                           p             = 2),\n",
    "#                                                            \"Prophet\"                 : Prophet(growth                  = \"linear\", # SLOW!!!\n",
    "#                                                                                                changepoints            = None,\n",
    "#                                                                                                n_changepoints          = 25,\n",
    "#                                                                                                changepoint_range       = 0.8,\n",
    "#                                                                                                yearly_seasonality      = \"auto\",\n",
    "#                                                                                                weekly_seasonality      = \"auto\",\n",
    "#                                                                                                daily_seasonality       = \"auto\",\n",
    "#                                                                                                holidays                = None,\n",
    "#                                                                                                seasonality_mode        = \"additive\",\n",
    "#                                                                                                seasonality_prior_scale = 10.0,\n",
    "#                                                                                                holidays_prior_scale    = 10.0,\n",
    "#                                                                                                changepoint_prior_scale = 0.05,\n",
    "#                                                                                                mcmc_samples            = 0,\n",
    "#                                                                                                interval_width          = 0.80,\n",
    "#                                                                                                uncertainty_samples     = 1000,\n",
    "#                                                                                                stan_backend            = None),\n",
    "#                                                            \"AgglomerativeClustering\" : AgglomerativeClustering(compute_distances  = False,  # SLOW!!!\n",
    "#                                                                                                                compute_full_tree  = 'auto', \n",
    "#                                                                                                             #    connectivity       = None, # KHONG SUA\n",
    "#                                                                                                                distance_threshold = None, \n",
    "#                                                                                                             #    linkage            = 'ward', # KHONG SUA\n",
    "#                                                                                                                memory             = None, \n",
    "#                                                                                                                metric             = 'euclidean', \n",
    "#                                                                                                                n_clusters         = 3),\n",
    "#                                                            \"HDBSCAN\"                  : HDBSCAN(min_cluster_size               = 5,\n",
    "#                                                                                                                        min_samples                    = None,\n",
    "#                                                                                                                        metric                         = 'euclidean',\n",
    "#                                                                                                                        alpha                          = 1.0,\n",
    "#                                                                                                                        p                              = None,\n",
    "#                                                                                                                        algorithm                      = 'best',\n",
    "#                                                                                                                        leaf_size                      = 40,\n",
    "#                                                                                                                        memory                         = Memory(location=None),\n",
    "#                                                                                                                        approx_min_span_tree           = True,\n",
    "#                                                                                                                        gen_min_span_tree              = False,\n",
    "#                                                                                                                        core_dist_n_jobs               = 1,\n",
    "#                                                                                                                        cluster_selection_method       = 'eom',\n",
    "#                                                                                                                        allow_single_cluster           = False,\n",
    "#                                                                                                                        prediction_data                = False,\n",
    "#                                                                                                                        match_reference_implementation = False),\n",
    "#                                                            \"VanillaAutoencoder\"      : \"Dell bt ghi gi o day de tuning @@@\"\n",
    "#                                                            }),\n",
    "#                                       factor       = 1.5,   # Stick with Prophet model\n",
    "#                                       window_size  = 10,    # Stick with Agglomerative Clustering & DBSCAN model\n",
    "#                                       dendrogram   = False, # Stick with Agglomerative Clustering model\n",
    "#                                       display      = False,\n",
    "#                                       start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "#                                       end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "#                                       freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"DH\"] = dataset.handle_feature_outliers_over_time(data         = station[\"DH\"],\n",
    "                                                          data_cols    = numerical_features,\n",
    "                                                          station_name = \"DongHoi\",\n",
    "                                                          method       = \"machine_learning\",\n",
    "                                                          models       = dict({\n",
    "                                                                               \"IsolationForest\"         : IsolationForest(bootstrap     = False, \n",
    "                                                                                                                           contamination = \"auto\", \n",
    "                                                                                                                           max_features  = 1.0, \n",
    "                                                                                                                           max_samples   = \"auto\", \n",
    "                                                                                                                           n_estimators  = 100, \n",
    "                                                                                                                           n_jobs        = None, \n",
    "                                                                                                                           random_state  = None, \n",
    "                                                                                                                           verbose       = 0, \n",
    "                                                                                                                           warm_start    = False),\n",
    "                                                                               \"LocalOutlierFactor\"      : LocalOutlierFactor(algorithm     = \"auto\", \n",
    "                                                                                                                              contamination = \"auto\", \n",
    "                                                                                                                              leaf_size     = 30, \n",
    "                                                                                                                              metric        = \"minkowski\", \n",
    "                                                                                                                              metric_params = None, \n",
    "                                                                                                                              n_jobs        = None, \n",
    "                                                                                                                              n_neighbors   = 20, \n",
    "                                                                                                                              novelty       = False, \n",
    "                                                                                                                              p             = 2),\n",
    "                                                                               \"Prophet\"                 : Prophet(growth                  = \"linear\", # SLOW!!!\n",
    "                                                                                                                   changepoints            = None,\n",
    "                                                                                                                   n_changepoints          = 25,\n",
    "                                                                                                                   changepoint_range       = 0.8,\n",
    "                                                                                                                   yearly_seasonality      = \"auto\",\n",
    "                                                                                                                   weekly_seasonality      = \"auto\",\n",
    "                                                                                                                   daily_seasonality       = \"auto\",\n",
    "                                                                                                                   holidays                = None,\n",
    "                                                                                                                   seasonality_mode        = \"additive\",\n",
    "                                                                                                                   seasonality_prior_scale = 10.0,\n",
    "                                                                                                                   holidays_prior_scale    = 10.0,\n",
    "                                                                                                                   changepoint_prior_scale = 0.05,\n",
    "                                                                                                                   mcmc_samples            = 0,\n",
    "                                                                                                                   interval_width          = 0.80,\n",
    "                                                                                                                   uncertainty_samples     = 1000,\n",
    "                                                                                                                   stan_backend            = None),\n",
    "                                                                               \"AgglomerativeClustering\" : AgglomerativeClustering(compute_distances  = False,  # SLOW!!!\n",
    "                                                                                                                                   compute_full_tree  = 'auto', \n",
    "                                                                                                                                #    connectivity       = None, # KHONG SUA\n",
    "                                                                                                                                   distance_threshold = None, \n",
    "                                                                                                                                #    linkage            = 'ward', # KHONG SUA\n",
    "                                                                                                                                   memory             = None, \n",
    "                                                                                                                                   metric             = 'euclidean', \n",
    "                                                                                                                                   n_clusters         = 3),\n",
    "                                                                               \"HDBSCAN\"                  : HDBSCAN(min_cluster_size               = 5,\n",
    "                                                                                                                       min_samples                    = None,\n",
    "                                                                                                                       metric                         = 'euclidean',\n",
    "                                                                                                                       alpha                          = 1.0,\n",
    "                                                                                                                       p                              = None,\n",
    "                                                                                                                       algorithm                      = 'best',\n",
    "                                                                                                                       leaf_size                      = 40,\n",
    "                                                                                                                       memory                         = Memory(location=None),\n",
    "                                                                                                                       approx_min_span_tree           = True,\n",
    "                                                                                                                       gen_min_span_tree              = False,\n",
    "                                                                                                                       core_dist_n_jobs               = 1,\n",
    "                                                                                                                       cluster_selection_method       = 'eom',\n",
    "                                                                                                                       allow_single_cluster           = False,\n",
    "                                                                                                                       prediction_data                = False,\n",
    "                                                                                                                       match_reference_implementation = False),\n",
    "                                                                               \"VanillaAutoencoder\"      : \"Dell bt ghi gi o day de tuning @@@\"\n",
    "                                                                               }),\n",
    "                                                          factor       = 1.5,   # Stick with Prophet model\n",
    "                                                          window_size  = 10,    # Stick with Agglomerative Clustering & DBSCAN model\n",
    "                                                          dendrogram   = False, # Stick with Agglomerative Clustering model\n",
    "                                                          display      = True,\n",
    "                                                          start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                                          end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                                          freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Noi Bai</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots.plot_feature_outliers_over_time(data         = station[\"NB\"],\n",
    "#                                       data_cols    = numerical_features,\n",
    "#                                       station_name = \"NoiBai\",\n",
    "#                                       method       = \"machine_learning\",\n",
    "#                                       models       = dict({\n",
    "#                                                            \"IsolationForest\"         : IsolationForest(bootstrap     = False, \n",
    "#                                                                                                        contamination = \"auto\", \n",
    "#                                                                                                        max_features  = 1.0, \n",
    "#                                                                                                        max_samples   = \"auto\", \n",
    "#                                                                                                        n_estimators  = 100, \n",
    "#                                                                                                        n_jobs        = None, \n",
    "#                                                                                                        random_state  = None, \n",
    "#                                                                                                        verbose       = 0, \n",
    "#                                                                                                        warm_start    = False),\n",
    "#                                                            \"LocalOutlierFactor\"      : LocalOutlierFactor(algorithm     = \"auto\", \n",
    "#                                                                                                           contamination = \"auto\", \n",
    "#                                                                                                           leaf_size     = 30, \n",
    "#                                                                                                           metric        = \"minkowski\", \n",
    "#                                                                                                           metric_params = None, \n",
    "#                                                                                                           n_jobs        = None, \n",
    "#                                                                                                           n_neighbors   = 20, \n",
    "#                                                                                                           novelty       = False, \n",
    "#                                                                                                           p             = 2),\n",
    "#                                                            \"Prophet\"                 : Prophet(growth                  = \"linear\", # SLOW!!!\n",
    "#                                                                                                changepoints            = None,\n",
    "#                                                                                                n_changepoints          = 25,\n",
    "#                                                                                                changepoint_range       = 0.8,\n",
    "#                                                                                                yearly_seasonality      = \"auto\",\n",
    "#                                                                                                weekly_seasonality      = \"auto\",\n",
    "#                                                                                                daily_seasonality       = \"auto\",\n",
    "#                                                                                                holidays                = None,\n",
    "#                                                                                                seasonality_mode        = \"additive\",\n",
    "#                                                                                                seasonality_prior_scale = 10.0,\n",
    "#                                                                                                holidays_prior_scale    = 10.0,\n",
    "#                                                                                                changepoint_prior_scale = 0.05,\n",
    "#                                                                                                mcmc_samples            = 0,\n",
    "#                                                                                                interval_width          = 0.80,\n",
    "#                                                                                                uncertainty_samples     = 1000,\n",
    "#                                                                                                stan_backend            = None),\n",
    "#                                                            \"AgglomerativeClustering\" : AgglomerativeClustering(compute_distances  = False,  # SLOW!!!\n",
    "#                                                                                                                compute_full_tree  = 'auto', \n",
    "#                                                                                                             #    connectivity       = None, # KHONG SUA\n",
    "#                                                                                                                distance_threshold = None, \n",
    "#                                                                                                             #    linkage            = 'ward', # KHONG SUA\n",
    "#                                                                                                                memory             = None, \n",
    "#                                                                                                                metric             = 'euclidean', \n",
    "#                                                                                                                n_clusters         = 3),\n",
    "#                                                            \"HDBSCAN\"                  : HDBSCAN(min_cluster_size               = 5,\n",
    "#                                                                                                                        min_samples                    = None,\n",
    "#                                                                                                                        metric                         = 'euclidean',\n",
    "#                                                                                                                        alpha                          = 1.0,\n",
    "#                                                                                                                        p                              = None,\n",
    "#                                                                                                                        algorithm                      = 'best',\n",
    "#                                                                                                                        leaf_size                      = 40,\n",
    "#                                                                                                                        memory                         = Memory(location=None),\n",
    "#                                                                                                                        approx_min_span_tree           = True,\n",
    "#                                                                                                                        gen_min_span_tree              = False,\n",
    "#                                                                                                                        core_dist_n_jobs               = 1,\n",
    "#                                                                                                                        cluster_selection_method       = 'eom',\n",
    "#                                                                                                                        allow_single_cluster           = False,\n",
    "#                                                                                                                        prediction_data                = False,\n",
    "#                                                                                                                        match_reference_implementation = False),\n",
    "#                                                            \"VanillaAutoencoder\"      : \"Dell bt ghi gi o day de tuning @@@\"\n",
    "#                                                            }),\n",
    "#                                       factor       = 1.5,   # Stick with Prophet model\n",
    "#                                       window_size  = 10,    # Stick with Agglomerative Clustering & DBSCAN model\n",
    "#                                       dendrogram   = False, # Stick with Agglomerative Clustering model\n",
    "#                                       display      = False,\n",
    "#                                       start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "#                                       end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "#                                       freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"NB\"] = dataset.handle_feature_outliers_over_time(data         = station[\"NB\"],\n",
    "                                                          data_cols    = numerical_features,\n",
    "                                                          station_name = \"NoiBai\",\n",
    "                                                          method       = \"machine_learning\",\n",
    "                                                          models       = dict({\n",
    "                                                                               \"IsolationForest\"         : IsolationForest(bootstrap     = False, \n",
    "                                                                                                                           contamination = \"auto\", \n",
    "                                                                                                                           max_features  = 1.0, \n",
    "                                                                                                                           max_samples   = \"auto\", \n",
    "                                                                                                                           n_estimators  = 100, \n",
    "                                                                                                                           n_jobs        = None, \n",
    "                                                                                                                           random_state  = None, \n",
    "                                                                                                                           verbose       = 0, \n",
    "                                                                                                                           warm_start    = False),\n",
    "                                                                               \"LocalOutlierFactor\"      : LocalOutlierFactor(algorithm     = \"auto\", \n",
    "                                                                                                                              contamination = \"auto\", \n",
    "                                                                                                                              leaf_size     = 30, \n",
    "                                                                                                                              metric        = \"minkowski\", \n",
    "                                                                                                                              metric_params = None, \n",
    "                                                                                                                              n_jobs        = None, \n",
    "                                                                                                                              n_neighbors   = 20, \n",
    "                                                                                                                              novelty       = False, \n",
    "                                                                                                                              p             = 2),\n",
    "                                                                               \"Prophet\"                 : Prophet(growth                  = \"linear\", # SLOW!!!\n",
    "                                                                                                                   changepoints            = None,\n",
    "                                                                                                                   n_changepoints          = 25,\n",
    "                                                                                                                   changepoint_range       = 0.8,\n",
    "                                                                                                                   yearly_seasonality      = \"auto\",\n",
    "                                                                                                                   weekly_seasonality      = \"auto\",\n",
    "                                                                                                                   daily_seasonality       = \"auto\",\n",
    "                                                                                                                   holidays                = None,\n",
    "                                                                                                                   seasonality_mode        = \"additive\",\n",
    "                                                                                                                   seasonality_prior_scale = 10.0,\n",
    "                                                                                                                   holidays_prior_scale    = 10.0,\n",
    "                                                                                                                   changepoint_prior_scale = 0.05,\n",
    "                                                                                                                   mcmc_samples            = 0,\n",
    "                                                                                                                   interval_width          = 0.80,\n",
    "                                                                                                                   uncertainty_samples     = 1000,\n",
    "                                                                                                                   stan_backend            = None),\n",
    "                                                                               \"AgglomerativeClustering\" : AgglomerativeClustering(compute_distances  = False,  # SLOW!!!\n",
    "                                                                                                                                   compute_full_tree  = 'auto', \n",
    "                                                                                                                                #    connectivity       = None, # KHONG SUA\n",
    "                                                                                                                                   distance_threshold = None, \n",
    "                                                                                                                                #    linkage            = 'ward', # KHONG SUA\n",
    "                                                                                                                                   memory             = None, \n",
    "                                                                                                                                   metric             = 'euclidean', \n",
    "                                                                                                                                   n_clusters         = 3),\n",
    "                                                                               \"HDBSCAN\"                  : HDBSCAN(min_cluster_size               = 5,\n",
    "                                                                                                                       min_samples                    = None,\n",
    "                                                                                                                       metric                         = 'euclidean',\n",
    "                                                                                                                       alpha                          = 1.0,\n",
    "                                                                                                                       p                              = None,\n",
    "                                                                                                                       algorithm                      = 'best',\n",
    "                                                                                                                       leaf_size                      = 40,\n",
    "                                                                                                                       memory                         = Memory(location=None),\n",
    "                                                                                                                       approx_min_span_tree           = True,\n",
    "                                                                                                                       gen_min_span_tree              = False,\n",
    "                                                                                                                       core_dist_n_jobs               = 1,\n",
    "                                                                                                                       cluster_selection_method       = 'eom',\n",
    "                                                                                                                       allow_single_cluster           = False,\n",
    "                                                                                                                       prediction_data                = False,\n",
    "                                                                                                                       match_reference_implementation = False),\n",
    "                                                                               \"VanillaAutoencoder\"      : \"Dell bt ghi gi o day de tuning @@@\"\n",
    "                                                                               }),\n",
    "                                                          factor       = 1.5,   # Stick with Prophet model\n",
    "                                                          window_size  = 10,    # Stick with Agglomerative Clustering & DBSCAN model\n",
    "                                                          dendrogram   = False, # Stick with Agglomerative Clustering model\n",
    "                                                          display      = True,\n",
    "                                                          start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                                          end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                                          freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Quy Nhon</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots.plot_feature_outliers_over_time(data         = station[\"QN\"],\n",
    "#                                       data_cols    = numerical_features,\n",
    "#                                       station_name = \"QuyNhon\",\n",
    "#                                       method       = \"machine_learning\",\n",
    "#                                       models       = dict({\n",
    "#                                                            \"IsolationForest\"         : IsolationForest(bootstrap     = False, \n",
    "#                                                                                                        contamination = \"auto\", \n",
    "#                                                                                                        max_features  = 1.0, \n",
    "#                                                                                                        max_samples   = \"auto\", \n",
    "#                                                                                                        n_estimators  = 100, \n",
    "#                                                                                                        n_jobs        = None, \n",
    "#                                                                                                        random_state  = None, \n",
    "#                                                                                                        verbose       = 0, \n",
    "#                                                                                                        warm_start    = False),\n",
    "#                                                            \"LocalOutlierFactor\"      : LocalOutlierFactor(algorithm     = \"auto\", \n",
    "#                                                                                                           contamination = \"auto\", \n",
    "#                                                                                                           leaf_size     = 30, \n",
    "#                                                                                                           metric        = \"minkowski\", \n",
    "#                                                                                                           metric_params = None, \n",
    "#                                                                                                           n_jobs        = None, \n",
    "#                                                                                                           n_neighbors   = 20, \n",
    "#                                                                                                           novelty       = False, \n",
    "#                                                                                                           p             = 2),\n",
    "#                                                            \"Prophet\"                 : Prophet(growth                  = \"linear\", # SLOW!!!\n",
    "#                                                                                                changepoints            = None,\n",
    "#                                                                                                n_changepoints          = 25,\n",
    "#                                                                                                changepoint_range       = 0.8,\n",
    "#                                                                                                yearly_seasonality      = \"auto\",\n",
    "#                                                                                                weekly_seasonality      = \"auto\",\n",
    "#                                                                                                daily_seasonality       = \"auto\",\n",
    "#                                                                                                holidays                = None,\n",
    "#                                                                                                seasonality_mode        = \"additive\",\n",
    "#                                                                                                seasonality_prior_scale = 10.0,\n",
    "#                                                                                                holidays_prior_scale    = 10.0,\n",
    "#                                                                                                changepoint_prior_scale = 0.05,\n",
    "#                                                                                                mcmc_samples            = 0,\n",
    "#                                                                                                interval_width          = 0.80,\n",
    "#                                                                                                uncertainty_samples     = 1000,\n",
    "#                                                                                                stan_backend            = None),\n",
    "#                                                            \"AgglomerativeClustering\" : AgglomerativeClustering(compute_distances  = False,  # SLOW!!!\n",
    "#                                                                                                                compute_full_tree  = 'auto', \n",
    "#                                                                                                             #    connectivity       = None, # KHONG SUA\n",
    "#                                                                                                                distance_threshold = None, \n",
    "#                                                                                                             #    linkage            = 'ward', # KHONG SUA\n",
    "#                                                                                                                memory             = None, \n",
    "#                                                                                                                metric             = 'euclidean', \n",
    "#                                                                                                                n_clusters         = 3),\n",
    "#                                                            \"HDBSCAN\"                  : HDBSCAN(min_cluster_size               = 5,\n",
    "#                                                                                                                        min_samples                    = None,\n",
    "#                                                                                                                        metric                         = 'euclidean',\n",
    "#                                                                                                                        alpha                          = 1.0,\n",
    "#                                                                                                                        p                              = None,\n",
    "#                                                                                                                        algorithm                      = 'best',\n",
    "#                                                                                                                        leaf_size                      = 40,\n",
    "#                                                                                                                        memory                         = Memory(location=None),\n",
    "#                                                                                                                        approx_min_span_tree           = True,\n",
    "#                                                                                                                        gen_min_span_tree              = False,\n",
    "#                                                                                                                        core_dist_n_jobs               = 1,\n",
    "#                                                                                                                        cluster_selection_method       = 'eom',\n",
    "#                                                                                                                        allow_single_cluster           = False,\n",
    "#                                                                                                                        prediction_data                = False,\n",
    "#                                                                                                                        match_reference_implementation = False),\n",
    "#                                                            \"VanillaAutoencoder\"      : \"Dell bt ghi gi o day de tuning @@@\"\n",
    "#                                                            }),\n",
    "#                                       factor       = 1.5,   # Stick with Prophet model\n",
    "#                                       window_size  = 10,    # Stick with Agglomerative Clustering & DBSCAN model\n",
    "#                                       dendrogram   = False, # Stick with Agglomerative Clustering model\n",
    "#                                       display      = False,\n",
    "#                                       start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "#                                       end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "#                                       freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"QN\"] = dataset.handle_feature_outliers_over_time(data         = station[\"QN\"],\n",
    "                                                          data_cols    = numerical_features,\n",
    "                                                          station_name = \"QuyNhon\",\n",
    "                                                          method       = \"machine_learning\",\n",
    "                                                          models       = dict({\n",
    "                                                                               \"IsolationForest\"         : IsolationForest(bootstrap     = False, \n",
    "                                                                                                                           contamination = \"auto\", \n",
    "                                                                                                                           max_features  = 1.0, \n",
    "                                                                                                                           max_samples   = \"auto\", \n",
    "                                                                                                                           n_estimators  = 100, \n",
    "                                                                                                                           n_jobs        = None, \n",
    "                                                                                                                           random_state  = None, \n",
    "                                                                                                                           verbose       = 0, \n",
    "                                                                                                                           warm_start    = False),\n",
    "                                                                               \"LocalOutlierFactor\"      : LocalOutlierFactor(algorithm     = \"auto\", \n",
    "                                                                                                                              contamination = \"auto\", \n",
    "                                                                                                                              leaf_size     = 30, \n",
    "                                                                                                                              metric        = \"minkowski\", \n",
    "                                                                                                                              metric_params = None, \n",
    "                                                                                                                              n_jobs        = None, \n",
    "                                                                                                                              n_neighbors   = 20, \n",
    "                                                                                                                              novelty       = False, \n",
    "                                                                                                                              p             = 2),\n",
    "                                                                               \"Prophet\"                 : Prophet(growth                  = \"linear\", # SLOW!!!\n",
    "                                                                                                                   changepoints            = None,\n",
    "                                                                                                                   n_changepoints          = 25,\n",
    "                                                                                                                   changepoint_range       = 0.8,\n",
    "                                                                                                                   yearly_seasonality      = \"auto\",\n",
    "                                                                                                                   weekly_seasonality      = \"auto\",\n",
    "                                                                                                                   daily_seasonality       = \"auto\",\n",
    "                                                                                                                   holidays                = None,\n",
    "                                                                                                                   seasonality_mode        = \"additive\",\n",
    "                                                                                                                   seasonality_prior_scale = 10.0,\n",
    "                                                                                                                   holidays_prior_scale    = 10.0,\n",
    "                                                                                                                   changepoint_prior_scale = 0.05,\n",
    "                                                                                                                   mcmc_samples            = 0,\n",
    "                                                                                                                   interval_width          = 0.80,\n",
    "                                                                                                                   uncertainty_samples     = 1000,\n",
    "                                                                                                                   stan_backend            = None),\n",
    "                                                                               \"AgglomerativeClustering\" : AgglomerativeClustering(compute_distances  = False,  # SLOW!!!\n",
    "                                                                                                                                   compute_full_tree  = 'auto', \n",
    "                                                                                                                                #    connectivity       = None, # KHONG SUA\n",
    "                                                                                                                                   distance_threshold = None, \n",
    "                                                                                                                                #    linkage            = 'ward', # KHONG SUA\n",
    "                                                                                                                                   memory             = None, \n",
    "                                                                                                                                   metric             = 'euclidean', \n",
    "                                                                                                                                   n_clusters         = 3),\n",
    "                                                                               \"HDBSCAN\"                  : HDBSCAN(min_cluster_size               = 5,\n",
    "                                                                                                                       min_samples                    = None,\n",
    "                                                                                                                       metric                         = 'euclidean',\n",
    "                                                                                                                       alpha                          = 1.0,\n",
    "                                                                                                                       p                              = None,\n",
    "                                                                                                                       algorithm                      = 'best',\n",
    "                                                                                                                       leaf_size                      = 40,\n",
    "                                                                                                                       memory                         = Memory(location=None),\n",
    "                                                                                                                       approx_min_span_tree           = True,\n",
    "                                                                                                                       gen_min_span_tree              = False,\n",
    "                                                                                                                       core_dist_n_jobs               = 1,\n",
    "                                                                                                                       cluster_selection_method       = 'eom',\n",
    "                                                                                                                       allow_single_cluster           = False,\n",
    "                                                                                                                       prediction_data                = False,\n",
    "                                                                                                                       match_reference_implementation = False),\n",
    "                                                                               \"VanillaAutoencoder\"      : \"Dell bt ghi gi o day de tuning @@@\"\n",
    "                                                                               }),\n",
    "                                                          factor       = 1.5,   # Stick with Prophet model\n",
    "                                                          window_size  = 10,    # Stick with Agglomerative Clustering & DBSCAN model\n",
    "                                                          dendrogram   = False, # Stick with Agglomerative Clustering model\n",
    "                                                          display      = True,\n",
    "                                                          start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                                          end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                                          freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Thanh Hoa</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots.plot_feature_outliers_over_time(data         = station[\"TH\"],\n",
    "#                                       data_cols    = numerical_features,\n",
    "#                                       station_name = \"ThanhHoa\",\n",
    "#                                       method       = \"machine_learning\",\n",
    "#                                       models       = dict({\n",
    "#                                                            \"IsolationForest\"         : IsolationForest(bootstrap     = False, \n",
    "#                                                                                                        contamination = \"auto\", \n",
    "#                                                                                                        max_features  = 1.0, \n",
    "#                                                                                                        max_samples   = \"auto\", \n",
    "#                                                                                                        n_estimators  = 100, \n",
    "#                                                                                                        n_jobs        = None, \n",
    "#                                                                                                        random_state  = None, \n",
    "#                                                                                                        verbose       = 0, \n",
    "#                                                                                                        warm_start    = False),\n",
    "#                                                            \"LocalOutlierFactor\"      : LocalOutlierFactor(algorithm     = \"auto\", \n",
    "#                                                                                                           contamination = \"auto\", \n",
    "#                                                                                                           leaf_size     = 30, \n",
    "#                                                                                                           metric        = \"minkowski\", \n",
    "#                                                                                                           metric_params = None, \n",
    "#                                                                                                           n_jobs        = None, \n",
    "#                                                                                                           n_neighbors   = 20, \n",
    "#                                                                                                           novelty       = False, \n",
    "#                                                                                                           p             = 2),\n",
    "#                                                            \"Prophet\"                 : Prophet(growth                  = \"linear\", # SLOW!!!\n",
    "#                                                                                                changepoints            = None,\n",
    "#                                                                                                n_changepoints          = 25,\n",
    "#                                                                                                changepoint_range       = 0.8,\n",
    "#                                                                                                yearly_seasonality      = \"auto\",\n",
    "#                                                                                                weekly_seasonality      = \"auto\",\n",
    "#                                                                                                daily_seasonality       = \"auto\",\n",
    "#                                                                                                holidays                = None,\n",
    "#                                                                                                seasonality_mode        = \"additive\",\n",
    "#                                                                                                seasonality_prior_scale = 10.0,\n",
    "#                                                                                                holidays_prior_scale    = 10.0,\n",
    "#                                                                                                changepoint_prior_scale = 0.05,\n",
    "#                                                                                                mcmc_samples            = 0,\n",
    "#                                                                                                interval_width          = 0.80,\n",
    "#                                                                                                uncertainty_samples     = 1000,\n",
    "#                                                                                                stan_backend            = None),\n",
    "#                                                            \"AgglomerativeClustering\" : AgglomerativeClustering(compute_distances  = False,  # SLOW!!!\n",
    "#                                                                                                                compute_full_tree  = 'auto', \n",
    "#                                                                                                             #    connectivity       = None, # KHONG SUA\n",
    "#                                                                                                                distance_threshold = None, \n",
    "#                                                                                                             #    linkage            = 'ward', # KHONG SUA\n",
    "#                                                                                                                memory             = None, \n",
    "#                                                                                                                metric             = 'euclidean', \n",
    "#                                                                                                                n_clusters         = 3),\n",
    "#                                                            \"HDBSCAN\"                  : HDBSCAN(min_cluster_size               = 5,\n",
    "#                                                                                                                        min_samples                    = None,\n",
    "#                                                                                                                        metric                         = 'euclidean',\n",
    "#                                                                                                                        alpha                          = 1.0,\n",
    "#                                                                                                                        p                              = None,\n",
    "#                                                                                                                        algorithm                      = 'best',\n",
    "#                                                                                                                        leaf_size                      = 40,\n",
    "#                                                                                                                        memory                         = Memory(location=None),\n",
    "#                                                                                                                        approx_min_span_tree           = True,\n",
    "#                                                                                                                        gen_min_span_tree              = False,\n",
    "#                                                                                                                        core_dist_n_jobs               = 1,\n",
    "#                                                                                                                        cluster_selection_method       = 'eom',\n",
    "#                                                                                                                        allow_single_cluster           = False,\n",
    "#                                                                                                                        prediction_data                = False,\n",
    "#                                                                                                                        match_reference_implementation = False),\n",
    "#                                                            \"VanillaAutoencoder\"      : \"Dell bt ghi gi o day de tuning @@@\"\n",
    "#                                                            }),\n",
    "#                                       factor       = 1.5,   # Stick with Prophet model\n",
    "#                                       window_size  = 10,    # Stick with Agglomerative Clustering & DBSCAN model\n",
    "#                                       dendrogram   = False, # Stick with Agglomerative Clustering model\n",
    "#                                       display      = False,\n",
    "#                                       start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "#                                       end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "#                                       freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"TH\"] = dataset.handle_feature_outliers_over_time(data         = station[\"TH\"],\n",
    "                                                          data_cols    = numerical_features,\n",
    "                                                          station_name = \"ThanhHoa\",\n",
    "                                                          method       = \"machine_learning\",\n",
    "                                                          models       = dict({\n",
    "                                                                               \"IsolationForest\"         : IsolationForest(bootstrap     = False, \n",
    "                                                                                                                           contamination = \"auto\", \n",
    "                                                                                                                           max_features  = 1.0, \n",
    "                                                                                                                           max_samples   = \"auto\", \n",
    "                                                                                                                           n_estimators  = 100, \n",
    "                                                                                                                           n_jobs        = None, \n",
    "                                                                                                                           random_state  = None, \n",
    "                                                                                                                           verbose       = 0, \n",
    "                                                                                                                           warm_start    = False),\n",
    "                                                                               \"LocalOutlierFactor\"      : LocalOutlierFactor(algorithm     = \"auto\", \n",
    "                                                                                                                              contamination = \"auto\", \n",
    "                                                                                                                              leaf_size     = 30, \n",
    "                                                                                                                              metric        = \"minkowski\", \n",
    "                                                                                                                              metric_params = None, \n",
    "                                                                                                                              n_jobs        = None, \n",
    "                                                                                                                              n_neighbors   = 20, \n",
    "                                                                                                                              novelty       = False, \n",
    "                                                                                                                              p             = 2),\n",
    "                                                                               \"Prophet\"                 : Prophet(growth                  = \"linear\", # SLOW!!!\n",
    "                                                                                                                   changepoints            = None,\n",
    "                                                                                                                   n_changepoints          = 25,\n",
    "                                                                                                                   changepoint_range       = 0.8,\n",
    "                                                                                                                   yearly_seasonality      = \"auto\",\n",
    "                                                                                                                   weekly_seasonality      = \"auto\",\n",
    "                                                                                                                   daily_seasonality       = \"auto\",\n",
    "                                                                                                                   holidays                = None,\n",
    "                                                                                                                   seasonality_mode        = \"additive\",\n",
    "                                                                                                                   seasonality_prior_scale = 10.0,\n",
    "                                                                                                                   holidays_prior_scale    = 10.0,\n",
    "                                                                                                                   changepoint_prior_scale = 0.05,\n",
    "                                                                                                                   mcmc_samples            = 0,\n",
    "                                                                                                                   interval_width          = 0.80,\n",
    "                                                                                                                   uncertainty_samples     = 1000,\n",
    "                                                                                                                   stan_backend            = None),\n",
    "                                                                               \"AgglomerativeClustering\" : AgglomerativeClustering(compute_distances  = False,  # SLOW!!!\n",
    "                                                                                                                                   compute_full_tree  = 'auto', \n",
    "                                                                                                                                #    connectivity       = None, # KHONG SUA\n",
    "                                                                                                                                   distance_threshold = None, \n",
    "                                                                                                                                #    linkage            = 'ward', # KHONG SUA\n",
    "                                                                                                                                   memory             = None, \n",
    "                                                                                                                                   metric             = 'euclidean', \n",
    "                                                                                                                                   n_clusters         = 3),\n",
    "                                                                               \"HDBSCAN\"                  : HDBSCAN(min_cluster_size               = 5,\n",
    "                                                                                                                       min_samples                    = None,\n",
    "                                                                                                                       metric                         = 'euclidean',\n",
    "                                                                                                                       alpha                          = 1.0,\n",
    "                                                                                                                       p                              = None,\n",
    "                                                                                                                       algorithm                      = 'best',\n",
    "                                                                                                                       leaf_size                      = 40,\n",
    "                                                                                                                       memory                         = Memory(location=None),\n",
    "                                                                                                                       approx_min_span_tree           = True,\n",
    "                                                                                                                       gen_min_span_tree              = False,\n",
    "                                                                                                                       core_dist_n_jobs               = 1,\n",
    "                                                                                                                       cluster_selection_method       = 'eom',\n",
    "                                                                                                                       allow_single_cluster           = False,\n",
    "                                                                                                                       prediction_data                = False,\n",
    "                                                                                                                       match_reference_implementation = False),\n",
    "                                                                               \"VanillaAutoencoder\"      : \"Dell bt ghi gi o day de tuning @@@\"\n",
    "                                                                               }),\n",
    "                                                          factor       = 1.5,   # Stick with Prophet model\n",
    "                                                          window_size  = 10,    # Stick with Agglomerative Clustering & DBSCAN model\n",
    "                                                          dendrogram   = False, # Stick with Agglomerative Clustering model\n",
    "                                                          display      = True,\n",
    "                                                          start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                                          end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                                          freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">TSN</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots.plot_feature_outliers_over_time(data         = station[\"TSN\"],\n",
    "#                                       data_cols    = numerical_features,\n",
    "#                                       station_name = \"TSN\",\n",
    "#                                       method       = \"machine_learning\",\n",
    "#                                       models       = dict({\n",
    "#                                                            \"IsolationForest\"         : IsolationForest(bootstrap     = False, \n",
    "#                                                                                                        contamination = \"auto\", \n",
    "#                                                                                                        max_features  = 1.0, \n",
    "#                                                                                                        max_samples   = \"auto\", \n",
    "#                                                                                                        n_estimators  = 100, \n",
    "#                                                                                                        n_jobs        = None, \n",
    "#                                                                                                        random_state  = None, \n",
    "#                                                                                                        verbose       = 0, \n",
    "#                                                                                                        warm_start    = False),\n",
    "#                                                            \"LocalOutlierFactor\"      : LocalOutlierFactor(algorithm     = \"auto\", \n",
    "#                                                                                                           contamination = \"auto\", \n",
    "#                                                                                                           leaf_size     = 30, \n",
    "#                                                                                                           metric        = \"minkowski\", \n",
    "#                                                                                                           metric_params = None, \n",
    "#                                                                                                           n_jobs        = None, \n",
    "#                                                                                                           n_neighbors   = 20, \n",
    "#                                                                                                           novelty       = False, \n",
    "#                                                                                                           p             = 2),\n",
    "#                                                            \"Prophet\"                 : Prophet(growth                  = \"linear\", # SLOW!!!\n",
    "#                                                                                                changepoints            = None,\n",
    "#                                                                                                n_changepoints          = 25,\n",
    "#                                                                                                changepoint_range       = 0.8,\n",
    "#                                                                                                yearly_seasonality      = \"auto\",\n",
    "#                                                                                                weekly_seasonality      = \"auto\",\n",
    "#                                                                                                daily_seasonality       = \"auto\",\n",
    "#                                                                                                holidays                = None,\n",
    "#                                                                                                seasonality_mode        = \"additive\",\n",
    "#                                                                                                seasonality_prior_scale = 10.0,\n",
    "#                                                                                                holidays_prior_scale    = 10.0,\n",
    "#                                                                                                changepoint_prior_scale = 0.05,\n",
    "#                                                                                                mcmc_samples            = 0,\n",
    "#                                                                                                interval_width          = 0.80,\n",
    "#                                                                                                uncertainty_samples     = 1000,\n",
    "#                                                                                                stan_backend            = None),\n",
    "#                                                            \"AgglomerativeClustering\" : AgglomerativeClustering(compute_distances  = False,  # SLOW!!!\n",
    "#                                                                                                                compute_full_tree  = 'auto', \n",
    "#                                                                                                             #    connectivity       = None, # KHONG SUA\n",
    "#                                                                                                                distance_threshold = None, \n",
    "#                                                                                                             #    linkage            = 'ward', # KHONG SUA\n",
    "#                                                                                                                memory             = None, \n",
    "#                                                                                                                metric             = 'euclidean', \n",
    "#                                                                                                                n_clusters         = 3),\n",
    "#                                                            \"HDBSCAN\"                  : HDBSCAN(min_cluster_size               = 5,\n",
    "#                                                                                                                        min_samples                    = None,\n",
    "#                                                                                                                        metric                         = 'euclidean',\n",
    "#                                                                                                                        alpha                          = 1.0,\n",
    "#                                                                                                                        p                              = None,\n",
    "#                                                                                                                        algorithm                      = 'best',\n",
    "#                                                                                                                        leaf_size                      = 40,\n",
    "#                                                                                                                        memory                         = Memory(location=None),\n",
    "#                                                                                                                        approx_min_span_tree           = True,\n",
    "#                                                                                                                        gen_min_span_tree              = False,\n",
    "#                                                                                                                        core_dist_n_jobs               = 1,\n",
    "#                                                                                                                        cluster_selection_method       = 'eom',\n",
    "#                                                                                                                        allow_single_cluster           = False,\n",
    "#                                                                                                                        prediction_data                = False,\n",
    "#                                                                                                                        match_reference_implementation = False),\n",
    "#                                                            \"VanillaAutoencoder\"      : \"Dell bt ghi gi o day de tuning @@@\"\n",
    "#                                                            }),\n",
    "#                                       factor       = 1.5,   # Stick with Prophet model\n",
    "#                                       window_size  = 10,    # Stick with Agglomerative Clustering & DBSCAN model\n",
    "#                                       dendrogram   = False, # Stick with Agglomerative Clustering model\n",
    "#                                       display      = False,\n",
    "#                                       start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "#                                       end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "#                                       freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"TSN\"] = dataset.handle_feature_outliers_over_time(data         = station[\"TSN\"],\n",
    "                                                           data_cols    = numerical_features,\n",
    "                                                           station_name = \"TSN\",\n",
    "                                                           method       = \"machine_learning\",\n",
    "                                                           models       = dict({\n",
    "                                                                                \"IsolationForest\"         : IsolationForest(bootstrap     = False, \n",
    "                                                                                                                            contamination = \"auto\", \n",
    "                                                                                                                            max_features  = 1.0, \n",
    "                                                                                                                            max_samples   = \"auto\", \n",
    "                                                                                                                            n_estimators  = 100, \n",
    "                                                                                                                            n_jobs        = None, \n",
    "                                                                                                                            random_state  = None, \n",
    "                                                                                                                            verbose       = 0, \n",
    "                                                                                                                            warm_start    = False),\n",
    "                                                                                \"LocalOutlierFactor\"      : LocalOutlierFactor(algorithm     = \"auto\", \n",
    "                                                                                                                               contamination = \"auto\", \n",
    "                                                                                                                               leaf_size     = 30, \n",
    "                                                                                                                               metric        = \"minkowski\", \n",
    "                                                                                                                               metric_params = None, \n",
    "                                                                                                                               n_jobs        = None, \n",
    "                                                                                                                               n_neighbors   = 20, \n",
    "                                                                                                                               novelty       = False, \n",
    "                                                                                                                               p             = 2),\n",
    "                                                                                \"Prophet\"                 : Prophet(growth                  = \"linear\", # SLOW!!!\n",
    "                                                                                                                    changepoints            = None,\n",
    "                                                                                                                    n_changepoints          = 25,\n",
    "                                                                                                                    changepoint_range       = 0.8,\n",
    "                                                                                                                    yearly_seasonality      = \"auto\",\n",
    "                                                                                                                    weekly_seasonality      = \"auto\",\n",
    "                                                                                                                    daily_seasonality       = \"auto\",\n",
    "                                                                                                                    holidays                = None,\n",
    "                                                                                                                    seasonality_mode        = \"additive\",\n",
    "                                                                                                                    seasonality_prior_scale = 10.0,\n",
    "                                                                                                                    holidays_prior_scale    = 10.0,\n",
    "                                                                                                                    changepoint_prior_scale = 0.05,\n",
    "                                                                                                                    mcmc_samples            = 0,\n",
    "                                                                                                                    interval_width          = 0.80,\n",
    "                                                                                                                    uncertainty_samples     = 1000,\n",
    "                                                                                                                    stan_backend            = None),\n",
    "                                                                                \"AgglomerativeClustering\" : AgglomerativeClustering(compute_distances  = False,  # SLOW!!!\n",
    "                                                                                                                                    compute_full_tree  = 'auto', \n",
    "                                                                                                                                 #    connectivity       = None, # KHONG SUA\n",
    "                                                                                                                                    distance_threshold = None, \n",
    "                                                                                                                                 #    linkage            = 'ward', # KHONG SUA\n",
    "                                                                                                                                    memory             = None, \n",
    "                                                                                                                                    metric             = 'euclidean', \n",
    "                                                                                                                                    n_clusters         = 3),\n",
    "                                                                                \"HDBSCAN\"                  : HDBSCAN(min_cluster_size               = 5,\n",
    "                                                                                                                       min_samples                    = None,\n",
    "                                                                                                                       metric                         = 'euclidean',\n",
    "                                                                                                                       alpha                          = 1.0,\n",
    "                                                                                                                       p                              = None,\n",
    "                                                                                                                       algorithm                      = 'best',\n",
    "                                                                                                                       leaf_size                      = 40,\n",
    "                                                                                                                       memory                         = Memory(location=None),\n",
    "                                                                                                                       approx_min_span_tree           = True,\n",
    "                                                                                                                       gen_min_span_tree              = False,\n",
    "                                                                                                                       core_dist_n_jobs               = 1,\n",
    "                                                                                                                       cluster_selection_method       = 'eom',\n",
    "                                                                                                                       allow_single_cluster           = False,\n",
    "                                                                                                                       prediction_data                = False,\n",
    "                                                                                                                       match_reference_implementation = False),\n",
    "                                                                                \"VanillaAutoencoder\"      : \"Dell bt ghi gi o day de tuning @@@\"\n",
    "                                                                                }),\n",
    "                                                           factor       = 1.5,   # Stick with Prophet model\n",
    "                                                           window_size  = 10,    # Stick with Agglomerative Clustering & DBSCAN model\n",
    "                                                           dendrogram   = False, # Stick with Agglomerative Clustering model\n",
    "                                                           display      = True,\n",
    "                                                           start_time   = \"1990-01-01 00:00:00+07:00\",\n",
    "                                                           end_time     = \"2025-01-01 00:00:00+07:00\",\n",
    "                                                           freq         = \"1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import tensorflow as tf\n",
    "# pd.options.mode.chained_assignment = None\n",
    "# from matplotlib.pylab import rcParams\n",
    "# import importlib\n",
    "# importlib.reload(dataset)\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# sns.set_theme(style='whitegrid', palette='muted')\n",
    "# rcParams['figure.figsize'] = 14, 8\n",
    "# np.random.seed(1)\n",
    "# tf.random.set_seed(1)\n",
    "\n",
    "# df = station[\"TSN\"]\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feature in numerical_features:\n",
    "#     plt.figure(figsize=(14, 6))\n",
    "#     sns.lineplot(data=df, x='time', y=feature, label=feature)\n",
    "#     plt.xlabel('Time')\n",
    "#     plt.ylabel('Value')\n",
    "#     plt.legend()\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = int(len(df) * 0.8)\n",
    "# test_size = len(df) - train_size\n",
    "# train, test = df.iloc[0:train_size], df.iloc[train_size:len(df)]\n",
    "# print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_sp = dataset.LSTMAutoencoder()\n",
    "# detector_tcc = dataset.LSTMAutoencoder()\n",
    "# detector_tp = dataset.LSTMAutoencoder()\n",
    "# detector_u10 = dataset.LSTMAutoencoder()\n",
    "# detector_v10 = dataset.LSTMAutoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_sp.prepare_data(train, test, 'sp', 24)\n",
    "# detector_tcc.prepare_data(train, test, 'tcc', 24)\n",
    "# detector_tp.prepare_data(train, test, 'tp', 24)\n",
    "# detector_u10.prepare_data(train, test, 'u10', 24)\n",
    "# detector_v10.prepare_data(train, test, 'v10', 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_sp.train(epochs=20, batch_size=256, model_path='/TSN/sp')\n",
    "# detector_tcc.train(epochs=20, batch_size=256, model_path='/TSN/tcc')\n",
    "# detector_tp.train(epochs=20, batch_size=256, model_path='/TSN/tp')\n",
    "# detector_u10.train(epochs=20, batch_size=256, model_path='/TSN/u10')\n",
    "# detector_v10.train(epochs=20, batch_size=256, model_path='/TSN/v10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_data=df[['time', 'sp']]\n",
    "# detector_sp.detect_anomalies(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_sp.plot_anomaly_scores(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_sp.reconstruct_and_replace_anomalies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_sp.plot_reconstruct_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_data=df[['time', 'tcc']]\n",
    "# detector_tcc.detect_anomalies(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_tcc.plot_anomaly_scores(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_tcc.reconstruct_and_replace_anomalies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_tcc.plot_reconstruct_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_data=df[['time', 'tp']]\n",
    "# detector_tp.detect_anomalies(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_tp.plot_anomaly_scores(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_tp.reconstruct_and_replace_anomalies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_tp.plot_reconstruct_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_data=df[['time', 'u10']]\n",
    "# detector_u10.detect_anomalies(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_u10.plot_anomaly_scores(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_u10.reconstruct_and_replace_anomalies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_u10.plot_reconstruct_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_data=df[['time', 'v10']]\n",
    "# detector_v10.detect_anomalies(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_v10.plot_anomaly_scores(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_v10.reconstruct_and_replace_anomalies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector_v10.plot_reconstruct_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">5.2. Feature Engineering</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">T√≠nh h∆∞·ªõng gi√≥, v·∫≠n t·ªëc gi√≥ (g·ª≠i file Excel c√¥ t√≠nh cho nh√≥m)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in station.items():\n",
    "    # Convert data u10, v10 sang dang m/s\n",
    "    u_unit = df[\"u10\"].to_numpy() * units(\"m/s\")\n",
    "    v_unit = df[\"v10\"].to_numpy() * units(\"m/s\")\n",
    "    \n",
    "    # Tinh wind direction, huong gio thoi la from\n",
    "    wind_direction_deg = metpy.calc.wind_direction(u          = u_unit,\n",
    "                                                   v          = v_unit,\n",
    "                                                   convention = \"from\")\n",
    "    \n",
    "    # Tinh wind speed\n",
    "    wind_speed = metpy.calc.wind_speed(u = u_unit,\n",
    "                                       v = v_unit)\n",
    "    \n",
    "    # Chuyen data wind speed tu dang series sang dang dataframe\n",
    "    wind_speed         = pd.DataFrame(data = wind_speed,         columns = [\"wind_speed\"])\n",
    "    wind_direction_deg = pd.DataFrame(data = wind_direction_deg, columns = [\"wind_direction_deg\"])\n",
    "    \n",
    "    station[name][\"wind_speed\"]         = wind_speed\n",
    "    station[name][\"wind_direction_deg\"] = wind_direction_deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"CaMau\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Time-based features (hour, day, month, season‚Ä¶)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in station.items():\n",
    "    station[name] = features.extract_time_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"CaMau\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Arrange columns</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"CaMau\"].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_pos = list(['time', 'latitude', 'longitude', 'ymd', 'year', 'month', 'day',\n",
    "                 'number', 'step', 'surface', 'sp', 'tcc', 'tp', 'u10', 'v10', 'wind_speed', 'wind_direction_deg'])\n",
    "\n",
    "for name, df in station.items():\n",
    "    station[name] = df[cols_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"CaMau\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">5.3. Save data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[\"TSN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥\n",
    "os.makedirs(\"../data/processed/datasets\", exist_ok=True)\n",
    "\n",
    "# Duy·ªát t·ª´ng station v√† l∆∞u file\n",
    "for name, df in station.items():\n",
    "    file_name = f\"../data/processed/datasets/{name}90.24_cleaned.csv\"\n",
    "    df.to_csv(file_name, index=False)\n",
    "    print(f\"ƒê√£ l∆∞u: {file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
