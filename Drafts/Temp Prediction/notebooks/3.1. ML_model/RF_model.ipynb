{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style = \"color:orange\">1. Introduction<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">1.1. Project purpose</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xây dựng và đánh giá tập hợp các mô hình dự báo chuỗi thời gian để dự đoán nhiệt độ cực đại hàng ngày tại các khu vực đô thị và ven biển Việt Nam. Dự án áp dụng cả thuật toán Machine Learning truyền thống và Deep Learning hiện đại nhằm cải thiện độ chính xác dự báo so với các phương pháp thống kê thông thường.\n",
    "\n",
    "<b>Nguồn dữ liệu</b>: Bộ dữ liệu ERA5 (ECMWF) với các bản ghi nhiệt độ từ 1990 đến 2024, được xử lý và biến đổi để huấn luyện các mô hình như Random Forest, XGBoost, LSTM, Transformer, TFT, và N-BEATS.\n",
    "\n",
    "<b>Kết quả</b>: Đánh giá hiệu năng giữa các mô hình qua nhiều kịch bản thực nghiệm và đề xuất hệ thống cảnh báo nhiệt độ sớm ứng dụng thực tế."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">1.2. Data source and description</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Thông tin dữ liệu trong đề tài:</h4>\n",
    "\n",
    "<ul>\n",
    "<li><b>Thời gian thu thập:</b> từ năm <b>1990 đến 2024</b></li>\n",
    "<li><b>Định dạng ban đầu:</b> .grib, sau đó chuyển đổi sang .csv để xử lý</li>\n",
    "</ul>\n",
    "\n",
    "<h4>Các biến số chính trong tập dữ liệu:</h4>\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Tên cột dữ liệu</th>\n",
    "<th>Ý nghĩa</th>\n",
    "<th>Đơn vị đo</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td><code>NAME</code></td><td>Tên tỉnh/thành phố nơi thu thập dữ liệu</td><td>-</td></tr>\n",
    "<tr><td><code>LATITUDE</code></td><td>Vĩ độ địa lý của điểm đo</td><td>Độ</td></tr>\n",
    "<tr><td><code>LONGITUDE</code></td><td>Kinh độ địa lý của điểm đo</td><td>Độ</td></tr>\n",
    "<tr><td><code>YMD</code></td><td>Ngày/tháng/năm đo đạc</td><td>dd/mm/yyyy</td></tr>\n",
    "<tr><td><code>YEAR</code></td><td>Năm đo đạc</td><td>Năm</td></tr>\n",
    "<tr><td><code>MONTH</code></td><td>Tháng đo đạc</td><td>Tháng</td></tr>\n",
    "<tr><td><code>DAY</code></td><td>Ngày đo đạc</td><td>Ngày</td></tr>\n",
    "<tr><td><code>TEMP_max</code></td><td>Nhiệt độ không khí cực đại trong ngày</td><td>°C</td></tr>\n",
    "<tr><td><code>TEMP_ave</code></td><td>Nhiệt độ trung bình trong ngày</td><td>°C</td></tr>\n",
    "<tr><td><code>DEW_ave</code></td><td>Điểm sương trung bình trong ngày</td><td>°C</td></tr>\n",
    "<tr><td><code>DEW_max</code></td><td>Điểm sương cao nhất trong ngày</td><td>°C</td></tr>\n",
    "<tr><td><code>RH_ave</code></td><td>Độ ẩm tương đối trung bình trong ngày</td><td>%</td></tr>\n",
    "<tr><td><code>RH_max</code></td><td>Độ ẩm tương đối cực đại trong ngày</td><td>%</td></tr>\n",
    "<tr><td><code>AT_ave</code></td><td>Nhiệt độ cảm nhận trung bình trong ngày (Apparent Temp.)</td><td>°C</td></tr>\n",
    "<tr><td><code>AT_max</code></td><td>Nhiệt độ cảm nhận cao nhất trong ngày</td><td>°C</td></tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<p><b>Biến mục tiêu chính:</b></p>\n",
    "<ul>\n",
    "<li><code>TEMP_max</code> — Nhiệt độ không khí cực đại hàng ngày (°C)</li>\n",
    "</ul>\n",
    "\n",
    "<p><b>Lưu ý:</b> Dữ liệu gốc của ERA5 có thể chứa giá trị thiếu, giá trị ngoại lai và một số dị bản khí tượng đặc thù. Do đó, quá trình làm sạch dữ liệu, xử lý giá trị thiếu, phát hiện ngoại lệ và chuẩn hóa dữ liệu là các bước bắt buộc trước khi tiến hành huấn luyện và dự báo.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_type = {\n",
    "    'NAME'       : 'Categorical',        # Tên tỉnh/thành phố (chuỗi)\n",
    "    'LATITUDE'   : 'Numerical',          # Vĩ độ (°)\n",
    "    'LONGITUDE'  : 'Numerical',          # Kinh độ (°)\n",
    "    'YMD'        : 'Datetime',           # Ngày/tháng/năm (dd/mm/yyyy)\n",
    "    'YEAR'       : 'Numerical',          # Năm (năm)\n",
    "    'MONTH'      : 'Numerical',          # Tháng (1-12)\n",
    "    'DAY'        : 'Numerical',          # Ngày (1-31)\n",
    "\n",
    "    'TEMP_max'   : 'Numerical',          # Nhiệt độ cực đại trong ngày (°C)\n",
    "    'TEMP_ave'   : 'Numerical',          # Nhiệt độ trung bình trong ngày (°C)\n",
    "    'DEW_ave'    : 'Numerical',          # Điểm sương trung bình trong ngày (°C)\n",
    "    'DEW_max'    : 'Numerical',          # Điểm sương cực đại trong ngày (°C)\n",
    "    'RH_ave'     : 'Numerical',          # Độ ẩm tương đối trung bình trong ngày (%)\n",
    "    'RH_max'     : 'Numerical',          # Độ ẩm tương đối cực đại trong ngày (%)\n",
    "    'AT_ave'     : 'Numerical',          # Nhiệt độ cảm nhận trung bình trong ngày (°C)\n",
    "    'AT_max'     : 'Numerical'           # Nhiệt độ cảm nhận cực đại trong ngày (°C)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">1.3. Goals</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../image/Ảnh chụp màn hình 2025-06-24 210616.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">2. Import Libraries</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">2.1. Configuration and display settings</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")  # đường dẫn đến thư mục chứa src\n",
    "\n",
    "from src.utilities import(config, \n",
    "                          dataset, \n",
    "                          features, \n",
    "                          plots)\n",
    "\n",
    "from src.models import(anomaly_models,\n",
    "                       forecasting_models,\n",
    "                       model_utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">2.2. Required Python packages</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder  # Encode feature\n",
    "from sklearn.preprocessing import OrdinalEncoder # Encode feature\n",
    "from sklearn.preprocessing import MinMaxScaler   # Scale feature\n",
    "from sklearn.preprocessing import StandardScaler # Scale feature\n",
    "from sklearn.preprocessing import LabelEncoder   # Encode target\n",
    "# from scipy.stats import boxcox # Normalized feature\n",
    "\n",
    "# from sklearn.feature_selection import mutual_info_classif # PCA\n",
    "from sklearn.model_selection   import train_test_split\n",
    "from sklearn.model_selection   import RepeatedKFold\n",
    "from sklearn.model_selection   import GridSearchCV\n",
    "from sklearn.model_selection   import validation_curve\n",
    "from sklearn.model_selection   import learning_curve\n",
    "\n",
    "\n",
    "from sklearn.ensemble     import RandomForestRegressor\n",
    "from xgboost              import XGBRegressor\n",
    "\n",
    "# %pip install tensorflow\n",
    "from tensorflow                 import keras\n",
    "from tensorflow.keras           import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import joblib\n",
    "\n",
    "# Classification\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import matthews_corrcoef\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.metrics import roc_curve\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# Regression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "# from sklearn.metrics import median_absolute_error\n",
    "# from sklearn.metrics import max_error\n",
    "# from sklearn.metrics import PredictionErrorDisplay\n",
    "\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">3. Data Loading</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = dict({\"CaMau\" : \"../../data/processed/datasets/CaMau90.24_cleaned.csv\",\n",
    "            \"DH\"    : \"../../data/processed/datasets/DH90.24_cleaned.csv\",\n",
    "            \"NB\"    : \"../../data/processed/datasets/NB90.24_cleaned.csv\",\n",
    "            \"QN\"    : \"../../data/processed/datasets/QN90.24_cleaned.csv\",\n",
    "            \"TH\"    : \"../../data/processed/datasets/TH90.24_cleaned.csv\",\n",
    "            \"TSN\"   : \"../../data/processed/datasets/TSN90.24_cleaned.csv\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">3.1. Loading the dataset</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_name = \"CaMau\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filepath_or_buffer = src[station_name],\n",
    "                 parse_dates        = True,\n",
    "                 index_col          = \"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">3.2. Displaying first few rows</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">3.3. Data summary</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">4. Data Preparation</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">4.1. Feature Selection</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Train/Validation/Test</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theo thứ tự thời gian (không shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[['sp', 'tcc', 'tp', 'u10', 'v10', 'wind_speed']]\n",
    "targets  = df['wind_direction_deg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aproach1\n",
    "# le = LabelEncoder()\n",
    "# targets_encoded = le.fit_transform(targets)\n",
    "\n",
    "\n",
    "# # Aproach2\n",
    "# targets_encoded = targets_encoded.replace({'e': 0, 'p': 1})\n",
    "\n",
    "# # Aproach3\n",
    "targets_encoded = targets.replace({'e': 0, 'p': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # y_train\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">4.2. Dimensionality Reducing</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">PCA</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_scores = plots.make_mi_scores_regression(\n",
    "    X_data = features, \n",
    "    y_data = targets\n",
    ")\n",
    "\n",
    "print(mi_scores.head(20))\n",
    "# print(mi_scores.head(20))\n",
    "# print(mi_scores.tail(20))  # uncomment to see bottom 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_mi_scores(mi_scores)\n",
    "# script.plot_mi_scores(mi_scores.head(62))\n",
    "# script.plot_mi_scores(mi_scores.tail(38))  # uncomment to see bottom 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Lấy số lượng cột cần giữ lại (17 số cột có điểm MI cao nhất)\n",
    "# num_columns_to_keep = 17\n",
    "\n",
    "# # Lấy danh sách các cột cần giữ lại\n",
    "# columns_to_keep = mi_scores.nlargest(num_columns_to_keep).index\n",
    "\n",
    "# # Cập nhật X_train_encoded và X_test_encoded chỉ với các cột được giữ lại\n",
    "# features = features[columns_to_keep]\n",
    "\n",
    "# # Hiển thị các cột còn lại\n",
    "# print(f\"Các cột được giữ lại:\\n{features.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">4.3. Feature Engineering</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = features.select_dtypes(exclude='number').columns\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = features.select_dtypes(include='number').columns\n",
    "numerical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Encode categorical feature</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[categorical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [match_type[col] for col in categorical_features]\n",
    "# encoder = OrdinalEncoder(\n",
    "#     categories = categories,\n",
    "#     handle_unknown = 'use_encoded_value', \n",
    "#     unknown_value  = -1\n",
    "# )\n",
    "encoder = OneHotEncoder(categories     = categories,\n",
    "                        sparse_output  = False,\n",
    "                        handle_unknown = \"ignore\")\n",
    "\n",
    "features_encoded  = deepcopy(features)\n",
    "\n",
    "# # Encode\n",
    "# features_cat_encoded  = encoder.fit_transform(features[categorical_features])\n",
    "\n",
    "# Kiểm tra số cột\n",
    "# print(\"Số cột của dữ liệu đã mã hóa: \", features_cat_encoded.shape[1])\n",
    "\n",
    "# Loại bỏ các cột trong `categorical` khỏi features_encoded\n",
    "features_encoded  = features_encoded.drop(columns=categorical_features)\n",
    "\n",
    "# Thêm các cột đã mã hóa vào features_encoded\n",
    "# encoded_columns = encoder.get_feature_names_out(categorical_features)\n",
    "\n",
    "# Tạo DataFrame từ các cột đã mã hóa và gán lại tên cột\n",
    "# features_encoded  = pd.DataFrame(features_cat_encoded, columns=encoded_columns, index=features.index)\n",
    "\n",
    "# Kết hợp lại với các cột còn lại trong features_encoded\n",
    "# features_encoded  = pd.concat([features_encoded, features.drop(columns=categorical_features)], axis=1)\n",
    "features_encoded  = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">4.4. Feature Scaling</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_encoded[numerical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification\n",
    "# sns.histplot(\n",
    "#     data = features_encoded[numerical_features],\n",
    "#     kde  = True,\n",
    "# )\n",
    "\n",
    "# Regression\n",
    "# for col in numerical_features:\n",
    "#     plt.figure(figsize=(8, 4))\n",
    "#     sns.histplot(\n",
    "#         data = features_encoded[col],\n",
    "#         kde  = True,\n",
    "#         # bins = 50\n",
    "#     )\n",
    "#     plt.title(f'Distribution of {col}')\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "\n",
    "plots.plot_Outlier(data      = features_encoded,\n",
    "                   data_cols = numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale\n",
    "# scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# features_scale = scaler.fit_transform(features_encoded[s])\n",
    "\n",
    "# features_encoded[numerical_features] = features_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_encoded[numerical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification\n",
    "# sns.histplot(\n",
    "#     data = features_encoded[numerical_features],\n",
    "#     kde  = True,\n",
    "# )\n",
    "\n",
    "# Regression\n",
    "# for col in numerical_features:\n",
    "#     plt.figure(figsize=(8, 4))\n",
    "#     sns.histplot(\n",
    "#         data = features_encoded[col],\n",
    "#         kde  = True,\n",
    "#         # bins = 50\n",
    "#     )\n",
    "#     plt.title(f'Distribution of {col}')\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "\n",
    "plots.plot_Outlier(data      = features_encoded,\n",
    "                   data_cols = numerical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">4.5. Save data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check so dong\n",
    "print(features_encoded.shape)\n",
    "print(targets_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index đồng bộ\n",
    "df_reset            = df.reset_index()\n",
    "features_encoded    = features_encoded.reset_index()\n",
    "targets_df          = pd.DataFrame(targets_encoded, columns=[\"wind_direction_deg\"]).reset_index()\n",
    "\n",
    "# Lấy những cột không trùng để ghép\n",
    "features_only_cols = [col for col in features_encoded.columns if col not in df_reset.columns]\n",
    "features_filtered  = features_encoded[features_only_cols]\n",
    "\n",
    "# Ghép df + features_filtered\n",
    "df_features = pd.concat([df_reset, features_filtered], axis=1)\n",
    "\n",
    "# Ghép tiếp target vào cuối\n",
    "final_df = pd.concat([df_features, targets_df], axis=1)\n",
    "\n",
    "# Tạo thư mục nếu chưa có\n",
    "os.makedirs(\"../../data/processed/datasets_encoded/\", exist_ok=True)\n",
    "file_name = f\"../../data/processed/datasets_encoded/{station_name}90.24_encoded.csv\"\n",
    "\n",
    "# Xuất file CSV đúng thứ tự\n",
    "final_df.to_csv(file_name, index=False)\n",
    "print(f\"Đã lưu: {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">5.Model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_encoded = dict({\"CaMau\" : \"../../data/processed/datasets_encoded/CaMau90.24_encoded.csv\",\n",
    "                    \"NB\"    : \"../../data/processed/datasets_encoded/NB90.24_encoded.csv\",\n",
    "                    \"QN\"    : \"../../data/processed/datasets_encoded/QN90.24_encoded.csv\",\n",
    "                    \"TSN\"   : \"../../data/processed/datasets_encoded/TSN90.24_encoded.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filepath_or_buffer = src_encoded[station_name],\n",
    "                 parse_dates        = True,\n",
    "                 index_col          = \"time\")\n",
    "targets_encoded  = df[\"wind_direction_deg\"]\n",
    "features_encoded = df.drop(columns = [\"wind_direction_deg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">5.1. Building models</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Building Models ~ aproach1</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Building Models ~ aproach2</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     \"n_estimators\"      : [100, 300, 500],        # nhiều cây giúp giảm phương sai\n",
    "#     \"max_depth\"         : [10, 20, 30, 50, None], # để kiểm tra độ sâu và tránh overfitting\n",
    "#     \"min_samples_split\" : [2, 5, 10],             # số mẫu tối thiểu để split node\n",
    "#     \"min_samples_leaf\"  : [1, 2, 4, 8],           # mẫu tối thiểu tại mỗi lá\n",
    "#     # \"max_features\"      : [\"sqrt\", \"log2\"],       # vẫn giữ để dùng GridSearchCV, nhưng Validation Curve thì bỏ qua\n",
    "#     \"bootstrap\"         : [True, False],          # kiểm tra dùng bagging hay không\n",
    "#     \"max_samples\"       : [0.5, 0.75, 1.0]        # (mới từ sklearn 0.22) — kiểm tra nếu bootstrap=True thì lấy bao nhiêu % dữ liệu\n",
    "# }\n",
    "\n",
    "\n",
    "# train_model.plot_VC(\n",
    "#     X_data     = features_encoded,\n",
    "#     Y_data     = targets_encoded,\n",
    "#     model      = RandomForestRegressor(),\n",
    "#     param_grid = param_grid,\n",
    "#     ylim       = [0.01,1.20],\n",
    "#     n_jobs     = 4 # Quan trong: value o day quyet dinh se su dung bao nhieu processor de chay ~> cang cao cang tot\n",
    "# ) # 0.9689 ~ 0.002335721469090121 ~ np.logspace(-10, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg_model = RandomForestRegressor(\n",
    "#     n_estimators       = ,\n",
    "#     max_depth          = ,\n",
    "#     min_samples_split  = ,\n",
    "#     min_samples_leaf   = ,\n",
    "#     max_features       = ,\n",
    "#     bootstrap          = \n",
    "# ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu model\n",
    "joblib.dump(reg_model, '../../models/trained_models/RF_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Building Models ~ aproach3</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     # \"alpha\": np.logspace(-3, 1, 20),         # Multinomial, Bernoulli\n",
    "#     \"var_smoothing\": np.logspace(-9, -5, 10) # Gaussian\n",
    "# }\n",
    "\n",
    "# reg_model = script.FindBestTuningModel(\n",
    "#     model      = GaussianNB(),\n",
    "#     param_grid = param_grid,\n",
    "#     train_X    = features_encoded,\n",
    "#     train_y    = targets_encoded\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Building Models ~ aproach4</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">5.2. Training models</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded = features_encoded[(features_encoded[\"year\"] >= 1990) & (features_encoded[\"year\"] < 2022)]\n",
    "X_test_encoded  = features_encoded[(features_encoded[\"year\"] >= 2022) & (features_encoded[\"year\"] <= 2024)]\n",
    "\n",
    "y_train_encoded = targets_encoded[(features_encoded[\"year\"] >= 1990) & (features_encoded[\"year\"] < 2022)].reset_index(drop=True)\n",
    "y_test_encoded  = targets_encoded[(features_encoded[\"year\"] >= 2022) & (features_encoded[\"year\"] <= 2024)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Training Models ~ aproach1</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_encoded, X_test_encoded, y_train_encoded, y_test_encoded = train_test_split(features_encoded, targets_encoded, test_size = 0.2, random_state=40024090)\n",
    "# best_nb_model.fit(X_train_encoded, y_train_encoded)\n",
    "# y_pred = best_nb_model.predict(X_test_encoded)\n",
    "\n",
    "reg_model.fit(X_train_encoded[features.columns], y_train_encoded)\n",
    "\n",
    "y_fit  = pd.DataFrame(data    = reg_model.predict(X_train_encoded[features.columns]), \n",
    "                      index   = y_train_encoded.index, \n",
    "                      columns = ['wind_direction_deg'])\n",
    "y_pred = pd.DataFrame(data    = reg_model.predict(X_test_encoded[features.columns]), \n",
    "                      index   = y_test_encoded.index, \n",
    "                      columns = ['wind_direction_deg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Training Models ~ aproach2</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script.plot_LC(\n",
    "#     X_data       = features_encoded,\n",
    "#     Y_data       = targets_encoded,\n",
    "#     model        = reg_model,\n",
    "#     train_sizes  = np.linspace(0.2, 1, 20),\n",
    "#     random_state = 40020409,\n",
    "#     n_jobs       = 4 # Quan trong: value o day quyet dinh se su dung bao nhieu processor de chay ~> cang cao cang tot\n",
    "# ) # 0.9693 ~ 0.4968 ~ np.linspace(0.2, 1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_encoded, X_test_encoded, y_train_encoded, y_test_encoded = train_test_split(\n",
    "#     features_encoded, targets_encoded, \n",
    "#     train_size   = 0.4968,\n",
    "#     random_state = 40020409\n",
    "# )\n",
    "# reg_model.fit(X_train_encoded, y_train_encoded)\n",
    "# y_pred = reg_model.predict(X_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">5.3. Save models</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu model\n",
    "joblib.dump(reg_model, '../../models/trained_models/RF_trained.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">6.Model Evaluation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">6.1. Load model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lại model\n",
    "reg_model = joblib.load('../../models/trained_models/RF_trained.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">6.2. Evaluation Metrics</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">r2_score</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_true      = y_train_encoded,\n",
    "         y_pred      = y_fit,\n",
    "         multioutput = \"uniform_average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">mean_absolute_error</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_true      = y_train_encoded,\n",
    "                    y_pred      = y_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">mean_squared_error</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_true      = y_train_encoded,\n",
    "                   y_pred      = y_fit,\n",
    "                   multioutput = \"uniform_average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">mean_squared_log_error</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_log_error(y_true      = y_train_encoded,\n",
    "                    y_pred      = y_fit,\n",
    "                    multioutput = \"uniform_average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">mean_absolute_percentage_error</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_true      = y_train_encoded,\n",
    "                               y_pred      = y_fit,\n",
    "                               multioutput = \"uniform_average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">6.3. Visualizing Evaluation results</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Training Deviance</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Feature Importance</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://127.0.0.1:55658/scikit-learn-1_5/auto_examples/ensemble/plot_gradient_boosting_regression.html\">Gradient Boosting regression</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, X_train, X_test, y_test, n_repeats=10, scoring=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    from packaging.version import parse as parse_version\n",
    "    import matplotlib\n",
    "\n",
    "    if hasattr(model, \"estimators_\"):  # MultiOutput\n",
    "        estimators = model.estimators_\n",
    "    else:\n",
    "        estimators = [model]\n",
    "\n",
    "    for i, estimator in enumerate(estimators):\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "        # MDI Importance (tree attribute)\n",
    "        if hasattr(estimator, \"feature_importances_\"):\n",
    "            mdi_importances = estimator.feature_importances_\n",
    "            feature_names = X_train.columns\n",
    "            assert len(mdi_importances) == len(feature_names), f\"Số feature không khớp ở estimator {i}\"\n",
    "            sorted_idx = np.argsort(mdi_importances)\n",
    "            pos = np.arange(len(feature_names)) + 0.5\n",
    "\n",
    "            axes[0].barh(pos, mdi_importances[sorted_idx], align=\"center\")\n",
    "            axes[0].set_yticks(pos)\n",
    "            axes[0].set_yticklabels(np.array(feature_names)[sorted_idx])\n",
    "            axes[0].set_title(f\"Feature Importance (MDI) - Target {i}\")\n",
    "        else:\n",
    "            axes[0].text(0.5, 0.5, 'Model không hỗ trợ MDI importance', ha='center')\n",
    "            axes[0].axis('off')\n",
    "\n",
    "        # Permutation Importance (test set)\n",
    "        result = permutation_importance(\n",
    "            estimator, X_test, y_test.iloc[:, i] if y_test.ndim == 2 else y_test,\n",
    "            n_repeats=n_repeats, random_state=42, n_jobs=-1, scoring=scoring\n",
    "        )\n",
    "\n",
    "        sorted_idx_perm = result.importances_mean.argsort()\n",
    "        tick_labels_parameter_name = (\n",
    "            \"tick_labels\"\n",
    "            if parse_version(matplotlib.__version__) >= parse_version(\"3.9\")\n",
    "            else \"labels\"\n",
    "        )\n",
    "        tick_labels_dict = {\n",
    "            tick_labels_parameter_name: np.array(X_train.columns)[sorted_idx_perm]\n",
    "        }\n",
    "\n",
    "        axes[1].boxplot(result.importances[sorted_idx_perm].T, vert=False, **tick_labels_dict)\n",
    "        axes[1].set_title(f\"Permutation Importance (test set) - Target {i}\")\n",
    "\n",
    "        fig.tight_layout()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "plot_feature_importance(\n",
    "    model         = reg_model,\n",
    "    X_train       = X_train_encoded[features.columns],\n",
    "    X_test        = X_test_encoded[features.columns],\n",
    "    y_test        = y_test_encoded,\n",
    "    n_repeats     = 10,\n",
    "    scoring       = None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Confusion matrix</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix is a tool for summarizing the performance of a classification algorithm. A confusion matrix will give us a clear picture of classification model performance and the types of errors produced by the model. It gives us a summary of correct and incorrect predictions broken down by each category. The summary is represented in a tabular form.\n",
    "\n",
    "Four types of outcomes are possible while evaluating a classification model performance. These four outcomes are described below:-\n",
    "\n",
    "True Positives (TP) – True Positives occur when we predict an observation belongs to a certain class and the observation actually belongs to that class.\n",
    "\n",
    "True Negatives (TN) – True Negatives occur when we predict an observation does not belong to a certain class and the observation actually does not belong to that class.\n",
    "\n",
    "False Positives (FP) – False Positives occur when we predict an observation belongs to a certain class but the observation actually does not belong to that class. This type of error is called Type I error.\n",
    "\n",
    "False Negatives (FN) – False Negatives occur when we predict an observation does not belong to a certain class but the observation actually belongs to that class. This is a very serious error and it is called Type II error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots.plot_CF_aproach1(y_test_encoded, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script.plot_CF_aproach2(y_test_encoded, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Classification metrices</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Report\n",
    "\n",
    "Classification report is another way to evaluate the classification model performance. It displays the precision, recall, f1 and support scores for the model. I have described these terms in later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_test_encoded, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:tomato\">Class probabilities</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred1 = best_nb_model.predict_proba(X_test_encoded)[:, 1]\n",
    "\n",
    "# # Vẽ histogram + kde\n",
    "# sns_hist = sns.histplot(\n",
    "#     y_pred1, \n",
    "#     bins=10, \n",
    "#     kde=True, \n",
    "#     edgecolor='black', \n",
    "#     alpha=0.7\n",
    "# )\n",
    "\n",
    "# # Lấy các thông tin cần từ histogram\n",
    "# counts, edges = np.histogram(y_pred1, bins=10)\n",
    "# total = len(y_pred1)\n",
    "\n",
    "# # Thêm số phần trăm lên từng cột\n",
    "# for i in range(len(counts)):\n",
    "#     bin_center = (edges[i] + edges[i+1]) / 2\n",
    "#     height = counts[i]\n",
    "#     plt.text(bin_center, height + height*0.01, \n",
    "#              f'{(counts[i]/total*100):.1f}%', \n",
    "#              ha='center', fontsize=9)\n",
    "\n",
    "# plt.title('Histogram of predicted probabilities with KDE')\n",
    "# plt.xlim(0, 1)\n",
    "# plt.xlabel('Predicted probability (class=1)')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.grid(True, axis='y')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">6.4. Interpretation of performance</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Logistic Regression\n",
    "# # lr_param_grid = {\n",
    "# #     'C': np.logspace(-4, 4, 10),    # regularization strength\n",
    "# #     'solver': ['lbfgs', 'liblinear']\n",
    "# # }\n",
    "# # best_lr_model = script.FindBestModel(\n",
    "# #     LogisticRegression(max_iter=1000),\n",
    "# #     lr_param_grid,\n",
    "# #     X_train_encoded, y_train_encoded\n",
    "# # )\n",
    "\n",
    "# # # Decision Tree\n",
    "# # dt_param_grid = {\n",
    "# #     'max_depth': [3, 5, 7, 10, None],\n",
    "# #     'min_samples_split': [2, 5, 10],\n",
    "# #     'criterion': ['gini', 'entropy']\n",
    "# # }\n",
    "# # best_dt_model = script.FindBestModel(\n",
    "# #     DecisionTreeClassifier(random_state=42),\n",
    "# #     dt_param_grid,\n",
    "# #     X_train_encoded, y_train_encoded\n",
    "# # )\n",
    "\n",
    "# # # Random Forest\n",
    "# # rf_param_grid = {\n",
    "# #     'n_estimators': [100, 300, 500],\n",
    "# #     'max_features': ['sqrt', 'log2', 5],\n",
    "# #     'max_depth': [None, 5, 10],\n",
    "# #     'min_samples_split': [2, 5, 10]\n",
    "# # }\n",
    "# # best_rf_model = script.FindBestModel(\n",
    "# #     RandomForestClassifier(random_state=42),\n",
    "# #     rf_param_grid,\n",
    "# #     X_train_encoded, y_train_encoded\n",
    "# # )\n",
    "\n",
    "# # KNN\n",
    "# # knn_param_grid = {\n",
    "# #     'n_neighbors': [3, 5, 7, 9, 11],\n",
    "# #     'weights': ['uniform', 'distance'],\n",
    "# #     'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "# # }\n",
    "# # best_knn_model = script.FindBestTuningModel(\n",
    "# #     KNeighborsClassifier(),\n",
    "# #     knn_param_grid,\n",
    "# #     X_train_encoded, y_train_encoded\n",
    "# # )\n",
    "\n",
    "# # # SVM\n",
    "# # svm_param_grid = {\n",
    "# #     'C': np.logspace(-3, 3, 7),\n",
    "# #     'kernel': ['linear', 'rbf'],\n",
    "# #     'gamma': ['scale', 'auto']\n",
    "# # }\n",
    "# # best_svm_model = script.FindBestModel(\n",
    "# #     SVC(probability=True, random_state=42),\n",
    "# #     svm_param_grid,\n",
    "# #     X_train_encoded, y_train_encoded\n",
    "# # )\n",
    "\n",
    "# # # XGBoost\n",
    "# # xgb_param_grid = {\n",
    "# #     'n_estimators': [100, 300, 500],\n",
    "# #     'max_depth': [3, 5, 7],\n",
    "# #     'learning_rate': [0.01, 0.1, 0.2],\n",
    "# #     'subsample': [0.6, 0.8, 1.0],\n",
    "# #     'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "# # }\n",
    "# # best_xgb_model = script.FindBestModel(\n",
    "# #     XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "# #     xgb_param_grid,\n",
    "# #     X_train_encoded, y_train_encoded\n",
    "# # )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Logistic Regression\n",
    "# best_lr_model = LogisticRegression(C=1.0, solver='lbfgs', max_iter=1000)\n",
    "# best_lr_model.fit(X_train_encoded, y_train_encoded)\n",
    "\n",
    "# # Decision Tree\n",
    "# best_dt_model = DecisionTreeClassifier(max_depth=5, min_samples_split=2, criterion='gini', random_state=42)\n",
    "# best_dt_model.fit(X_train_encoded, y_train_encoded)\n",
    "\n",
    "# # Random Forest\n",
    "# best_rf_model = RandomForestClassifier(n_estimators=100, max_features='sqrt', max_depth=5, min_samples_split=2, random_state=42)\n",
    "# best_rf_model.fit(X_train_encoded, y_train_encoded)\n",
    "\n",
    "# # KNN\n",
    "# # best_knn_model = KNeighborsClassifier(n_neighbors=5, weights='uniform', metric='euclidean')\n",
    "# # best_knn_model.fit(X_train_encoded, y_train_encoded)\n",
    "\n",
    "# # # SVM\n",
    "# # best_svm_model = SVC(C=1.0, kernel='rbf', gamma='scale', probability=True, random_state=42)\n",
    "# # best_svm_model.fit(X_train_encoded, y_train_encoded)\n",
    "\n",
    "# # # XGBoost\n",
    "# # best_xgb_model = XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "# # best_xgb_model.fit(X_train_encoded, y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Logistic Regression : Training set score = {:.4f}'.format(best_lr_model.score(X_train_encoded, y_train_encoded)))\n",
    "# print('                      Test set score     = {:.4f}'.format(best_lr_model.score(X_test_encoded, y_test_encoded)))\n",
    "\n",
    "# print('Decision Tree        : Training set score = {:.4f}'.format(best_dt_model.score(X_train_encoded, y_train_encoded)))\n",
    "# print('                      Test set score     = {:.4f}'.format(best_dt_model.score(X_test_encoded, y_test_encoded)))\n",
    "\n",
    "# print('Random Forest        : Training set score = {:.4f}'.format(best_rf_model.score(X_train_encoded, y_train_encoded)))\n",
    "# print('                      Test set score     = {:.4f}'.format(best_rf_model.score(X_test_encoded, y_test_encoded)))\n",
    "\n",
    "# # print('K-Nearest Neighbors  : Training set score = {:.4f}'.format(best_knn_model.score(X_train_encoded, y_train_encoded)))\n",
    "# # print('                      Test set score     = {:.4f}'.format(best_knn_model.score(X_test_encoded, y_test_encoded)))\n",
    "\n",
    "# # print('Support Vector Machine : Training set score = {:.4f}'.format(best_svm_model.score(X_train_encoded, y_train_encoded)))\n",
    "# # print('                        Test set score     = {:.4f}'.format(best_svm_model.score(X_test_encoded, y_test_encoded)))\n",
    "\n",
    "# # print('XGBoost               : Training set score = {:.4f}'.format(best_xgb_model.score(X_train_encoded, y_train_encoded)))\n",
    "# # print('                        Test set score     = {:.4f}'.format(best_xgb_model.score(X_test_encoded, y_test_encoded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_probs  = best_lr_model.predict_proba(X_test_encoded)[:, 1]\n",
    "# dt_probs  = best_dt_model.predict_proba(X_test_encoded)[:, 1]\n",
    "# rf_probs  = best_rf_model.predict_proba(X_test_encoded)[:, 1]\n",
    "# # knn_probs = best_knn_model.predict_proba(X_test_encoded)[:, 1]\n",
    "# # svm_probs = best_svm_model.predict_proba(X_test_encoded)[:, 1]\n",
    "# # xgb_probs = best_xgb_model.predict_proba(X_test_encoded)[:, 1]\n",
    "\n",
    "# y_pred1 = best_nb_model.predict_proba(X_test_encoded)[:, 1]\n",
    "\n",
    "# nb_auc = roc_auc_score(y_test_encoded, y_pred1)\n",
    "# lr_auc = roc_auc_score(y_test_encoded, lr_probs)\n",
    "# dt_auc  = roc_auc_score(y_test_encoded, dt_probs)\n",
    "# rf_auc = roc_auc_score(y_test_encoded, rf_probs)\n",
    "# # knn_auc = roc_auc_score(y_test_encoded, knn_probs)\n",
    "# # svm_auc = roc_auc_score(y_test_encoded, svm_probs)\n",
    "# # xgb_auc = roc_auc_score(y_test_encoded, xgb_probs)\n",
    "# print(f'Naive Bayes         : AUROC = {nb_auc:.3f}')\n",
    "# print(f'Logistic Regression : AUROC = {lr_auc:.3f}')\n",
    "# print(f'Decision Tree       : AUROC = {dt_auc:.3f}')\n",
    "# print(f'Random Forest       : AUROC = {rf_auc:.3f}')\n",
    "# # print(f'K-Nearest Neighbors : AUROC = {knn_auc:.3f}')\n",
    "# # print(f'SVM                 : AUROC = {svm_auc:.3f}')\n",
    "# # print(f'XGBoost             : AUROC = {xgb_auc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_fpr, nb_tpr, _ = roc_curve(y_test_encoded, y_pred1)\n",
    "# lr_fpr, lr_tpr, _ = roc_curve(y_test_encoded, lr_probs)\n",
    "# fpr_dt, tpr_dt, _ = roc_curve(y_test_encoded, dt_probs)\n",
    "# rf_fpr, rf_tpr, _ = roc_curve(y_test_encoded, rf_probs)\n",
    "# # fpr_knn, tpr_knn, _ = roc_curve(y_test_encoded, knn_probs)\n",
    "# # fpr_svm, tpr_svm, _ = roc_curve(y_test_encoded, svm_probs)\n",
    "# # fpr_xgb, tpr_xgb, _ = roc_curve(y_test_encoded, xgb_probs)\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# plt.plot(nb_fpr, nb_tpr,   label='Naive Bayes        (AUC = %0.4f)' % nb_auc)\n",
    "# plt.plot(lr_fpr, lr_tpr,   label='LogisticRegression (AUC = %0.4f)' % lr_auc)\n",
    "# plt.plot(fpr_dt, tpr_dt,   label=f'Decision Tree     (AUC={dt_auc:.4f})')\n",
    "# plt.plot(rf_fpr, rf_tpr,   label=f'Random Forest     (AUC = %0.4f)' % rf_auc)\n",
    "# # plt.plot(fpr_knn, tpr_knn, label=f'KNN               (AUC={knn_auc:.4f})')\n",
    "# # plt.plot(fpr_svm, tpr_svm, label=f'SVM               (AUC={svm_auc:.4f})')\n",
    "# # plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost           (AUC={xgb_auc:.4f})')\n",
    "# plt.plot([0, 1], [0, 1],   label='Random Guess       (AUC=0.500)')\n",
    "\n",
    "# plt.title('ROC Curve Comparison')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.grid(True)\n",
    "# plt.legend()  \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred1 = best_rf_model.predict_proba(X_test_encoded)[:, 1]\n",
    "\n",
    "# # Vẽ histogram + kde\n",
    "# sns_hist = sns.histplot(\n",
    "#     y_pred1, \n",
    "#     bins=10, \n",
    "#     kde=True, \n",
    "#     edgecolor='black', \n",
    "#     alpha=0.7\n",
    "# )\n",
    "\n",
    "# # Lấy các thông tin cần từ histogram\n",
    "# counts, edges = np.histogram(y_pred1, bins=10)\n",
    "# total = len(y_pred1)\n",
    "\n",
    "# # Thêm số phần trăm lên từng cột\n",
    "# for i in range(len(counts)):\n",
    "#     bin_center = (edges[i] + edges[i+1]) / 2\n",
    "#     height = counts[i]\n",
    "#     plt.text(bin_center, height + height*0.01, \n",
    "#              f'{(counts[i]/total*100):.1f}%', \n",
    "#              ha='center', fontsize=9)\n",
    "\n",
    "# plt.title('Histogram of predicted probabilities with KDE')\n",
    "# plt.xlim(0, 1)\n",
    "# plt.xlabel('Predicted probability (class=1)')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.grid(True, axis='y')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">7. Conclusion</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Deepcopy test_df cho tập validation\n",
    "# X_val_encoded = deepcopy(test_df)\n",
    "\n",
    "# # Sắp xếp đúng thứ tự cột gốc trước khi encode (có thể không cần nếu test_df đã đúng thứ tự)\n",
    "# X_val_encoded = X_val_encoded[columns_to_keep]\n",
    "\n",
    "# # Encode tập validation\n",
    "# X_val_cat_encoded = encoder.transform(X_val_encoded[categorical_feature])\n",
    "\n",
    "# # Tạo DataFrame mới từ cột mã hóa\n",
    "# encoded_columns = encoder.get_feature_names_out(categorical_feature)\n",
    "# X_val_cat_encoded_df = pd.DataFrame(X_val_cat_encoded, columns=encoded_columns, index=X_val_encoded.index)\n",
    "\n",
    "# # Loại bỏ cột categorical khỏi X_val_encoded\n",
    "# X_val_encoded = X_val_encoded.drop(columns=categorical_feature)\n",
    "\n",
    "# # Nối lại cột encode vào\n",
    "# X_val_encoded = pd.concat([X_val_encoded, X_val_cat_encoded_df], axis=1)\n",
    "\n",
    "# # Sắp xếp lại thứ tự cột theo đúng feature_encoded\n",
    "# X_val_encoded = X_val_encoded[features_encoded.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure X_val_encoded contains only the columns used during training\n",
    "# # X_val_encoded = X_val_encoded[columns_to_keep]\n",
    "\n",
    "# # Predict the target variable\n",
    "# y_val_predict = model.predict(X_val_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred2 = best_nb_model.predict_proba(X_val_encoded)[:, 1]\n",
    "\n",
    "# # Vẽ histogram + kde\n",
    "# sns_hist = sns.histplot(\n",
    "#     y_pred2, \n",
    "#     bins=10, \n",
    "#     kde=True, \n",
    "#     edgecolor='black', \n",
    "#     alpha=0.7\n",
    "# )\n",
    "\n",
    "# # Lấy các thông tin cần từ histogram\n",
    "# counts, edges = np.histogram(y_pred2, bins=10)\n",
    "# total = len(y_pred2)\n",
    "\n",
    "# # Thêm số phần trăm lên từng cột\n",
    "# for i in range(len(counts)):\n",
    "#     bin_center = (edges[i] + edges[i+1]) / 2\n",
    "#     height = counts[i]\n",
    "#     plt.text(bin_center, height + height*0.01, \n",
    "#              f'{(counts[i]/total*100):.1f}%', \n",
    "#              ha='center', fontsize=9)\n",
    "\n",
    "# plt.title('Histogram of predicted probabilities with KDE')\n",
    "# plt.xlim(0, 1)\n",
    "# plt.xlabel('Predicted probability (class=1)')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.grid(True, axis='y')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
