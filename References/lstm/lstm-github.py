from __future__ import print_function
import numpy             as np
import pandas            as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing   import MinMaxScaler
from sklearn.metrics         import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, r2_score
from keras.models            import Sequential
from keras.layers            import Dense, Dropout, LSTM, Input
from keras.callbacks         import ModelCheckpoint, CSVLogger

# ƒê·ªçc d·ªØ li·ªáu t·ª´ file stock/AMZN.csv
df          = pd.read_csv('stock/AMZN.csv')
df['Date']  = pd.to_datetime(df['Date'], format="%Y-%m-%d %H:%M:%S%z")
df          = df.sort_values(by='Date')  # S·∫Øp x·∫øp theo c·ªôt Date tƒÉng d·∫ßn
df['Close'] = df['Close'].astype(float)

# Chu·∫©n b·ªã d·ªØ li·ªáu: ch·ªâ s·ª≠ d·ª•ng c·ªôt Close
df1       = pd.DataFrame(df, columns=['Date', 'Close'])
df1.index = df1['Date']
df1.drop('Date', axis=1, inplace=True)

data       = df1.values
train_size = int(len(data) * 0.8)
train_data = data[:train_size]
test_data  = data[train_size:]

# Chu·∫©n h√≥a d·ªØ li·ªáu v·ªõi MinMaxScaler
scaler   = MinMaxScaler(feature_range=(0, 1))
sc_train = scaler.fit_transform(train_data)
sc_test  = scaler.transform(test_data)

# T·∫°o d·ªØ li·ªáu hu·∫•n luy·ªán v·ªõi window size 10
window_size = 10
X_train, y_train = [], []
for i in range(window_size, len(train_data)):
    X_train.append(sc_train[i - window_size:i, 0])
    y_train.append(sc_train[i, 0])

X_train = np.array(X_train)
y_train = np.array(y_train)

# Reshape d·ªØ li·ªáu cho LSTM [samples, time steps, features]
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
y_train = np.reshape(y_train, (y_train.shape[0], 1))

# T·∫°o d·ªØ li·ªáu ki·ªÉm tra
X_test, y_test = [], []
for i in range(window_size, len(test_data)):
    X_test.append(sc_test[i - window_size:i, 0])
    y_test.append(sc_test[i, 0])

X_test = np.array(X_test)
y_test = np.array(y_test)
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
y_test = np.reshape(y_test, (y_test.shape[0], 1))

# X√¢y d·ª±ng m√¥ h√¨nh LSTM v·ªõi Input layer
model = Sequential()
model.add(Input(shape=(window_size, 1)))
model.add(LSTM(4))
model.add(Dropout(0.1))
model.add(Dense(1, activation='linear'))  # H·ªìi quy n√™n d√πng activation linear

# Bi√™n d·ªãch m√¥ h√¨nh
model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])

# ƒê·ªãnh nghƒ©a callbacks
checkpointer = ModelCheckpoint(filepath       = "logs/lstm/checkpoint-{epoch:02d}.keras",
                               verbose        = 1,
                               save_best_only = True,
                               monitor        = 'val_loss',
                               mode           = 'min')
csv_logger   = CSVLogger('logs/lstm/training_set_iranalysis.csv', 
                         separator = ',', 
                         append    = False)
 
# Hu·∫•n luy·ªán m√¥ h√¨nh
batch_size = 32  # ƒê·∫∑t gi√° tr·ªã m·∫∑c ƒë·ªãnh cho batch_size
model.fit(X_train, y_train,
          batch_size      = batch_size,
          epochs          = 200,
          validation_data = (X_test, y_test),
          callbacks       = [checkpointer, csv_logger])

# L∆∞u m√¥ h√¨nh
model.save("logs/lstm/lstm1layer_model.keras")

# D·ª± ƒëo√°n
y_train_predict = model.predict(X_train)
y_test_predict  = model.predict(X_test)


# T√≠nh ch·ªâ s·ªë tr√™n d·ªØ li·ªáu chu·∫©n h√≥a
def print_metrics(y_true, y_pred, dataset_name):
    mae  = mean_absolute_error(y_true, y_pred)
    mape = mean_absolute_percentage_error(y_true, y_pred) * 100
    mse  = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2   = r2_score(y_true, y_pred)

    print(f'\n---üìò {dataset_name} ---')
    print(f'MSE: {mse:.6f}')
    print(f'RMSE: {rmse:.6f}')
    print(f'MAE: {mae:.6f}')
    print(f'MAPE: {mape:.6f}%')
    print(f'R2 Score: {r2:.6f}')


# In k·∫øt qu·∫£ tr√™n d·ªØ li·ªáu chu·∫©n h√≥a
print_metrics(y_train, y_train_predict, 'T·∫≠p hu·∫•n luy·ªán (Train)')
print_metrics(y_test, y_test_predict, 'T·∫≠p ki·ªÉm tra (Test)')

# V·∫Ω bi·ªÉu ƒë·ªì so s√°nh gi√° d·ª± ƒëo√°n v√† gi√° th·ª±c t·∫ø
train_data1 = df1[window_size:train_size].copy()
test_data1  = df1[train_size + window_size:].copy()

y_train_predict_inverse = scaler.inverse_transform(y_train_predict)
y_test_predict_inverse  = scaler.inverse_transform(y_test_predict)

plt.figure(figsize=(24, 8))
plt.plot(df1.index, df1['Close'], label='Gi√° th·ª±c t·∫ø', color='red')
train_data1['D·ª± ƒëo√°n'] = y_train_predict_inverse.flatten()
plt.plot(train_data1.index, train_data1['D·ª± ƒëo√°n'], label='Gi√° d·ª± ƒëo√°n train', color='green')
test_data1['D·ª± ƒëo√°n'] = y_test_predict_inverse.flatten()
plt.plot(test_data1.index, test_data1['D·ª± ƒëo√°n'], label='Gi√° d·ª± ƒëo√°n test', color='blue')
plt.title('LSTM - So s√°nh gi√° d·ª± b√°o v√† gi√° th·ª±c t·∫ø')
plt.xlabel('Th·ªùi gian')
plt.ylabel('Gi√° ƒë√≥ng c·ª≠a (USD)')
plt.legend()
plt.savefig('logs/lstm/Close_comparison_lstm.png')
plt.close()